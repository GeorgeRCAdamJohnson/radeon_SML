{
  "title": "Convolutional neural network",
  "summary": "A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as the transformer",
  "content": "Convolutional neural network Type of artificial neural network body.skin-minerva body.skin--responsive Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Neural networks Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neural radiance field Physics-informed neural networks Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Q-learning Policy gradient SARSA Temporal difference (TD) Multi-agent Self-play Learning with humans Active learning Crowdsourcing Human-in-the-loop Mechanistic interpretability RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Topological deep learning Journals and conferences AAAI ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning html.skin-theme-clientpref-night v t e A convolutional neural network ( CNN ) is a type of feedforward neural network that learns features via filter (or kernel ) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. 1 Convolution-based networks are the de-facto standard in deep learning -based approaches to computer vision 2 and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as the transformer . Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections. 3 4 For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, 5 6 only 25 weights for each convolutional layer are required to process 5x5-sized tiles. 7 8 Higher-layer features are extracted from wider context windows, compared to lower-layer features. Some applications of CNNs include: image and video recognition , 9 recommender systems , 10 image classification , image segmentation , medical image analysis , natural language processing , 11 brain–computer interfaces , 12 and financial time series . 13 CNNs are also known as shift invariant or space invariant artificial neural networks , based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation- equivariant responses known as feature maps. 14 15 Counter-intuitively, most convolutional neural networks are not invariant to translation , due to the downsampling operation they apply to the input. 16 Feedforward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer . The full connectivity of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set. 17 Convolutional networks were inspired by biological processes 18 19 20 21 in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field . The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms . This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered . This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks. Architecture Main article: Layer (deep learning) Comparison of the LeNet (1995) and AlexNet (2012) convolution, pooling and dense layers A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product , and its activation function is commonly ReLU . As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers , fully connected layers, and normalization layers. Here it should be noted how close a convolutional neural network is to a matched filter . 22 Convolutional layers In a CNN, the input is a tensor with shape: (number of inputs) × (input height) × (input width) × (input channels ) After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) × (feature map height) × (feature map width) × (feature map channels ). Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. 23 Each convolutional neuron processes data only for its receptive field . 1D convolutional neural network feed forward example Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. 7 For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks. 3 4 To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, 24 which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 1 × 1 displaystyle 1 times 1 kernels. Pooling layers Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. 25 26 There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, 27 28 while average pooling takes the average value. Fully connected layers Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. Receptive field In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field . Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer . Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution 29 30 expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, 31 thus having a variable receptive field size. Weights Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting. 32 Deconvolutional A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers. 33 A deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix. 34 An unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is x ↦ x x x x displaystyle x mapsto begin bmatrix x x x x end bmatrix . Deconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve. 35 History CNN are often compared to the way the brain achieves vision processing in living organisms . 36 Receptive fields in the visual cortex Main article: Surround suppression Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field . Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field . 37 Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. citation needed The cortex in each hemisphere represents the contralateral visual field . citation needed Their 1968 paper identified two basic visual cell types in the brain: 19 simple cells , whose output is maximized by straight edges having particular orientations within their receptive field complex cells , which have larger receptive fields , whose output is insensitive to the exact position of the edges in the field. Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks. 38 37 Fukushima's analog threshold elements in a vision model In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer. This is the essential core of a convolutional network, but the weights were not trained. In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function . 39 40 Neocognitron, origin of the trainable CNN architecture The neocognitron 18 was introduced by Fukushima in 1980. 20 28 41 The neocognitron introduced the two basic types of layers: later known as a convolutional layer, which contains units whose receptive fields cover a patch of the previous layer. A shared-weights receptive-field group (a plane in neocognitron terminology) is often called a filter, and a layer typically has several such filters. and applies inhibition (divisive normalization) pooled from a somewhat larger patch and across different filters in a layer, and applies a saturating activation function. The patch weights are nonnegative and are not trainable in the original neocognitron. The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted. Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. 18 Today, however, the CNN architecture is usually trained through backpropagation . Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become a very popular activation function for CNNs and deep neural networks in general. 42 Convolution in time The term convolution first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter , and demonstrated it on a speech recognition task. 8 They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function ( For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t). ). 8 Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here. Time delay neural networks The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance. 43 A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation . 44 Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one. 43 TDNNs are convolutional networks that share weights along the temporal dimension. 45 They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. 46 Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron. TDNNs improved the performance of far-distance speech recognition. 47 Image recognition with CNNs trained by gradient descent Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. 48 However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed. 49 Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989) 49 used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. Wei Zhang et al. (1988) 14 15 used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) 50 and breast cancer detection in mammograms (1994). 51 This approach became a foundation of modern computer vision . Max pooling In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. 27 In their system they used several TDNNs per word, one for each syllable . The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. In a variant of the neocognitron called the cresceptron , instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 used max pooling, where a downsampling unit computes the maximum of the activations of the units in its patch, 52 introducing this method into the vision field. Max pooling is often used in modern CNNs. 53 LeNet-5 Main article: LeNet LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, 54 classifies hand-written numbers on checks digitized in 32×32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources. It was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR 's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day. 55 Shift-invariant neural network A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988. 14 15 It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991 56 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) 50 and automatic detection of breast cancer in mammograms (1994) . 51 A different convolution-based design was proposed in 1988 57 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs. 58 59 GPU implementations Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs). In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU . 60 In 2005, another paper also emphasised the value of GPGPU for machine learning . 61 The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. 62 In the same period, GPUs were also used for unsupervised training of deep belief networks . 63 64 65 66 In 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs. 67 In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU. 25 In 2011, the network won an image recognition contest where they achieved superhuman performance for the first time. 68 Then they won more competitions and achieved state of the art on several benchmarks. 69 53 28 Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. 70 It was an early catalytic event for the AI boom . Compared to the training of CNNs using GPUs , not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD -level parallelism that is available on the Intel Xeon Phi . 71 72 Distinguishing features In the past, traditional multilayer perceptron (MLP) models were used for image recognition. example needed However, the full connectivity between nodes caused the curse of dimensionality , and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a locally connected layer . Pooling layer Main article: Pooling layer Worked example of 2x2 maxpooling with stride 2 Max pooling with a 2x2 filter and stride = 2 Another important concept of CNNs is pooling, which is used as a form of non-linear down-sampling . Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input. 78 Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input. Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting . This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer ) in a CNN architecture. 75 : 460–461 While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used. 16 74 The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations: f X , Y ( S ) = max a , b = 0 1 S 2 X + a , 2 Y + b . displaystyle f_ X,Y (S)= max _ a,b=0 ^ 1 S_ 2X+a,2Y+b . In this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well). In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ 2 -norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice. 79 Due to the effects of fast spatial reduction of the size of the representation, which? there is a recent trend towards using smaller filters 80 or discarding pooling layers altogether. 81 RoI pooling to size 2x2. In this example region proposal (an input parameter) has size 7x5. Channel max pooling A channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation. 82 See 83 84 for reviews for pooling methods. ReLU layer ReLU is the abbreviation of rectified linear unit . It was proposed by Alston Householder in 1941, 85 and used in CNN by Kunihiko Fukushima in 1969. 39 ReLU applies the non-saturating activation function f ( x ) = max ( 0 , x ) textstyle f(x)= max(0,x) . 70 It effectively removes negative values from an activation map by setting them to zero. 86 It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers. In 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks, 87 compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent f ( x ) = tanh ⁡ ( x ) displaystyle f(x)= tanh(x) , f ( x ) = | tanh ⁡ ( x ) | displaystyle f(x)=| tanh(x)| , and the sigmoid function σ ( x ) = ( 1 + e − x ) − 1 textstyle sigma (x)=(1+e^ -x )^ -1 . ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy. 88 Fully connected layer After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks . Their activations can thus be computed as an affine transformation , with matrix multiplication followed by a bias offset ( vector addition of a learned or fixed bias term). Loss layer Main articles: Loss function and Loss functions for classification The loss layer , or loss function , exemplifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. nb 3 Sigmoid cross-entropy loss is used for predicting K independent probability values in 0 , 1 displaystyle 0,1 . Euclidean loss is used for regressing to real-valued labels ( − ∞ , ∞ ) displaystyle (- infty , infty ) . Hyperparameters html body.mediawiki This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. ( June 2017 ) ( Learn how and when to remove this message ) Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). Padding Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied",
  "cached_at": "2025-10-25T20:02:11.863778"
}