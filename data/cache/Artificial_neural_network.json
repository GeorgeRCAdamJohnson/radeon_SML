{
  "title": "Neural network (machine learning)",
  "summary": "In machine learning, a neural network is a computational model inspired by the structure and functions of biological neural networks",
  "content": "Neural network (machine learning) Computational model used in machine learning, based on connected, hierarchical functions This article is about the computational models used for artificial intelligence. For other uses, see Neural network (disambiguation) . body.skin-minerva body.skin--responsive Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Neural networks Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neural radiance field Physics-informed neural networks Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Q-learning Policy gradient SARSA Temporal difference (TD) Multi-agent Self-play Learning with humans Active learning Crowdsourcing Human-in-the-loop Mechanistic interpretability RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Topological deep learning Journals and conferences AAAI ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning html.skin-theme-clientpref-night v t e An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain . Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. In machine learning , a neural network (also artificial neural network or neural net , abbreviated ANN or NN ) is a computational model inspired by the structure and functions of biological neural networks . 1 2 A neural network consists of connected units or nodes called artificial neurons , which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges , which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The signal is a real number , and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function . The strength of the signal at each connection is determined by a weight , which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer ) to the last layer (the output layer ), possibly passing through multiple intermediate layers ( hidden layers ). A network is typically called a deep neural network if it has at least two hidden layers. 3 Artificial neural networks are used for various tasks, including predictive modeling , adaptive control , and solving problems in artificial intelligence . They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. Training Neural networks are typically trained through empirical risk minimization . This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. 4 Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. 4 During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function . 5 This method allows the network to generalize to unseen data. /ref> The network correctly detects the starfish. However, the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes. In addition, a shell that was not included in the training gives a weak signal for the oval shape, also resulting in a weak signal for the sea urchin output. These weak signals may result in a result for sea urchin. br />In reality, textures and outlines would not be represented by single nodes, but rather by associated weight patterns of multiple nodes. , ' > Simplified example of training a neural network in object detection: The network is trained by multiple images that are known to depict starfish and sea urchins , which are correlated with nodes that represent visual features . The starfish match with a ringed texture and a star outline, whereas most sea urchins match with a striped texture and oval shape. However, the instance of a ring textured sea urchin creates a weakly weighted association between them. Subsequent run of the network on an input image (left): 6 The network correctly detects the starfish. However, the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes. In addition, a shell that was not included in the training gives a weak signal for the oval shape, also resulting in a weak signal for the sea urchin output. These weak signals may result in a false positive result for sea urchin. In reality, textures and outlines would not be represented by single nodes, but rather by associated weight patterns of multiple nodes. History Main article: History of artificial neural networks Early work Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression . It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. 7 8 9 10 11 Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism . Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts 12 (1943) considered a non-learning computational model for neural networks. 13 This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb 14 proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning . It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network . Farley and Clark 15 (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester , Holland, Habit and Duda (1956). 16 In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, 17 18 19 20 funded by the United States Office of Naval Research . 21 R. D. Joseph (1960) 22 mentions an even earlier perceptron-like device by Farley and Clark: 10 Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device. However, they dropped the subject. The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to the Golden Age of AI fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. 23 The first perceptrons did not have adaptive hidden units. However, Joseph (1960) 22 also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) 24 : section 16 cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning . Deep learning breakthroughs in the 1960s and 1970s Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling , a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, 25 or a generalization of Rosenblatt's perceptron. 26 A 1971 paper described a deep network with eight layers trained by this method, 27 which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or gates. 10 The first deep learning multilayer perceptron trained by stochastic gradient descent 28 was published in 1967 by Shun'ichi Amari . 29 In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. 10 Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. 10 30 31 The rectifier has become the most popular activation function for deep learning. 32 Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), 33 who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967). In 1976 transfer learning was introduced in neural networks learning. 34 35 Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. 36 37 38 Backpropagation Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 39 to networks of differentiable nodes. The terminology back-propagating errors was actually introduced in 1962 by Rosenblatt, 24 but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory . 40 In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). 41 42 10 G.M. Ostrovski et al. republished it in 1971. 43 44 Paul Werbos applied backpropagation to neural networks in 1982 45 46 (his 1974 PhD thesis, reprinted in a 1994 book, 47 did not yet describe the algorithm 44 ). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. 48 Convolutional neural networks Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 36 also introduced max pooling , 49 a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision . The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. 50 51 In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. 52 In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. 53 In 1990, Wei Zhang implemented a CNN on optical computing hardware. 54 In 1991, a CNN was applied to medical image object segmentation 55 and breast cancer detection in mammograms. 56 LeNet -5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images. 57 From 1988 onward, 58 59 the use of neural networks transformed the field of protein structure prediction , in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments . 60 Recurrent neural networks One origin of RNN was statistical mechanics . In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory , adding in the component of learning. 61 This was popularized as the Hopfield network by John Hopfield (1982). 62 Another origin of RNN was neuroscience. The word recurrent is used to describe loop-like structures in anatomy. In 1901, Cajal observed recurrent semicircles in the cerebellar cortex . 63 Hebb considered reverberating circuit as an explanation for short-term memory. 64 The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past. 12 In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, 65 66 used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks. In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. 67 68 In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. 65 69 It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology. Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology . In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the neural sequence chunker or neural history compressor 70 71 which introduced the important concepts of self-supervised pre-training (the P in ChatGPT ) and neural knowledge distillation . 10 In 1993, a neural history compressor system solved a Very Deep Learning task that required more than 1000 subsequent layers in an RNN unfolded in time. 72 In 1991, Sepp Hochreiter 's diploma thesis 73 identified and analyzed the vanishing gradient problem 73 74 and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. 75 76 This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. 77 It became the default choice for RNN architecture. During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski , Peter Dayan , Geoffrey Hinton , etc., including the Boltzmann machine , 78 restricted Boltzmann machine , 79 Helmholtz machine , 80 and the wake-sleep algorithm . 81 These were designed for unsupervised learning of deep generative models. Deep learning Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition . 82 83 In 2011, a CNN named DanNet 84 85 by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella , and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. 38 It then won more contests. 86 87 They also showed how max-pooling CNNs on GPU improved performance significantly. 88 In October 2012, AlexNet by Alex Krizhevsky , Ilya Sutskever , and Geoffrey Hinton 89 won the large-scale ImageNet competition by a significant the applications include clustering , the estimation of statistical distributions , compression and filtering . Reinforcement learning Main article: Reinforcement learning See also: Stochastic control In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning , the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , . . . , s n ∈ S displaystyle textstyle s_ 1 ,...,s_ n in S and actions a 1 , . . . , a m ∈ A displaystyle textstyle a_ 1 ,...,a_ m in A . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution P ( c t | s t ) displaystyle textstyle P(c_ t |s_ t ) , the observation distribution P ( x t | s t ) displaystyle textstyle P(x_ t |s_ t ) and the transition distribution P ( s t + 1 | s t , a t ) displaystyle textstyle P(s_ t+1 |s_ t ,a_ t ) , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. 132 133 Dynamic programming coupled with ANNs (giving neurodynamic programming) 134 has been applied to problems such as those involved in vehicle routing , 135 video games, natural resource management 136 137 and medicine 138 because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. Self-learning Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). 139 It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. 140 Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation: In situation s perform action a; Receive consequence situation s'; Compute emotion of being in consequence situation v(s'); Update crossbar memory w'(a,s) = w(a,s) + v(s'). The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations. 141 Neuroevolution Main article: Neuroevolution Neuroevolution can create neural network topologies and weights using evolutionary computation . It is competitive with sophisticated gradient descent approaches. 142 143 One advantage of neuroevolution is that it may be less prone to get caught in dead ends . 144 Stochastic neural network Stochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions citation needed , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima . 145 Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks . 146 Topological deep learning Topological deep learning , first introduced in 2017, 147 is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology , TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology . As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence , fostering a mutually beneficial relationship between AI and mathematics . Other In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods , 148 gene expression programming , 149 simulated annealing , 150 expectation–maximization , non-parametric methods and particle swarm optimization 151 are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks. 152 153 Modes This section includes a list of references , related reading , or external links , but its sources remain unclear because it lacks inline citations . Please help improve this section by introducing more precise citations. ( August 2019 ) ( Learn how and when to remove this message ) Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces noise into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use mini-batches , small batches with samples in each batch selected stochastically from the entire data set. Types Main article: Types of artificial neural networks ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology . Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be supervised by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers. Some of the main breakthroughs include: Convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; 154 155 where long short-term memory avoids the vanishing gradient problem 156 and can handle signals",
  "cached_at": "2025-10-25T20:02:10.352800"
}