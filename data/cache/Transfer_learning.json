{
  "title": "Transfer learning",
  "summary": "Transfer learning (TL) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task. For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing or transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency.",
  "content": "Transfer learning Machine learning technique Illustration of transfer learning .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}} .mw-parser-output .nobold{font-weight:normal} Part of a series on Machine learning and data mining Paradigms Supervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning Problems Classification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Neural networks Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neural radiance field Physics-informed neural networks Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Q-learning Policy gradient SARSA Temporal difference (TD) Multi-agent Self-play Learning with humans Active learning Crowdsourcing Human-in-the-loop Mechanistic interpretability RLHF Model diagnostics Coefficient of determination Confusion matrix Learning curve ROC curve Mathematical foundations Kernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Topological deep learning Journals and conferences AAAI ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR Related articles Glossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e Transfer learning ( TL ) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task. [ 1 ] For example, for image classification , knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning , although practical ties between the two fields are limited. Reusing or transferring information from previously learned tasks to new tasks has the potential to significantly improve learning efficiency. [ 2 ] Since transfer learning makes use of training with multiple objective functions it is related to cost-sensitive machine learning and multi-objective optimization . [ 3 ] History In 1976, Bozinovski and Fulgosi published a paper addressing transfer learning in neural network training. [ 4 ] [ 5 ] The paper gives a mathematical and geometrical model of the topic. In 1981, a report considered the application of transfer learning to a dataset of images representing letters of computer terminals, experimentally demonstrating positive and negative transfer learning. [ 6 ] In 1992, Lorien Pratt formulated the discriminability-based transfer (DBT) algorithm. [ 7 ] By 1998, the field had advanced to include multi-task learning , [ 8 ] along with more formal theoretical foundations. [ 9 ] Influential publications on transfer learning include the book Learning to Learn in 1998, [ 10 ] a 2009 survey [ 11 ] and a 2019 survey. [ 12 ] Ng said in his NIPS 2016 tutorial [ 13 ] [ 14 ] that TL would become the next driver of machine learning commercial success after supervised learning . In the 2020 paper, \"Rethinking Pre-Training and self-training\", [ 15 ] Zoph et al. reported that pre-training can hurt accuracy, and advocate self-training instead. Definition The definition of transfer learning is given in terms of domains and tasks. A domain D {\\displaystyle {\\mathcal {D}}} consists of: a feature space X {\\displaystyle {\\mathcal {X}}} and a marginal probability distribution P ( X ) {\\displaystyle P(X)} , where X = { x 1 , . . . , x n } ∈ X {\\displaystyle X=\\{x_{1},...,x_{n}\\}\\in {\\mathcal {X}}} . Given a specific domain, D = { X , P ( X ) } {\\displaystyle {\\mathcal {D}}=\\{{\\mathcal {X}},P(X)\\}} , a task consists of two components: a label space Y {\\displaystyle {\\mathcal {Y}}} and an objective predictive function f : X → Y {\\displaystyle f:{\\mathcal {X}}\\rightarrow {\\mathcal {Y}}} . The function f {\\displaystyle f} is used to predict the corresponding label f ( x ) {\\displaystyle f(x)} of a new instance x {\\displaystyle x} . This task, denoted by T = { Y , f ( x ) } {\\displaystyle {\\mathcal {T}}=\\{{\\mathcal {Y}},f(x)\\}} , is learned from the training data consisting of pairs { x i , y i } {\\displaystyle \\{x_{i},y_{i}\\}} , where x i ∈ X {\\displaystyle x_{i}\\in {\\mathcal {X}}} and y i ∈ Y {\\displaystyle y_{i}\\in {\\mathcal {Y}}} . [ 16 ] Given a source domain D S {\\displaystyle {\\mathcal {D}}_{S}} and learning task T S {\\displaystyle {\\mathcal {T}}_{S}} , a target domain D T {\\displaystyle {\\mathcal {D}}_{T}} and learning task T T {\\displaystyle {\\mathcal {T}}_{T}} , where D S ≠ D T {\\displaystyle {\\mathcal {D}}_{S}\\neq {\\mathcal {D}}_{T}} , or T S ≠ T T {\\displaystyle {\\mathcal {T}}_{S}\\neq {\\mathcal {T}}_{T}} , transfer learning aims to help improve the learning of the target predictive function f T ( ⋅ ) {\\displaystyle f_{T}(\\cdot )} in D T {\\displaystyle {\\mathcal {D}}_{T}} using the knowledge in D S {\\displaystyle {\\mathcal {D}}_{S}} and T S {\\displaystyle {\\mathcal {T}}_{S}} . [ 16 ] Applications Algorithms for transfer learning are available in Markov logic networks [ 17 ] and Bayesian networks . [ 18 ] Transfer learning has been applied to cancer subtype discovery, [ 19 ] building utilization, [ 20 ] [ 21 ] general game playing , [ 22 ] text classification , [ 23 ] [ 24 ] digit recognition, [ 25 ] medical imaging and spam filtering . [ 26 ] In 2020, it was discovered that, due to their similar physical natures, transfer learning is possible between electromyographic (EMG) signals from the muscles and classifying the behaviors of electroencephalographic (EEG) brainwaves, from the gesture recognition domain to the mental state recognition domain. It was noted that this relationship worked in both directions, showing that electroencephalographic can likewise be used to classify EMG. [ 27 ] The experiments noted that the accuracy of neural networks and convolutional neural networks were improved [ 28 ] through transfer learning both prior to any learning (compared to standard random weight distribution) and at the end of the learning process (asymptote). That is, results are improved by exposure to another domain. Moreover, the end-user of a pre-trained model can change the structure of fully-connected layers to improve performance. [ 29 ] See also Crossover (genetic algorithm) Domain adaptation General game playing Multi-task learning Multitask optimization Transfer of learning in educational psychology Zero-shot learning Feature learning external validity References .mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman} ↑ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}} West, Jeremy; Ventura, Dan; Warnick, Sean (2007). \"Spring Research Presentation: A Theoretical Foundation for Inductive Transfer\" . Brigham Young University, College of Physical and Mathematical Sciences. Archived from the original on 2007-08-01 . Retrieved 2007-08-05 . ↑ George Karimpanal, Thommen; Bouffanais, Roland (2019). \"Self-organizing maps for storage and transfer of knowledge in reinforcement learning\". Adaptive Behavior . 27 (2): 111– 126. arXiv : 1811.08318 . doi : 10.1177/1059712318818568 . ISSN 1059-7123 . S2CID 53774629 . ↑ Cost-Sensitive Machine Learning. (2011). USA: CRC Press, Page 63, https://books.google.com/books?id=8TrNBQAAQBAJ&amp;pg=PA63 ↑ Stevo. Bozinovski and Ante Fulgosi (1976). \"The influence of pattern similarity and transfer learning on the base perceptron training.\" (original in Croatian) Proceedings of Symposium Informatica 3-121-5, Bled. ↑ Stevo Bozinovski (2020) \"Reminder of the first paper on transfer learning in neural networks, 1976\" . Informatica 44: 291–302. ↑ S. Bozinovski (1981). \"Teaching space: A representation concept for adaptive pattern classification.\" COINS Technical Report, the University of Massachusetts at Amherst, No 81-28 [available online ] ↑ Pratt, L. Y. (1992). \"Discriminability-based transfer between neural networks\" (PDF) . NIPS Conference: Advances in Neural Information Processing Systems 5 . Morgan Kaufmann Publishers. pp. 204– 211. ↑ Caruana, R., \"Multitask Learning\", pp. 95-134 in Thrun &amp; Pratt 2012 ↑ Baxter, J., \"Theoretical Models of Learning to Learn\", pp. 71-95 Thrun &amp; Pratt 2012 ↑ Thrun &amp; Pratt 2012 . ↑ Pan, Sinno Jialin; Yang, Qiang (2010). \"A Survey on Transfer Learning\" (PDF) . IEEE Transactions on Knowledge and Data Engineering . 22 (10): 1345– 1359. Bibcode : 2010ITKDE..22.1345P . doi : 10.1109/TKDE.2009.191 . ↑ Zhuang, Fuzhen; Qi, Zhiyuan; Duan, Keyu; Xi, Dongbo; Zhu, Yongchun; Zhu, Hengshu; Xiong, Hui; He, Qing (2019). \"A Comprehensive Survey on Transfer Learning\". arXiv : 1911.02685 [ cs.LG ]. ↑ NIPS 2016 tutorial: \"Nuts and bolts of building AI applications using Deep Learning\" by Andrew Ng , 6 May 2018, archived from the original on 2021-12-19 , retrieved 2019-12-28 ↑ \"Nuts and bolts of building AI applications using Deep Learning, slides\" (PDF) . ↑ Zoph, Barret (2020). \"Rethinking pre-training and self-training\" (PDF) . Advances in Neural Information Processing Systems . 33 : 3833– 3845. arXiv : 2006.06882 . Retrieved 2022-12-20 . 1 2 Lin, Yuan-Pin; Jung, Tzyy-Ping (27 June 2017). \"Improving EEG-Based Emotion Classification Using Conditional Transfer Learning\" . Frontiers in Human Neuroscience . 11 334. doi : 10.3389/fnhum.2017.00334 . PMC 5486154 . PMID 28701938 . Material was copied from this source, which is available under a Creative Commons Attribution 4.0 International License . ↑ Mihalkova, Lilyana; Huynh, Tuyen; Mooney, Raymond J. (July 2007), \"Mapping and Revising Markov Logic Networks for Transfer\" (PDF) , Learning Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI-2007) , Vancouver, BC, pp. 608– 614 , retrieved 2007-08-05 {{ citation }} : CS1 maint: location missing publisher ( link ) ↑ Niculescu-Mizil, Alexandru; Caruana, Rich (March 21–24, 2007), \"Inductive Transfer for Bayesian Network Structure Learning\" (PDF) , Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS 2007) , retrieved 2007-08-05 ↑ Hajiramezanali, E. &amp; Dadaneh, S. Z. &amp; Karbalayghareh, A. &amp; Zhou, Z. &amp; Qian, X. Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada. arXiv : 1810.09433 ↑ Arief-Ang, I.B.; Salim, F.D.; Hamilton, M. (2017-11-08). DA-HOC: semi-supervised domain adaptation for room occupancy prediction using CO2 sensor data . 4th ACM International Conference on Systems for Energy-Efficient Built Environments (BuildSys). Delft, Netherlands. pp. 1– 10. doi : 10.1145/3137133.3137146 . ISBN 978-1-4503-5544-5 . ↑ Arief-Ang, I.B.; Hamilton, M.; Salim, F.D. (2018-12-01). \"A Scalable Room Occupancy Prediction with Transferable Time Series Decomposition of CO2 Sensor Data\". ACM Transactions on Sensor Networks . 14 ( 3– 4): 21:1–21:28. doi : 10.1145/3217214 . S2CID 54066723 . ↑ Banerjee, Bikramjit, and Peter Stone. \" General Game Learning Using Knowledge Transfer .\" IJCAI. 2007. ↑ Do, Chuong B.; Ng, Andrew Y. (2005). \"Transfer learning for text classification\". Neural Information Processing Systems Foundation, NIPS*2005 (PDF) . Retrieved 2007-08-05 . ↑ Rajat, Raina; Ng, Andrew Y.; Koller, Daphne (2006). \"Constructing Informative Priors using Transfer Learning\". Twenty-third International Conference on Machine Learning (PDF) . Retrieved 2007-08-05 . ↑ Maitra, D. S.; Bhattacharya, U.; Parui, S. K. (August 2015). \"CNN based common approach to handwritten character recognition of multiple scripts\". 2015 13th International Conference on Document Analysis and Recognition (ICDAR) . pp. 1021– 1025. doi : 10.1109/ICDAR.2015.7333916 . ISBN 978-1-4799-1805-8 . S2CID 25739012 . ↑ Bickel, Steffen (2006). \"ECML-PKDD Discovery Challenge 2006 Overview\". ECML-PKDD Discovery Challenge Workshop (PDF) . Retrieved 2007-08-05 . ↑ Bird, Jordan J.; Kobylarz, Jhonatan; Faria, Diego R.; Ekart, Aniko; Ribeiro, Eduardo P. (2020). \"Cross-Domain MLP and CNN Transfer Learning for Biological Signal Processing: EEG and EMG\" . IEEE Access . 8 . Institute of Electrical and Electronics Engineers (IEEE): 54789– 54801. Bibcode : 2020IEEEA...854789B . doi : 10.1109/access.2020.2979074 . ISSN 2169-3536 . ↑ Maitra, Durjoy Sen; Bhattacharya, Ujjwal; Parui, Swapan K. (August 2015). \"CNN based common approach to handwritten character recognition of multiple scripts\". 2015 13th International Conference on Document Analysis and Recognition (ICDAR) . pp. 1021– 1025. doi : 10.1109/ICDAR.2015.7333916 . ISBN 978-1-4799-1805-8 . S2CID 25739012 . ↑ Kabir, H. M. Dipu; Abdar, Moloud; Jalali, Seyed Mohammad Jafar; Khosravi, Abbas; Atiya, Amir F.; Nahavandi, Saeid; Srinivasan, Dipti (January 7, 2022). \"SpinalNet: Deep Neural Network with Gradual Input\". IEEE Transactions on Artificial Intelligence . 4 (5): 1165– 1177. arXiv : 2007.03347 . doi : 10.1109/TAI.2022.3185179 . S2CID 220381239 . Sources Thrun, Sebastian; Pratt, Lorien (6 December 2012). Learning to Learn . Springer Science &amp; Business Media. ISBN 978-1-4615-5529-2 .",
  "cached_at": "2025-10-25T19:35:12.931314"
}