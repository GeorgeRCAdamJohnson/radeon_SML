{
  "title": "AI safety",
  "summary": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment, monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.",
  "content": "AI safety Research area on making AI safe and beneficial .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}} Part of a series on Artificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Open-source Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy AI alignment Artificial consciousness The bitter lesson Chinese room Friendly AI Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom AI bubble Controversies Deepfake pornography Taylor Swift deepfake pornography controversy Google Gemini image generation controversy Pause Giant AI Experiments Removal of Sam Altman from OpenAI Statement on AI Risk Tay (chatbot) Théâtre D'opéra Spatial Voiceverse NFT plagiarism scandal Glossary Glossary .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models. [ 1 ] [ 2 ] Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit , the United States and the United Kingdom both established their own AI Safety Institute . However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities. [ 3 ] Motivations Scholars discuss current risks from critical systems failures, [ 4 ] bias , [ 5 ] and AI-enabled surveillance, [ 6 ] as well as emerging risks like technological unemployment , digital manipulation, [ 7 ] weaponization, [ 8 ] AI-enabled cyberattacks [ 9 ] and bioterrorism . [ 10 ] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents, [ 11 ] or from AI enabling perpetually stable dictatorships. [ 12 ] Existential safety .mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}} See also: Existential risk from artificial general intelligence Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\". [ 13 ] Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\". [ 14 ] AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology [ 15 ] [ 16 ] [ 17 ] – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction )\" outcome of advanced AI. [ 15 ] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\". [ 18 ] History Risks from AI began to be seriously discussed at the start of the computer age : {{Cite news| issn = 0362-4331| last = Markoff| first = John|author-link = John Markoff| title = In 1949, He Imagined an Age of Robots| work = The New York Times| access-date = 2022-11-23| date = 2013-05-20| url = https://www.nytimes.com/2013/05/21/science/mit-scholars-1949-essay-on-machine-age-is-found.html| archive-date = 2022-11-23| archive-url = https://web.archive.org/web/20221123061554/https://www.nytimes.com/2013/05/21/science/mit-scholars-1949-essay-on-machine-age-is-found.html| url-status = live}}&lt;/ref>\"}},\"i\":0}}]}' id=\"mwbw\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes. — Norbert Wiener (1949) [ 19 ] In 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines. [ 20 ] From 2008 to 2009, the Association for the Advancement of Artificial Intelligence ( AAAI ) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\". [ 21 ] In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\" [ 22 ] at the Philosophy and Theory of Artificial Intelligence conference, [ 23 ] listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\". [ 24 ] In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies . He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction. [ 25 ] His argument that future advanced systems may pose a threat to human existence prompted Elon Musk , [ 26 ] Bill Gates , [ 27 ] and Stephen Hawking [ 28 ] to voice similar concerns. In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. [ 29 ] To date, the letter has been signed by over 8000 people including Yann LeCun , Shane Legg , Yoshua Bengio , and Stuart Russell . In the same year, a group of academics led by professor Stuart J. Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\". [ 30 ] In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, [ 31 ] which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI. [ 32 ] In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published. [ 33 ] In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI , where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\". [ 34 ] In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, [ 35 ] and assurance. [ 36 ] The following year, researchers organized a workshop at ICLR that focused on these problem areas. [ 37 ] In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety. [ 2 ] In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety. [ 38 ] The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models. [ 39 ] During the summit the intention to create the International Scientific Report on the Safety of Advanced AI [ 40 ] was announced. In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November. [ 41 ] In 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report. The report, commissioned by 30 nations and the United Nations, represents the first global scientific review of potential risks associated with advanced artificial intelligence. It details potential threats stemming from misuse, malfunction, and societal disruption, with the objective of informing policy through evidence-based findings, without providing specific recommendations. [ 42 ] [ 43 ] Research focus .mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}@media print{body.ns-0 .mw-parser-output .ambox{display:none!important}} This section relies excessively on references to primary sources . Please improve this section by adding secondary or tertiary sources . Find sources: \".\" AI safety -Hendrycks – news · newspapers · books · scholar · JSTOR ( July 2023 ) ( Learn how and when to remove this message ) AI safety research areas include robustness, monitoring, and alignment. [ 2 ] [ 36 ] Robustness Adversarial robustness AI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\". [ 44 ] For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. [ 45 ] This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible. [ 46 ] [ 47 ] [ 48 ] Carefully crafted noise can be added to an image to cause it to be misclassified with high confidence. The image on the right is predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example. [ 45 ] Adversarial robustness is often associated with security. [ 49 ] Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses. [ 50 ] Network intrusion [ 51 ] and malware [ 52 ] detection systems also must be adversarially robust since attackers may design their attacks to fool detectors. Models that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. [ 53 ] Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. [ 54 ] This issue can be addressed by improving the adversarial robustness of the reward model. [ 55 ] More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward. [ 56 ] Monitoring Estimating uncertainty It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis. [ 57 ] ML models generally express confidence by outputting probabilities; however, they are often overconfident, [ 58 ] especially in situations that differ from those that they were trained to handle. [ 59 ] Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct. Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. [ 60 ] Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, [ 61 ] though a range of additional techniques are in use. [ 62 ] [ 63 ] Detecting malicious use Scholars [ 8 ] and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, [ 64 ] manipulate public opinion, [ 65 ] [ 66 ] or automate cyber attacks. [ 67 ] These worries are a practical concern for companies like OpenAI which host powerful AI tools online. [ 68 ] In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity. [ 69 ] Transparency Neural networks have often been described as black boxes , [ 70 ] meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. [ 71 ] This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear. [ 72 ] It also raises debates in healthcare over whether statistically efficient but opaque models should be used. [ 73 ] One critical benefit of transparency is explainability . [ 74 ] It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment. [ 74 ] Another benefit is to reveal the cause of failures. [ 70 ] At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels. [ 75 ] Transparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. [ 76 ] Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision. [ 77 ] Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future. [ 78 ] \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. [ 79 ] [ 80 ] For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in Spider-Man costumes, sketches of Spider-Man, and the word 'spider'. [ 81 ] It also involves explaining connections between these neurons or 'circuits'. [ 82 ] [ 83 ] For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context. [ 84 ] \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations. [ 85 ] Detecting trojans Machine learning models can potentially contain \"trojans\" or \" backdoors \": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view; [ 2 ] or a trojaned autonomous vehicle may function normally until a specific trigger is visible. [ 86 ] Note that an adversary must have access to the system's training data in order to plant a trojan. [ citation needed ] This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data. [ 87 ] Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images. [ 88 ] In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools. [ 56 ] A 2024 research paper by Anthropic showed that large language models could be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such as supervised fine-tuning , reinforcement learning and adversarial training, failed to remove these backdoors. [ 89 ] Alignment .mw-parser-output .excerpt-hat .mw-editsection-like{font-style:normal} This section is an excerpt from AI alignment . [ edit ] In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives. [ 90 ] It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals , such as gaining human approval . But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned. [ 90 ] [ 91 ] AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways ( reward hacking ). [ 90 ] [ 92 ] Advanced AI systems may develop unwanted instrumental strategies , such as seeking power or survival because such strategies help them achieve their assigned final goals. [ 90 ] [ 93 ] [ 94 ] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions . [ 95 ] [ 96 ] Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed. [ 97 ] [ 98 ] Today, some of these issues affect existing commercial systems such as LLMs, [ 99 ] [ 100 ] [ 101 ] robots , [ 102 ] autonomous vehicles , [ 103 ] and social media recommendation engines . [ 99 ] [ 94 ] [ 104 ] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities. [ 105 ] [ 92 ] [ 91 ] Many prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like ( AGI ) and superhuman cognitive capabilities ( ASI ), and could endanger human civilization if misaligned. [ 106 ] [ 94 ] These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI , Anthropic , and Google DeepMind . [ 107 ] [ 108 ] [ 109 ] These risks remain debated. [ 110 ] AI alignment is a subfield of AI safety, the study of how to build safe AI systems. [ 111 ] [ 112 ] Other subfields of AI safety include robustness, monitoring, and capability control . [ 113 ] Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. [ 113 ] Alignment research has connections to interpretability research , [ 114 ] [ 115 ] (adversarial) robustness, [ 116 ] anomaly detection , calibrated uncertainty , [ 114 ] formal verification , [ 117 ] preference learning , [ 118 ] [ 119 ] [ 120 ] safety-critical engineering , [ 121 ] game theory , [ 122 ] algorithmic fairness , [ 116 ] [ 123 ] and social sciences . [ 124 ] [ 125 ] Systemic safety and sociotechnical factors It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents . [ 126 ] Some scholars have suggested that this framework falls short. [ 126 ] For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. [ 126 ] Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways... Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture. [ 126 ] In the broader context of safety engineering , structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework. [ 127 ] Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation. [ 2 ] Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities. [ 128 ] Cyber defense Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. [ 129 ] This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused. [ 8 ] Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency. [ 130 ] Improving institutional decision-making The advancement of AI in economic and military domains could precipitate unprecedented political challenges. [ 131 ] Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe. [ 132 ] AI researchers have argued that AI technologies could also be used to assist decision-making. [ 2 ] For example, researchers are beginning to develop AI forecasting [ 133 ] and advisory systems. [ 134 ] Facilitating cooperation Many of the largest global threats ( nuclear war , [ 135 ] climate change , [ 136 ] etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes. [ 136 ] A salient AI cooperation challenge is avoiding a 'race to the bottom'. [ 137 ] In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political [ 138 ] and technical [ 139 ] efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games). [ 140 ] Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact. [ 140 ] [ 128 ] Challenges of large language models In recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al. [ 141 ] have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem. [ 142 ] [ 143 ] The unique challenges posed by LLMs also extend to security vulnerabilities. These include various manipulation techniques, such as prompt injection , Misinformation Generation and model stealing, [ 144 ] which can be exploited to compromise their intended function. This can allow attackers to bypass safety measures and elicit unintended responses In governance The AI Safety Summit of November 2023 [ 145 ] AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems. [ 132 ] Research AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. [ 146 ] Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment, [ 147 ] weaponization, [ 148 ] disinformation, [ 149 ] surveillance, [ 150 ] and the concentration of power. [ 151 ] Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, [ 152 ] the availability of AI models, [ 153 ] and 'race to the bottom' dynamics. [ 137 ] [ 154 ] Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\". [ 138 ] A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems. [ 155 ] [ 156 ] [ 157 ] A key challenge for these approaches is a lack of widely accepted standards, and ambiguity about what the methods would require. [ 158 ] [ 159 ] Efforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia 's Guardrails, [ 160 ] Llama Guard, [ 161 ] Preamble 's customizable guardrails [ 162 ] and Claude's Constitution mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability. [ 163 ] Philosophical perspectives See also: Ethics of artificial intelligence The field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. By embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries. [ 164 ] Scaling local measures to global solutions In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers [ 165 ] argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide. [ 166 ] [ 167 ] Government action See also: Regulation of artificial intelligence Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\". [ 168 ] [ 169 ] Others, such as business magnate Elon Musk , call for pre-emptive action to mitigate catastrophic risks. [ 170 ] Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\". [ 171 ] Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\". [ 172 ] In September 2021, the People's Republic of China (PRC) published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, [ 173 ] which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\". [ 174 ] The strategy describes actions to assess long-term AI risks, including catastrophic risks. [ 174 ] The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\". [ 175 ] [ 176 ] China Media Project stated \"key aspects of its approach remain fundamentally unsafe by the standards of democratic societies worldwide\", arguing that part of China's AI safety approach is focused on strengthening the CCP 's information control. [ 177 ] Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. [ 178 ] The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks . [ 179 ] [ 180 ] And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research. [ 181 ] In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of \"safe, secure and trustworthy\" AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI. [ 182 ] In May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation . Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit , stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco. [ 183 ] Corporate self-regulation AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. [ 184 ] One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing, [ 185 ] offering bounties for finding failures, [ 185 ] sharing AI incidents [ 185 ] (an AI incident database was created for this purpose), [ 186 ] following guidelines to determine whether to publish research or models, [ 153 ] and improving information and cyber security in AI labs. [ 187 ] Companies have also made commitments. Cohere , OpenAI , and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse. [ 188 ] To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\" [ 189 ] Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles [ 34 ] and the Autonomous Weapons Open Letter. [ 190 ] See also AI alignment Artificial intelligence and elections Artificial intelligence detection software References .mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman} ↑ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}} Ahmed, Shazeda; Jaźwińska, Klaudia; Ahlawat, Archana; Winecoff, Amy; Wang, Mona (2024-04-14). \"Field-building and the epistemic culture of AI safety\" . First Monday . doi : 10.5210/fm.v29i4.13626 . ISSN 1396-0466 . 1 2 3 4 5 6 Hendrycks, Dan; Carlini, Nicholas ; Schulman, John; Steinhardt, Jacob (2022-06-16). \"Unsolved Problems in ML Safety\". arXiv : 2109.13916 . {{ cite journal }} : Cite journal requires | journal= ( help ) ↑ Perrigo, Billy (2023-11-02). \"U.K.'s AI Safety Summit Ends With Limited, but Meaningful, Progress\" . Time . Retrieved 2024-06-02 . ↑ De-Arteaga, Maria (2020-05-13). Machine Learning in High-Stakes Settings: Risks and Opportunities (PhD). Carnegie Mellon University. ↑ Mehrabi, Ninareh; Morstatter, Fred; Saxena, Nripsuta; Lerman, Kristina; Galstyan, Aram (2021). \"A Survey on Bias and Fairness in Machine Learning\" . ACM Computing Surveys . 54 (6): 1– 35. arXiv : 1908.09635 . doi : 10.1145/3457607 . ISSN 0360-0300 . S2CID 201666566 . Archived from the original on 2022-11-23 . Retrieved 2022-11-28 . ↑ Feldstein, Steven (2019). The Global Expansion of AI Surveillance (Report). Carnegie Endowment for International Peace. ↑ Barnes, Beth (2021). \"Risks from AI persuasion\" . Lesswrong . Archived from the original on 2022-11-23 . Retrieved 2022-11-23 . 1 2 3 Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C; Steinhardt, Jacob; Flynn, Carrick (2018-04-30). \"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation\" . Apollo-University Of Cambridge Repository, Apollo-University Of Cambridge Repository. Apollo - University of Cambridge Repository. doi : 10.17863/cam.22520 . S2CID 3385567 . Archived from the original on 2022-11-23 . Retrieved 2022-11-28 . {{ cite journal }} : Cite journal requires | journal= ( help ) ↑ Davies, Pascale (December 26, 2022). \"How NATO is preparing for a new era of AI cyber attacks\" . euronews . Retrieved 2024-03-23 . ↑ Ahuja, Anjana (February 7, 2024). \"AI's bioterrorism potential should not be ruled out\" . Financial Times . Retrieved 2024-03-23 . ↑ Carlsmith, Joseph (2022-06-16). \"Is Power-Seeking AI an Existential Risk?\". arXiv : 2206.13353 . {{ cite journal }} : Cite journal requires | journal= ( help ) ↑ Minardi, Di (16 October 2020). \"The grim fate that could be 'worse than extinction' \" . BBC . Retrieved 2024-03-23 . ↑ \"AGI Expert Peter Voss Says AI Alignment Problem is Bogus | NextBigFuture.com\" . 2023-04-04 . Retrieved 2023-07-23 . ↑ Dafoe, Allan (2016). \"Yes, We Are Worried About the Existential Risk of Artificial Intelligence\" . MIT Technology Review . Archived from the original on 2022-11-28 . Retrieved 2022-11-28 . 1 2 Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain (2018-07-31). \"Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts\" . Journal of Artificial Intelligence Research . 62 : 729– 754. arXiv : 1705.08807 . doi : 10.1613/jair.1.11222 . ISSN 1076-9757 . S2CID 8746462 . Archived from the original on 2023-02-10 . Retrieved 2022-11-28 . ↑ Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan (2021-05-05). \"Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers\". Journal of Artificial Intelligence Research . 71 . arXiv : 2105.0211",
  "cached_at": "2025-10-25T19:35:25.133018"
}