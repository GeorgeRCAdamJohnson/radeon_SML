{
  "title": "Fairness (machine learning)",
  "summary": "Fairness in machine learning (ML) refers to the various attempts to correct algorithmic bias in automated decision processes based on ML models. Decisions made by such models after a learning process may be considered unfair if they were based on variables considered sensitive",
  "content": "Fairness (machine learning) Measurement of algorithmic bias html body.mediawiki body.mediawiki body.skin-minerva This article has multiple issues. Please help improve it or discuss these issues on the talk page . ( Learn how and when to remove these messages ) This article contains instructions or advice . Wikipedia is not a guidebook; please help rewrite such content to be encyclopedic or move it to Wikiversity , Wikibooks , or Wikivoyage . ( December 2019 ) This article may be too technical for most readers to understand . Please help improve it to make it understandable to non-experts , without removing the technical details. ( December 2019 ) ( Learn how and when to remove this message ) ( Learn how and when to remove this message ) Fairness in machine learning (ML) refers to the various attempts to correct algorithmic bias in automated decision processes based on ML models. Decisions made by such models after a learning process may be considered unfair if they were based on variables considered sensitive (e.g., gender, ethnicity, sexual orientation, or disability). As is the case with many ethical concepts, definitions of fairness and bias can be controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. Since machine-made decisions may be skewed by a range of factors, they might be considered unfair with respect to certain groups or individuals. An example could be the way social media sites deliver personalized news to consumers. Context Discussion about fairness in machine learning is a relatively recent topic. Since 2016 there has been a sharp increase in research into the topic. 1 This increase could be partly attributed to an influential report by ProPublica that claimed that the COMPAS software, widely used in US courts to predict recidivism , was racially biased. 2 One topic of research and discussion is the definition of fairness, as there is no universal definition, and different definitions can be in contradiction with each other, which makes it difficult to judge machine learning models. 3 Other research topics include the origins of bias, the types of bias, and methods to reduce bias. 4 In recent years tech companies have made tools and manuals on how to detect and reduce bias in machine learning. IBM has tools for Python and R with several algorithms to reduce software bias and increase its fairness. 5 6 Google has published guidelines and tools to study and combat bias in machine learning. 7 8 Facebook have reported their use of a tool, Fairness Flow, to detect bias in their AI . 9 However, critics have argued that the company's efforts are insufficient, reporting little use of the tool by employees as it cannot be used for all their programs and even when it can, use of the tool is optional. 10 It is important to note that the discussion about quantitative ways to test fairness and unjust discrimination in decision-making predates by several decades the rather recent debate on fairness in machine learning. 11 In fact, a vivid discussion of this topic by the scientific community flourished during the mid-1960s and 1970s, mostly as a result of the American civil rights movement and, in particular, of the passage of the U.S. Civil Rights Act of 1964 . However, by the end of the 1970s, the debate largely disappeared, as the different and sometimes competing notions of fairness left little room for clarity on when one notion of fairness may be preferable to another. Language Bias Language bias refers a type of statistical sampling bias tied to the language of a query that leads to a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository. better source needed 12 Luo et al. 12 show that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like What is liberalism? , ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like opposes state intervention in personal and economic life from the dominant Vietnamese perspective and limitation of government power from the prevalent Chinese perspective are absent. Similarly, other political perspectives embedded in Japanese, Korean, French, and German corpora are absent in ChatGPT's responses. ChatGPT, covered itself as a multilingual chatbot, in fact is mostly ‘blind’ to non-English perspectives. 12 Gender Bias Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms; it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. 13 Another example, utilizes data driven methods to identify gender bias in LinkedIn profiles. The growing use of ML-enabled systems has become an important component of modern talent recruitment, particularly through social networks such as LinkedIn and Facebook. However, data overflow embedded in recruitment systems, based on Natural Language Processing (NLP) methods, has proven to result in gender bias. 14 Political bias Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data. 15 Controversies Main article: Algorithmic bias § Impact The use of algorithmic decision making in the legal system has been a notable area of use under scrutiny. In 2014, then U.S. Attorney General Eric Holder raised concerns that risk assessment methods may be putting undue focus on factors not under a defendant's control, such as their education level or socio-economic displaystyle P(R_ A leftarrow a =1 mid A=a,X=x)=P(R_ A leftarrow b =1 mid A=a,X=x), quad forall a,b; that is: taken a random individual with sensitive attribute A = a displaystyle A=a and other features X = x displaystyle X=x and the same individual if she had A = b displaystyle A=b , they should have same chance of being accepted. The symbol R ^ A ← a displaystyle hat R _ A leftarrow a represents the counterfactual random variable R displaystyle R in the scenario where the sensitive attribute A displaystyle A is fixed to A = a displaystyle A=a . The conditioning on A = a , X = x displaystyle A=a,X=x means that this requirement is at the individual level, in that we are conditioning on all the variables identifying a single observation. Machine learning models are often trained upon data where the outcome depended on the decision made at that time. 47 For example, if a machine learning model has to determine whether an inmate will recidivate and will determine whether the inmate should be released early, the outcome could be dependent on whether the inmate was released early or not. Mishler et al. 48 propose a formula for counterfactual equalized odds: P ( R = 1 ∣ Y 0 = 0 , A = a ) = P ( R = 1 ∣ Y 0 = 0 , A = b ) ∧ P ( R = 0 ∣ Y 1 = 1 , A = a ) = P ( R = 0 ∣ Y 1 = 1 , A = b ) , ∀ a , b ; displaystyle P(R=1 mid Y^ 0 =0,A=a)=P(R=1 mid Y^ 0 =0,A=b) wedge P(R=0 mid Y^ 1 =1,A=a)=P(R=0 mid Y^ 1 =1,A=b), quad forall a,b; where R displaystyle R is a random variable, Y x displaystyle Y^ x denotes the outcome given that the decision x displaystyle x was taken, and A displaystyle A is a sensitive feature. Plecko and Bareinboim 49 propose a unified framework to deal with causal analysis of fairness. They suggest the use of a Standard Fairness Model , consisting of a causal graph with 4 types of variables: sensitive attributes ( A displaystyle A ), target variable ( Y displaystyle Y ), mediators ( W displaystyle W ) between A displaystyle A and Y displaystyle Y , representing possible indirect effects of sensitive attributes on the outcome, variables possibly sharing a common cause with A displaystyle A ( Z displaystyle Z ), representing possible spurious (i.e., non causal) effects of the sensitive attributes on the outcome. Within this framework, Plecko and Bareinboim 49 are therefore able to classify the possible effects that sensitive attributes may have on the outcome. Moreover, the granularity at which these effects are measured—namely, the conditioning variables used to average the effect—is directly connected to the individual vs. group aspect of fairness assessment. Bias mitigation strategies Fairness can be applied to machine learning algorithms in three different ways: data preprocessing , optimization during software training, or post-processing results of the algorithm. Preprocessing Usually, the classifier is not the only problem; the dataset is also biased. The discrimination of a dataset D textstyle D with respect to the group A = a textstyle A=a can be defined as follows: d i s c A = a ( D ) = | X ∈ D | X ( A ) ≠ a , X ( Y ) = + | | X ∈ D | X ( A ) ≠ a | − | X ∈ D | X ( A ) = a , X ( Y ) = + | | X ∈ D | X ( A ) = a | displaystyle disc_ A=a (D)= frac | X in D|X(A) neq a,X(Y)=+ | | X in D|X(A) neq a | - frac | X in D|X(A)=a,X(Y)=+ | | X in D|X(A)=a | That is, an approximation to the difference between the probabilities of belonging in the positive class given that the subject has a protected characteristic different from a textstyle a and equal to a textstyle a . Algorithms correcting bias at preprocessing remove information about dataset variables which might result in unfair decisions, while trying to alter as little as possible. This is not as simple as just removing the sensitive variable, because other attributes can be correlated to the protected one. A way to do this is to map each individual in the initial dataset to an intermediate representation in which it is impossible to identify whether it belongs to a particular protected group while maintaining as much information as possible. Then, the new representation of the data is adjusted to get the maximum accuracy in the algorithm. This way, individuals are mapped into a new multivariable representation where the probability of any member of a protected group to be mapped to a certain value in the new representation is the same as the probability of an individual which doesn't belong to the protected group. Then, this representation is used to obtain the prediction for the individual, instead of the initial data. As the intermediate representation is constructed giving the same probability to individuals inside or outside the protected group, this attribute is hidden to the classifier. An example is explained in Zemel et al. 50 where a multinomial random variable is used as an intermediate representation. In the process, the system is encouraged to preserve all information except that which can lead to biased decisions, and to obtain a prediction as accurate as possible. On the one hand, this procedure has the advantage that the preprocessed data can be used for any machine learning task. Furthermore, the classifier does not need to be modified, as the correction is applied to the dataset before processing. On the other hand, the other methods obtain better results in accuracy and fairness. citation needed Reweighing Reweighing is an example of a preprocessing algorithm. The idea is to assign a weight to each dataset point such that the weighted discrimination is 0 with respect to the designated group. 51 If the dataset D textstyle D was unbiased the sensitive variable A textstyle A and the target variable Y textstyle Y would be statistically independent and the probability of the joint distribution would be the product of the probabilities as follows: P e x p ( A = a ∧ Y = + ) = P ( A = a ) × P ( Y = + ) = | X ∈ D | X ( A ) = a | | D | × | X ∈ D | X ( Y ) = + | | D | displaystyle P_ exp (A=a wedge Y=+)=P(A=a) times P(Y=+)= frac | X in D|X(A)=a | times frac | X in D|X(Y)=+ | In reality, however, the dataset is not unbiased and the variables are not statistically independent so the observed probability is: P o b s ( A = a ∧ Y = + ) = | X ∈ D | X ( A ) = a ∧ X ( Y ) = + | | D | displaystyle P_ obs (A=a wedge Y=+)= frac | X in D|X(A)=a wedge X(Y)=+ | To compensate for the bias, the software adds a weight , lower for favored objects and higher for unfavored objects. For each X ∈ D textstyle X in D we get: W ( X ) = P e x p ( A = X ( A ) ∧ Y = X ( Y ) ) P o b s ( A = X ( A ) ∧ Y = X ( Y ) ) displaystyle W(X)= frac P_ exp (A=X(A) wedge Y=X(Y)) P_ obs (A=X(A) wedge Y=X(Y)) When we have for each X textstyle X a weight associated W ( X ) textstyle W(X) we compute the weighted discrimination with respect to group A = a textstyle A=a as follows: d i s c A = a ( D ) = ∑ W ( X ) X ∈ X ∈ D | X ( A ) ≠ a , X ( Y ) = + ∑ W ( X ) X ∈ X ∈ D | X ( A ) ≠ a − ∑ W ( X ) X ∈ X ∈ D | X ( A ) = a , X ( Y ) = + ∑ W ( X ) X ∈ X ∈ D | X ( A ) = a displaystyle disc_ A=a (D)= frac sum W(X)X in X in D|X(A) neq a,X(Y)=+ sum W(X)X in X in D|X(A) neq a - frac sum W(X)X in X in D|X(A)=a,X(Y)=+ sum W(X)X in X in D|X(A)=a It can be shown that after reweighting this weighted discrimination is 0. Inprocessing Another approach is to correct the bias at training time. This can be done by adding constraints to the optimization objective of the algorithm. 52 These constraints force the algorithm to improve fairness, by keeping the same rates of certain measures for the protected group and the rest of individuals. For example, we can add to the objective of the algorithm the condition that the false positive rate is the same for individuals in the protected group and the ones outside the protected group. The main measures used in this approach are false positive rate, false negative rate, and overall misclassification rate. It is possible to add just one or several of these constraints to the objective of the algorithm. Note that the equality of false negative rates implies the equality of true positive rates so this implies the equality of opportunity. After adding the restrictions to the problem it may turn intractable, so a relaxation on them may be needed. Adversarial debiasing We train two classifiers at the same time through some gradient-based method (f.e.: gradient descent ). The first one, the predictor tries to accomplish the task of predicting Y textstyle Y , the target variable, given X textstyle X , the input, by modifying its weights W textstyle W to minimize some loss function L P ( y ^ , y ) textstyle L_ P ( hat y ,y) . The second one, the adversary tries to accomplish the task of predicting A textstyle A , the sensitive variable, given Y ^ textstyle hat Y by modifying its weights U textstyle U to minimize some loss function L A ( a ^ , a ) textstyle L_ A ( hat a ,a) . 53 An important point here is that, to propagate correctly, Y ^ textstyle hat Y above must refer to the raw output of the classifier, not the discrete prediction; for example, with an artificial neural network and a classification problem, Y ^ textstyle hat Y could refer to the output of the softmax layer . Then we update U textstyle U to minimize L A textstyle L_ A at each training step according to the gradient ∇ U L A textstyle nabla _ U L_ A and we modify W textstyle W according to the expression: ∇ W L P − p r o j ∇ W L A ∇ W L P − α ∇ W L A displaystyle nabla _ W L_ P -proj_ nabla _ W L_ A nabla _ W L_ P - alpha nabla _ W L_ A where α alpha is a tunable hyperparameter that can vary at each time step. Graphic representation of the vectors used in adversarial debiasing as shown in Zhang et al. 53 The intuitive idea is that we want the predictor to try to minimize L P textstyle L_ P (therefore the term ∇ W L P textstyle nabla _ W L_ P ) while, at the same time, maximize L A textstyle L_ A (therefore the term − α ∇ W L A textstyle - alpha nabla _ W L_ A ), so that the adversary fails at predicting the sensitive variable from Y ^ textstyle hat Y . The term − p r o j ∇ W L A ∇ W L P textstyle -proj_ nabla _ W L_ A nabla _ W L_ P prevents the predictor from moving in a direction that helps the adversary decrease its loss function. It can be shown that training a predictor classification model with this algorithm improves demographic parity with respect to training it without the adversary . Postprocessing The final method tries to correct the results of a classifier to achieve fairness. In this method, we have a classifier that returns a score for each individual and we need to do a binary prediction for them. High scores are likely to get a positive outcome, while low scores are likely to get a negative one, but we can adjust the threshold to determine when to answer yes as desired. Note that variations in the threshold value affect the trade-off between the rates for true positives and true negatives. If the score function is fair in the sense that it is independent of the protected attribute, then any choice of the threshold will also be fair, but classifiers of this type tend to be biased, so a different threshold may be required for each protected group to achieve fairness. 54 A way to do this is plotting the true positive rate against the false negative rate at various threshold settings (this is called ROC curve ) and find a threshold where the rates for the protected group and other individuals are equal. 54 Reject option based classification Given a classifier let P ( + | X ) textstyle P(+|X) be the probability computed by the classifiers as the probability that the instance X textstyle X belongs to the positive class +. When P ( + | X ) textstyle P(+|X) is close to 1 or to 0, the instance X textstyle X is specified with high degree of certainty to belong to class + or – respectively. However, when P ( + | X ) textstyle P(+|X) is closer to 0.5 the classification is more unclear. 55 We say X textstyle X is a rejected in",
  "cached_at": "2025-10-25T20:05:34.222472"
}