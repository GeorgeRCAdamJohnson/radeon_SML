{
  "title": "Algorithmic bias",
  "summary": "Algorithmic bias describes systematic and repeatable harmful tendency in a computerized sociotechnical system to create \"unfair\" outcomes, such as \"privileging\" one category over another in ways different from the intended function of the algorithm.",
  "content": "Algorithmic bias Technological phenomenon with social implications &lt;a href=\\\"./Wikipedia:Good_articles*\\\" title=\\\"This is a good article. Click here for more information.\\\" id=\\\"mwBg\\\">&lt;img alt=\\\"This is a good article. Click here for more information.\\\" resource=\\\"./File:Symbol_support_vote.svg\\\" src=\\\"//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/20px-Symbol_support_vote.svg.png\\\" decoding=\\\"async\\\" data-file-width=\\\"180\\\" data-file-height=\\\"185\\\" data-file-type=\\\"drawing\\\" height=\\\"20\\\" width=\\\"19\\\" srcset=\\\"//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/40px-Symbol_support_vote.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/40px-Symbol_support_vote.svg.png 2x\\\" class=\\\"mw-file-element\\\" id=\\\"mwBw\\\"/>&lt;/a>&lt;/span>\\n\"}' id=\"mwCA\"/> A flow chart showing the decisions made by a recommendation engine , c. 2001 [ 1 ] .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}} Part of a series on Artificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Open-source Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy AI alignment Artificial consciousness The bitter lesson Chinese room Friendly AI Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom AI bubble Controversies Deepfake pornography Taylor Swift deepfake pornography controversy Google Gemini image generation controversy Pause Giant AI Experiments Removal of Sam Altman from OpenAI Statement on AI Risk Tay (chatbot) Théâtre D'opéra Spatial Voiceverse NFT plagiarism scandal Glossary Glossary .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e Algorithmic bias describes systematic and repeatable harmful tendency in a computerized sociotechnical system to create \" unfair \" outcomes, such as \"privileging\" one category over another in ways different from the intended function of the algorithm. [ 2 ] Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. [ 3 ] For example, algorithmic bias has been observed in search engine results and social media platforms . This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination. [ 4 ] This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024). As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias ), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design. [ 5 ] Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech . It has also arisen in criminal justice, [ 6 ] healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service. A 2021 survey identified multiple forms of algorithmic bias, including historical, representation, and measurement biases, each of which can contribute to unfair outcomes. [ 7 ] Definitions A 1969 diagram for how a simple computer program makes decisions, illustrating a very simple algorithm Algorithms are difficult to define , [ 8 ] but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output. [ 9 ] : 13 For a rigorous technical introduction, see Algorithms . Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence . [ 10 ] : 14–15 By analyzing and processing data, algorithms are the backbone of search engines, [ 11 ] social media websites, [ 12 ] recommendation engines, [ 13 ] online retail, [ 14 ] online advertising, [ 15 ] and more. [ 16 ] Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality. [ 17 ] : 2 [ 18 ] : 563 [ 19 ] : 294 [ 20 ] The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased . [ 21 ] : 332 This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on). Methods Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria. [ 22 ] : 3 Next, programmers assign priorities, or hierarchies , for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded. [ 22 ] : 4 Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers. [ 22 ] : 8 Other algorithms may reinforce stereotypes and preferences as they process and display \"relevant\" data for human users, for example, by selecting information based on previous choices of a similar user or group of users. [ 22 ] : 6 Beyond assembling and processing data, bias can emerge as a result of design. [ 23 ] For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores). [ 24 ] : 36 Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. These criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths. [ 23 ] Algorithms may also display an uncertainty bias , offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations. [ 25 ] : 4 History Early critiques This card was used to load software into an old mainframe computer. Each byte (the letter 'A', for example) is entered by punching holes. Though contemporary computers are more complex, they reflect this human decision-making process in collecting and processing data. [ 26 ] : 70 [ 27 ] : 16 The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason , artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded. [ 26 ] : 149 Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs \"embody law\", [ 26 ] : 40 that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including their biases and expectations. [ 26 ] : 109 While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects \"human decision making processes\" as data is being selected. [ 26 ] : 70, 105 Finally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results. [ 26 ] : 65 Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable. [ 26 ] : 226 An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions. [ 28 ] While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale. In recent years, as algorithms increasingly rely on machine learning methods applied to real-world data, algorithmic bias has become more prevalent due to inherent biases within the data itself. For instance, facial recognition systems have been shown to misidentify individuals from marginalized groups at significantly higher rates than white individuals, highlighting how biases in training datasets manifest in deployed systems. [ 29 ] A 2018 study by Joy Buolamwini and Timnit Gebru found that commercial facial recognition technologies exhibited error rates of up to 35% when identifying darker-skinned women, compared to less than 1% for lighter-skinned men. [ 30 ] Algorithmic biases are not only technical failures but often reflect systemic inequities embedded in historical and societal data. Researchers and critics, such as Cathy O'Neil in her book Weapons of Math Destruction (2016), emphasize that these biases can amplify existing social inequalities under the guise of objectivity. O'Neil argues that opaque, automated decision-making processes in areas such as credit scoring, predictive policing, and education can reinforce discriminatory practices while appearing neutral or scientific. [ 31 ] Contemporary critiques and responses Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. [ 32 ] The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten. [ 33 ] : 115 In theory, these biases may create new patterns of behavior, or \"scripts\", in relationship to specific technologies as the code interacts with other elements of society. [ 34 ] Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests. [ 35 ] : 180 The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist, [ 36 ] : 15 a process described by author Clay Shirky as \"algorithmic authority\". [ 37 ] Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources\", such as search results. [ 37 ] This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be created based on significantly wider criteria than just their popularity. [ 22 ] : 14 Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans. [ 36 ] : 16 [ 38 ] : 6 This can have the effect of reducing alternative options, compromises, or flexibility. [ 36 ] : 16 Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors. [ 39 ] : 71 While blind adherence to algorithmic decisions is a concern, an opposite issue arises when human decision-makers exhibit \"selective adherence\" to algorithmic advice. In such cases, individuals accept recommendations that align with their preexisting beliefs and disregard those that do not, thereby perpetuating existing biases and undermining the fairness objectives of algorithmic interventions. Consequently, incorporating fair algorithmic tools into decision-making processes does not automatically eliminate human biases. [ 40 ] Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft , which have co-created a working group named Fairness, Accountability, and Transparency in Machine Learning. [ 41 ] : 115 Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences. [ 41 ] : 117 In recent years, the study of the Fairness, Accountability, and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT. [ 42 ] Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied. [ 43 ] NIST's AI Risk Management Framework 1.0 and its 2024 Generative AI Profile provide practical guidance for governing and measuring bias mitigation in AI systems. Types Pre-existing Pre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies . Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious. [ 21 ] : 334 [ 19 ] : 294 Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines. [ 27 ] : 17 Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm. [ 33 ] : 116 [ 38 ] : 8 An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 British Nationality Act . [ 21 ] : 341 The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\" [ 21 ] : 341 [ 44 ] : 375 In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed. [ 21 ] : 342 Another source of bias, which has been called \"label choice bias\", [ 45 ] arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients [ 46 ] Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program. [ 45 ] Machine learning bias Machine learning bias refers to systematic and unfair disparities in the output of machine learning algorithms. These biases can manifest in various ways and are often a reflection of the data used to train these algorithms. Here are some key aspects: Language bias Language bias refers a type of statistical sampling bias tied to the language of a query that leads to \"a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository.\" [ 47 ] Luo et al.'s work [ 47 ] shows that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent. [ 47 ] Similarly, language models may exhibit bias against people within a language group based on the specific dialect they use. [ 48 ] Selection bias Selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias—that is, the model assigns a higher a priori probability to specific answer tokens (such as \"A\") when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model's performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings. [ 49 ] [ 50 ] Gender bias Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms; it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. [ 51 ] [ 52 ] Stereotyping Beyond gender and race, these models can reinforce a wide range of stereotypes , including those based on age, nationality, religion, or occupation. This can lead to outputs that homogenize, or unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways. [ 53 ] [ 54 ] A recent focus in research has been on the complex interplay between the grammatical properties of a language and real-world biases that can become embedded in AI systems, potentially perpetuating harmful stereotypes and assumptions. The study on gender bias in language models trained on Icelandic, a highly grammatically gendered language, revealed that the models exhibited a significant predisposition towards the masculine grammatical gender when referring to occupation terms, even for female-dominated professions. [ 55 ] This suggests the models amplified societal gender biases present in the training data. Political bias Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data. [ 56 ] [ 57 ] Racial bias Racial bias refers to the tendency of machine learning models to produce outcomes that unfairly discriminate against or stereotype individuals based on race or ethnicity. This bias often stems from training data that reflects historical and systemic inequalities. For example, AI systems used in hiring, law enforcement, or healthcare may disproportionately disadvantage certain racial groups by reinforcing existing stereotypes or underrepresenting them in key areas. Such biases can manifest in ways like facial recognition systems misidentifying individuals of certain racial backgrounds or healthcare algorithms underestimating the medical needs of minority patients. Addressing racial bias requires careful examination of data, improved transparency in algorithmic processes, and efforts to ensure fairness throughout the AI development lifecycle. [ 58 ] [ 59 ] Technical Facial recognition software used in conjunction with surveillance cameras was found to display bias in recognizing Asian and black faces over white faces. [ 35 ] : 191 Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. [ 21 ] : 332 Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. [ 21 ] : 336 Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list. [ 21 ] : 332 A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. [ 21 ] : 332 The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision . This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime. [ 18 ] : 574 Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury. [ 21 ] : 332 Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin , which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection. [ 36 ] : 21–22 Emergent Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. [ 21 ] : 334 Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms. [ 21 ] : 334, 336 This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion. [ 35 ] : 179 [ 19 ] : 294 Similarly, problems may emerge when training data (the samples \"fed\" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world. [ 60 ] In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). [ 21 ] : 338 The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference. [ 21 ] : 338 [ 61 ] Additional emergent biases include: Correlations Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data. [ 25 ] : 6 In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care. [ 62 ] [ clarification needed ] Unanticipated uses Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand. [ 21 ] : 334 These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society. [ 35 ] : 179 Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship . The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law. [ 21 ] : 342 Feedback loops Emergent bias may also create a feedback loop , or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. [ 63 ] [ 64 ] For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. [ 65 ] The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. [ 63 ] [ 66 ] [ 67 ] The Human Rights Data Analysis Group , which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing. [ 64 ] Another well known example of such an algorithm exhibiting such behavior is COMPAS , a software that determines an individual's likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on. [ 68 ] Recommender systems such as those used to recommend online videos or news articles can create feedback loops. [ 69 ] When users click on content that is suggested by algorithms, it influences the next set of suggestions. [ 70 ] Over time this may lead to users entering a filter bubble and being unaware of important or useful content. [ 71 ] [ 72 ] Impact Commercial influences Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress , the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment. [ 73 ] : 2 [ 21 ] : 331 In a 1998 paper describing Google , the founders of the company had adopted a policy of transparency in search results regarding paid placement, arguing that \"advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\" [ 74 ] This bias would be an \"invisible\" manipulation of the user. [ 73 ] : 3 Voting behavior A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate. [ 75 ] Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. [ 76 ] Legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users\", if intentionally manipulated. [ 77 ] : 335 Gender discrimination In 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for men's names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew\", but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site. [ 78 ] In 2012, the department store franchise Target was cited for gathering data points to infer when female customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners. [ 79 ] : 94 [ 80 ] Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers. [ 79 ] : 98 Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\". [ 81 ] : 31 In 2017, Google adjusted these results along with others that surfaced hate groups , racist views, child abuse and pornography, and other upsetting and offensive content. [ 82 ] Other examples include the display of higher-paying jobs to male applicants on job search websites. [ 83 ] Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. [ 84 ] In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations. [ 85 ] In fact, current machine translation systems fail to reproduce the real world distribution of female workers. [ 86 ] In 2015, Amazon.com turned off an AI system it developed to screen job applications when they realized it was biased against women. [ 87 ] The recruitment tool excluded applicants who attended all-women's colleges and resumes that included the word \"women's\". [ 88 ] A similar problem emerged with music streaming services—In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against female artists. [ 89 ] Spotify's song recommendations suggested more male artists over female artists. Racial and ethnic discrimination Algorithms have been criticized as a method for obscuring racial prejudices in decision-making. [ 90 ] [ 91 ] [ 92 ] : 158 Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. [ 93 ] For example, black people are likely to receive longer sentences than white people who committed the same crime. [ 94 ] [ 95 ] This could potentially mean that a system amplifies the original biases in the data. In 2015, Google apologized when a couple of black users complained that an image-identification algorithm in its Photos application identified them as gorillas . [ 96 ] In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. [ 97 ] Such examples are the product of bias in biometric data sets. [ 96 ] Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points. [ 92 ] : 154 Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent. [ 98 ] Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name. [ 99 ] A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm's model of lung function. [ 100 ] [ 101 ] In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases. [ 102 ] A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on \"creditworthiness\" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities. [ 103 ] [ non-primary source needed ] Another study, published in August 2024, on Large language model investigates how language models perpetuate covert racism, particularly through dialect prejudice against speakers of African American English (AAE). It highlights that these models exhibit more negative stereotypes about AAE speakers than any recorded human biases, while their overt stereotypes are more positive. This discrepancy raises concerns about the potential harmful consequences of such biases in decision-making processes. [ 104 ] A study published by the Anti-Defamation League in 2025 found that several major LLMs, including ChatGPT , Llama , Claude , and Gemini showed anti-Israel bias. [ 105 ] A 2018 study found that commercial gender classification systems had significantly higher error rates for darker-skinned women, with error rates up to 34.7%, compared to near-perfect accuracy for lighter-skinned men. [ 106 ] Law enforcement and legal proceedings Algorithms already have numerous applications in legal systems. An example of this is COMPAS , a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist . ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label \"high-risk\" as white defendants. [ 107 ] [ 108 ] One example is the use of risk assessments in criminal sentencing in the United States and parole hearings , judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. [ 109 ] For the time period starting in 1920 and ending in 1970, the nationality of a criminal's father was a consideration in those risk assessment scores. [ 110 ] : 4 Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites. [ 109 ] One study that set out to examine \"Risk, Race, &amp; Recidivism: Predictive Bias and Disparate Impact\" alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation. [ 111 ] In the pretrial detention context, a law review article argues that algorithmic risk assessments violate 14th Amendment Equal Protection rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored. [ 112 ] Online hate speech In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. [ 113 ] The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks\", whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. [ 113 ] Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent",
  "cached_at": "2025-10-25T19:38:35.416089"
}