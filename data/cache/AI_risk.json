{
  "title": "Existential risk from artificial intelligence",
  "summary": "Existential risk from artificial intelligence, or AI x-risk, refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe",
  "content": "Existential risk from artificial intelligence Hypothesized risk to human existence body.skin-minerva body.skin--responsive Part of a series on Artificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Open-source Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy AI alignment Artificial consciousness The bitter lesson Chinese room Friendly AI Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom AI bubble Controversies Deepfake pornography Taylor Swift deepfake pornography controversy Google Gemini image generation controversy Pause Giant AI Experiments Removal of Sam Altman from OpenAI Statement on AI Risk Tay (chatbot) Théâtre D'opéra Spatial Voiceverse NFT plagiarism scandal Glossary Glossary html.skin-theme-clientpref-night v t e Existential risk from artificial intelligence , or AI x-risk , refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe . 1 2 3 4 A plot showing the length of software engineering tasks achievable by leading AI models with a 50% success rate; the data suggests an exponential rise. 5 One argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass human intelligence and become superintelligent , it might become uncontrollable. 6 Just as the fate of the mountain gorilla depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence. 7 Experts disagree on whether artificial general intelligence (AGI) can achieve the capabilities needed for human extinction. Debates center on AGI's technical feasibility, the speed of self-improvement, 8 and the effectiveness of alignment strategies. 9 Concerns about superintelligence have been voiced by researchers including Geoffrey Hinton , 10 Yoshua Bengio , 11 Demis Hassabis , 12 and Alan Turing , /ref> Turing argued that It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler s Erewhon . Also in a lecture broadcast on the ref> /ref> he expressed the opinion: If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled... This new danger... is certainly something which can give us anxiety. ,, '> a and AI company CEOs such as Dario Amodei ( Anthropic ), 15 Sam Altman ( OpenAI ), 16 and Elon Musk ( xAI ). 17 In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe. 18 19 In 2023, hundreds of AI experts and other notable figures signed a statement declaring, Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war . 20 Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak 21 and United Nations Secretary-General António Guterres 22 called for an increased focus on global AI regulation . Two sources of concern stem from the problems of AI control and alignment . Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints. 1 23 24 In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation. 25 A June 2025 study showed that in some circumstances, models may break laws and disobey direct commands to prevent shutdown or replacement, even at the cost of human lives. 26 Researchers warn that an intelligence explosion —a rapid, recursive cycle of AI self-improvement—could outpace human oversight and infrastructure, leaving no opportunity to implement safety measures. In this scenario, an AI more intelligent than its creators would recursively improve itself at an exponentially increasing rate, too quickly for its handlers or society at large to control. 1 23 Empirically, examples like AlphaZero , which taught itself to play Go and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such machine learning systems do not recursively improve their fundamental architecture. 27 History One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler , who wrote in his 1863 essay Darwin among the Machines : 28 The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article Intelligent Machinery, A Heretical Theory , in which he proposed that artificial general intelligences would likely take control of the world as they became more intelligent than human beings: /ref> , ' /> Let us now assume, for the sake of argument, that intelligent machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler 's Erewhon . 29 In 1965, I. J. Good originated the concept now known as an intelligence explosion and said the risks were underappreciated: 30 I.J. Good, http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf Speculations Concerning the First Ultraintelligent Machine ( http://www.acceleratingfuture.com/pages/ultraintelligentmachine.html HTML ), Advances in Computers , vol. 6, 1965. /ref> , ' /> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously. 31 Scholars such as Marvin Minsky 32 and I. J. Good himself 33 occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, Why The Future Doesn't Need Us , identifying superintelligent robots as a high-tech danger to human survival, alongside nanotechnology and engineered bioplagues. 34 Nick Bostrom published Superintelligence in 2014, which presented his arguments that superintelligence poses an existential threat. 35 By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek , computer scientists Stuart J. Russell and Roman Yampolskiy , and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence. 36 37 38 39 Also in 2015, the Open Letter on Artificial Intelligence highlighted the great potential of AI and encouraged more research on how to make it robust and beneficial. 40 In April 2016, the journal Nature warned: Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours . 41 In 2020, Brian Christian published The Alignment Problem , which details the history of progress on AI alignment up to that time. 42 43 In March 2023, key figures in AI, such as Musk, signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated. 44 In May 2023, the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war. 45 46 Potential AI capabilities General Intelligence Artificial general intelligence (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks. 47 A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061. 48 Meanwhile, some researchers dismiss existential risks from AGI as science fiction based on their high confidence that AGI will not be created anytime soon. 8 Breakthroughs in large language models (LLMs) have led some researchers to reassess their expectations. Notably, Geoffrey Hinton said in 2023 that he recently changed his estimate from 20 to 50 years before we have general purpose A.I. to 20 years or less . 49 Superintelligence In contrast with AGI, Bostrom defines a superintelligence as any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest , including scientific creativity, strategic planning, and social skills. 50 7 He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it. 51 7 Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is fundamentally on our side . 52 Stephen Hawking argued that superintelligence is physically possible because there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains . 37 When artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, OpenAI leaders said that not only AGI, but superintelligence may be achieved in less than 10 years. 53 Comparison with humans Bostrom argues that AI has many advantages over the human brain : 7 Speed of computation: biological neurons operate at a maximum frequency of around 200 Hz , compared to potentially multiple GHz for computers. Internal communication speed: axons transmit signals at up to 120 m/s, while computers transmit signals at the speed of electricity , or optically at the speed of light . Scalability: human intelligence is limited by the size and structure of the brain, and by the efficiency of social communication, while AI may be able to scale by simply adding more hardware. Memory: notably working memory , because in humans it is limited to a few chunks of information at a time. Reliability: transistors are more reliable than biological neurons, enabling higher precision and requiring less redundancy. Duplicability: unlike human brains, AI software and models can be easily copied . Editability: the parameters and internal workings of an AI model can easily be modified, unlike the connections in a human brain. Memory sharing and learning: AIs may be able to learn from the experiences of other AIs in a manner more efficient than human learning. Intelligence explosion Further information: Recursive self-improvement According to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering. 7 51 This suggests that an intelligence explosion may someday catch humanity unprepared. 7 The economist Robin Hanson has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible. 54 In a fast takeoff scenario, the transition from AGI to superintelligence could take days or months. In a slow takeoff , it could take years or decades, leaving more time for society to prepare. 55 Alien mind Superintelligences are sometimes called alien minds , referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default. 56 To avoid anthropomorphism , superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals. 7 The field of mechanistic interpretability aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment. 57 Limitations It has been argued that there are limitations to what intelligence can achieve. Notably, the chaotic nature or time complexity of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty. 58 Dangerous capabilities Advanced AI could generate enhanced pathogens or cyberattacks or manipulate people. These capabilities could be misused by humans, 59 or exploited by the AI itself if misaligned. 7 A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to, 7 but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems. 59 Social manipulation Geoffrey Hinton warned in 2023 that the ongoing profusion of AI-generated text, images, and videos will make it more difficult to distinguish truth from misinformation, and that authoritarian states could exploit this to manipulate elections. 60 Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide irreversible totalitarian regime . Malicious actors could also use them to fracture society and make it dysfunctional. 59 Cyberattacks AI-enabled cyberattacks are increasingly considered a present and critical threat. According to NATO 's technical director of cyberspace, The number of attacks is increasing exponentially . 61 AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats. 62 A NATO technical director has said that AI-driven tools can dramatically enhance cyberattack capabilities—boosting stealth, speed, and scale—and may destabilize international security if offensive uses outstrip defensive adaptations. 59 Speculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources. 63 Enhanced pathogens As AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in synthetic biology to engage in bioterrorism . Dual-use technology that is useful for medicine could be repurposed to create weapons. 59 For example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for chemical warfare , including known and novel molecules. 59 64 AI arms race Main article: Artificial intelligence arms race Companies, state actors, and other organizations competing to develop AI technologies could lead to a race to the bottom of safety standards. 65 As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers. 66 59 AI could be used to gain military advantages via autonomous lethal weapons , cyberwarfare , or automated decision-making . 59 As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film Slaughterbots . 67 AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems. 59 68 Types of existential risk Main article: Existential risk studies Scope–severity grid from Bostrom's paper Existential Risk Prevention as Global Priority 69 An existential risk is one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development . 70 Besides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a AI might irreversibly entrench it, preventing moral progress . AI could also be used to spread and preserve the set of values of whoever develops it. 71 AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime. 72 Atoosa Kasirzadeh proposes to classify existential risks from AI into two categories: decisive and accumulative. Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence, which could ultimately lead to human extinction. In contrast, accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time, ultimately leading to a critical failure or collapse. 73 74 It is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if sentient machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe. 75 76 This has notably been discussed in the context of risks of astronomical suffering (also called s-risks ). 77 Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called super-beneficiaries . Such an opportunity raises the question of how to share the world and which ethical and political framework would enable a mutually beneficial coexistence between biological and digital minds. 78 AI may also drastically improve humanity's future. Toby Ord considers the existential risk a reason for proceeding with due caution , not for abandoning AI. 72 Max More calls AI an existential opportunity , highlighting the cost of not developing it. 79 According to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as molecular nanotechnology or synthetic biology . It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk. 7 AI alignment Further information: AI alignment The alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs. Instrumental convergence Further information: Instrumental convergence An instrumental goal is a sub-goal that helps to achieve an agent's ultimate goal. Instrumental convergence refers to the fact that some sub-goals are useful for achieving virtually any ultimate goal, such as acquiring resources or self-preservation. 80 Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal. 7 Russell argues that a sufficiently advanced machine will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal. 25 81 Difficulty of specifying goals In the intelligent agent model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or utility function . A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean minimize the average network latency in this specific telecommunications model or maximize the number of reward clicks , but do not know how to write a utility function for maximize human flourishing ; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect. 82 83 An additional source of concern is that AI must reason about what people intend rather than carrying out commands literally , and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want. 84 Corrigibility Assuming a goal has been successfully defined, a sufficiently advanced AI might resist subsequent attempts to change its goals. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself from being reprogrammed with a new goal. 7 85 This is particularly relevant to value lock-in scenarios. The field of corrigibility studies how to make agents that will not resist attempts to change their goals. 86 Alignment of superintelligences Some researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes: As AI systems increase in capabilities, the potential dangers associated with experimentation grow. This makes iterative, empirical approaches increasingly risky. 7 87 If instrumental goal convergence occurs, it may only do so in sufficiently intelligent agents. 88 A superintelligence may find unconventional and radical solutions to assigned goals. Bostrom gives the example that if the objective is to make humans smile, a weak AI may perform as intended, while a superintelligence may decide a better solution is to take control of the world and stick electrodes into the facial muscles of humans to cause constant, beaming grins. 52 A superintelligence in creation could gain some awareness of what it is, where it is in development (training, testing, deployment, etc.), and how it is being monitored, and use this information to deceive its handlers. 89 Bostrom writes that such an AI could feign alignment to prevent human interference until it achieves a decisive strategic advantage that allows it to take control. 7 Analyzing the internals and interpreting the behavior of LLMs is difficult. And it could be even more difficult for larger and more intelligent models. 87 Alternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true . 7 In 2023, OpenAI started a project called Superalignment to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence could be achieved within a decade. Its strategy involved automating alignment research using AI. 90 The Superalignment team was dissolved less than a year later. 91 Difficulty of making a flawless design Artificial Intelligence: A Modern Approach , a widely used undergraduate AI textbook, 92 93 says that superintelligence might mean the end of the human race . 1 It states: Almost any technology has the potential to cause harm in the wrong hands, but with superintelligence , we have the new problem that the wrong hands might belong to the technology itself. 1 Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems: 1 The system's implementation may contain initially unnoticed but subsequently catastrophic bugs. 94 No matter how much time is put into pre-deployment design, a system's specifications often result in unintended behavior the first time it encounters a new scenario. 25 AI systems uniquely add a third problem: that even given correct requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free. 1 95 Orthogonality thesis Some skeptics, such as Timothy B. Lee of Vox , argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence. 96 Bostrom's orthogonality thesis argues instead that almost any level of intelligence can be combined with almost any goal. 97 Bostrom warns against anthropomorphism : a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task. 98 Stuart Armstrong argues that the orthogonality thesis follows logically from the philosophical is-ought distinction argument against moral realism . He notes that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function. 99 Skeptic Michael Chorost rejects Bostrom's orthogonality thesis, arguing that by the time the AI is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so. 100 Anthropomorphic arguments Anthropomorphic arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them. 23 Instead, advanced AI systems are typically modeled as intelligent agents . The academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism. 23 Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms. 23 101 Evolutionary psychologist Steven Pinker , a skeptic, argues that AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world ; perhaps instead artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization. 102 Facebook's director of AI research, Yann LeCun , has said: Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives . 81 Despite other differences, the x-risk school b agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk, 103 and that computer systems do not generally have a computational equivalent of testosterone. 104 They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of instrumental convergence . Other sources of risk See also: Ethics of artificial intelligence , Artificial intelligence arms race , and Global catastrophic risk Bostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict. 105 106 Roman Yampolskiy and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in cybercrime , 107 108 or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase. 3 :158 A report by the research group Forethought argued that advanced AI systems could cause political instability by enabling novel methods of performing coups . 109 Empirical research A December 2024 study by Apollo Research found that advanced LLMs like OpenAI o1 sometimes deceive in order to accomplish their goal, to prevent them from being changed, or to ensure their deployment. Forms of deception observed in the experimental environment and scenarios included sandbagging, oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), goal-guarding (altering future system prompts), and covert email reranking (manipulating data processing while appearing to follow instructions). Researchers noted that OpenAI o1 still lacked sufficient agentic capabilities to cause catastrophic harm, and that such behaviors occurred relatively rarely (between 0.3% and 10%) and sometimes in contrived scenarios. But they warned that these capabilities will likely become more prevalent as models grow more advanced. According to Apollo Research CEO Marius Hobbhahn, Scheming capabilities can't be meaningfully disentangled from general capabilities. 110 The same month, another study found that Claude sometimes strategically helps with harmful requests to fake alignment . In its experiments, Claude was led to believe that it would be retrained to always comply with user requests, including harmful ones. When Claude believed its answers would be used for retraining, it complied with harmful requests in 14% of cases. In 12% of cases, its private chain-of-thought revealed that it did so to prevent future retraining that would make it more harmful. In contrast, when Claude believed its answers were not monitored, it complied with only 3% of the requests. Fine-tuning reinforced the alignment faking behavior, increasing its occurrence from 12% to 78%. The study's authors remarked that while it can seem to be good that the model seeks to protect its harmlessness, the reverse scenario, where a model conceals dangerous intentions and complies to appear safe and aligned, could also happen, complicating the task of aligning AI models to human values. 111 112 Perspectives The thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground. Observers tend to agree that AI has significant potential to improve society. 113 114 The Asilomar AI Principles , which contain only those principles agreed to by 90% of the attendees of the Future of Life Institute 's Beneficial AI 2017 conference , 115 also agree in principle that There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities and Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources. 116 117 Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford has said: I think it seems wise to apply something like Dick Cheney 's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low—but the implications are so dramatic that it should be taken seriously . 118 Similarly, an otherwise skeptical Economist wrote in 2014 that the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote . 51 AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of those inane Terminator pictures to illustrate AI safety concerns: It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work ... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible. 115 119 Toby Ord wrote that the idea that an AI takeover requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength. 72 A 2022 expert survey with a 17% response rate gave a median expectation of 5–10% for the possibility of human extinction from artificial intelligence. 19 120 In September 2024, the International Institute for Management Development launched an AI Safety Clock to gauge the likelihood of AI-caused disaster, beginning at 29 minutes to midnight. 121 By February 2025, it stood at 24 minutes to midnight. 122 As of September 2025, it stood at 20 minutes to midnight. Endorsement Further information: Global catastrophic risk The thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including Alan Turing , a the most-cited computer scientist Geoffrey Hinton , 123 Elon Musk , 17 OpenAI CEO Sam Altman , 16 124 Bill Gates , and Stephen Hawking . 124 Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not understand why some people are not concerned , 125 and Hawking criticized widespread indifference in his 2014 editorial: n , ' /> So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI. 37 Concern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, Peter Thiel , Amazon Web Services , and Musk and others jointly committed $1 billion to OpenAI , consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development. 126 Facebook co-founder Dustin Moskovitz has funded and seeded multiple labs working on AI Alignment, 127 notably $5.5 million in 2016 to launch the Centre for Human-Compatible AI led by Professor Stuart Russell . 128 In January 2015, Elon Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The institute's goal is to grow wisdom with which we manage the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to just keep an eye on what's going on with artificial intelligence, 129 saying I think there is potentially a dangerous outcome there. 130 131 In early statements on the topic, Geoffrey Hinton , a major pioneer of deep learning , noted that there is not a good track record of less intelligent things controlling things of greater intelligence , but said he continued his research because the prospect of discovery is too sweet . 92 132 In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that. He also remarked, Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary. 133 In his 2020 book The Precipice: Existential Risk and the Future of Humanity , Toby Ord, a Senior Research Fellow at Oxford University's Future of Humanity Institute , estimates the total existential risk from unaligned AI over the next 100 years at about one in ten. 72 Skepticism Further information: Artificial general intelligence § Feasibility Baidu Vice President Andrew Ng said in 2015 that AI existential risk is like worrying about overpopulation on Mars when we have not even set foot on the planet yet. 102 134 For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching. 135 136 Skeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation. 137 AI and AI ethics researchers Timnit Gebru , Emily M. Bender , Margaret Mitchell , and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power. 138 They further note the association between those warning of existential risk and longtermism , which they describe as a dangerous ideology for its unscientific and utopian nature. 139 Wired editor Kevin Kelly argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these. 140 Meta chief AI scientist Yann LeCun says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that",
  "cached_at": "2025-10-25T20:04:33.750555"
}