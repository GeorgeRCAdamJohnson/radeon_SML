{
  "title": "Friendly artificial intelligence",
  "summary": "Friendly artificial intelligence is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests such as fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained.",
  "content": "Friendly artificial intelligence AI to benefit humanity .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}} Part of a series on Artificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Open-source Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy AI alignment Artificial consciousness The bitter lesson Chinese room Friendly AI Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom AI bubble Controversies Deepfake pornography Taylor Swift deepfake pornography controversy Google Gemini image generation controversy Pause Giant AI Experiments Removal of Sam Altman from OpenAI Statement on AI Risk Tay (chatbot) Théâtre D'opéra Spatial Voiceverse NFT plagiarism scandal Glossary Glossary .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e Friendly artificial intelligence ( friendly AI or FAI ) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests such as fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics . While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained. Etymology and usage Eliezer Yudkowsky , AI researcher and creator of the term The term was coined by Eliezer Yudkowsky , [ 1 ] who is best known for popularizing the idea, [ 2 ] [ 3 ] to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig 's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach , describes the idea: [ 2 ] Yudkowsky (2008) goes into more detail about how to design a Friendly AI . He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design — to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes. \"Friendly\" is used in this context as technical terminology , and picks out agents that are safe and useful, not necessarily ones that are \"friendly\" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly explode in intelligence , on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society. [ 4 ] Risks of unfriendly AI .mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}} Main article: Existential risk from artificial general intelligence The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem , or the proto-robots of Gerbert of Aurillac and Roger Bacon . In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict. [ 5 ] By 1942 these themes prompted Isaac Asimov to create the \" Three Laws of Robotics \"—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm. [ 6 ] In modern times as the prospect of superintelligent AI looms nearer, philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity. He put it this way: Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.' In 2008, Eliezer Yudkowsky called for the creation of \"friendly AI\" to mitigate existential risk from advanced artificial intelligence . He explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\" [ 7 ] Steve Omohundro says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of basic \"drives\" , such as resource acquisition, self-preservation , and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, \"without special precautions\", cause the AI to exhibit undesired behavior. [ 8 ] [ 9 ] Alexander Wissner-Gross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold. [ 10 ] [ 11 ] Luke Muehlhauser, writing for the Machine Intelligence Research Institute , recommends that machine ethics researchers adopt what Bruce Schneier has called the \"security mindset\": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm. [ 12 ] In 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI'; [ 13 ] nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable. [ 14 ] [ 15 ] Coherent extrapolated volition Main article: Coherent extrapolated volition Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, our coherent extrapolated volition is \"our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted\". [ 16 ] Rather than a Friendly AI being designed directly by human programmers, it is to be designed by a \"seed AI\" programmed to first study human nature and then produce the AI that humanity would want, given sufficient time and insight, to arrive at a satisfactory answer. [ 16 ] The appeal to an objective through contingent human nature (perhaps expressed, for mathematical purposes, in the form of a utility function or other decision-theoretic formalism), as providing the ultimate criterion of \"Friendliness\", is an answer to the meta-ethical problem of defining an objective morality ; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity. Other approaches See also: AI control problem § Alignment , and AI safety Steve Omohundro has proposed a \"scaffolding\" approach to AI safety , in which one provably safe AI generation helps build the next provably safe generation. [ 17 ] Seth Baum argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, \"existing messages about beneficial AI are not always framed well\". Baum advocates for \"cooperative relationships, and positive framing of AI researchers\" and cautions against characterizing AI researchers as \"not want(ing) to pursue beneficial designs\". [ 18 ] In his book Human Compatible , AI researcher Stuart J. Russell lists three principles to guide the development of beneficial machines. He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers. The principles are as follows: [ 19 ] : 173 .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} The machine's only objective is to maximize the realization of human preferences. The machine is initially uncertain about what those preferences are. The ultimate source of information about human preferences is human behavior. The \"preferences\" Russell refers to \"are all-encompassing; they cover everything you might care about, arbitrarily far into the future.\" [ 19 ] : 173 Similarly, \"behavior\" includes any choice between options, [ 19 ] : 177 and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference. [ 19 ] : 201 Public policy James Barrat , author of Our Final Invention , suggested that \"a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency , but in partnership with corporations.\" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA , which discussed risks of biotechnology . [ 17 ] John McGinnis encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the National Institutes of Health , where \"Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards.\" McGinnis feels that peer review is better \"than regulation to address technical issues that are not possible to capture through bureaucratic mandates\". McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute , which generally aims to avoid government involvement in friendly AI. [ 20 ] Criticism See also: Technological singularity § Criticisms Some critics believe that both human-level AI and superintelligence are unlikely and that, therefore, friendly AI is unlikely. Writing in The Guardian , Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty and states that while we need to be \"cautious and prepared\" given the stakes involved, we \"don't need to be obsessing\" about the risks of superintelligence. [ 21 ] Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and Nick Bostrom 's proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that human beings would have had. [ 13 ] In an article in AI &amp; Society , Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent. [ 14 ] Some philosophers claim that any truly \"rational\" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful. [ 22 ] Other critics question whether artificial intelligence can be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal The New Atlantis , say that it will be impossible ever to guarantee \"friendly\" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work \"only when one has not only great powers of prediction about the likelihood of myriad possible outcomes but certainty and consensus on how one values the different outcomes. [ 23 ] The inner workings of advanced AI systems may be complex and difficult to interpret, leading to concerns about transparency and accountability. [ 24 ] See also .mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column} Affective computing AI alignment AI effect AI takeover Ambient intelligence Applications of artificial intelligence Artificial intelligence arms race Artificial intelligence systems integration Autonomous agent Embodied agent Emotion recognition Existential risk from artificial general intelligence Hallucination (artificial intelligence) Hybrid intelligent system Intelligence explosion Intelligent agent Intelligent control Machine ethics Machine Intelligence Research Institute OpenAI Regulation of algorithms Roko's basilisk Sentiment analysis Singularitarianism – a moral philosophy advocated by proponents of Friendly AI Suffering risks Technological singularity Three Laws of Robotics References .mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman} ↑ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}} Tegmark, Max (2014). \"Life, Our Universe and Everything\". Our Mathematical Universe: My Quest for the Ultimate Nature of Reality (First ed.). Knopf Doubleday Publishing. ISBN 978-0-307-74425-8 . Its owner may cede control to what Eliezer Yudkowsky terms a \"Friendly AI,\"... 1 2 Russell, Stuart ; Norvig, Peter (2009). Artificial Intelligence: A Modern Approach . Prentice Hall. ISBN 978-0-13-604259-4 . ↑ Leighton, Jonathan (2011). The Battle for Compassion: Ethics in an Apathetic Universe . Algora. ISBN 978-0-87586-870-7 . ↑ Wallach, Wendell; Allen, Colin (2009). Moral Machines: Teaching Robots Right from Wrong . Oxford University Press, Inc. ISBN 978-0-19-537404-9 . ↑ Kevin LaGrandeur (2011). \"The Persistent Peril of the Artificial Slave\" . Science Fiction Studies . 38 (2): 232. doi : 10.5621/sciefictstud.38.2.0232 . Archived from the original on January 13, 2023 . Retrieved May 6, 2013 . ↑ Isaac Asimov (1964). \"Introduction\" . The Rest of the Robots . Doubleday. ISBN 0-385-09041-2 . {{ cite book }} : ISBN / Date incompatibility ( help ) ↑ Eliezer Yudkowsky (2008). \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" (PDF) . In Nick Bostrom; Milan M. Ćirković (eds.). Global Catastrophic Risks . pp. 308– 345. Archived (PDF) from the original on October 19, 2013 . Retrieved October 19, 2013 . ↑ Omohundro, S. M. (February 2008). \"The basic AI drives\". Artificial General Intelligence . 171 : 483– 492. CiteSeerX 10.1.1.393.8356 . ↑ Bostrom, Nick (2014). \"Chapter 7: The Superintelligent Will\". Superintelligence: Paths, Dangers, Strategies . Oxford: Oxford University Press. ISBN 978-0-19-967811-2 . ↑ Dvorsky, George (April 26, 2013). \"How Skynet Might Emerge From Simple Physics\" . Gizmodo . Archived from the original on October 8, 2021 . Retrieved December 23, 2021 . ↑ Wissner-Gross, A. D. ; Freer, C. E. (2013). \"Causal entropic forces\" . Physical Review Letters . 110 (16) 168702. Bibcode : 2013PhRvL.110p8702W . doi : 10.1103/PhysRevLett.110.168702 . hdl : 1721.1/79750 . PMID 23679649 . ↑ Muehlhauser, Luke (July 31, 2013). \"AI Risk and the Security Mindset\" . Machine Intelligence Research Institute . Archived from the original on July 19, 2014 . Retrieved July 15, 2014 . 1 2 Muehlhauser, Luke; Bostrom, Nick (December 17, 2013). \"Why We Need Friendly AI\". Think . 13 (36): 41– 47. doi : 10.1017/s1477175613000316 . ISSN 1477-1756 . S2CID 143657841 . 1 2 Boyles, Robert James M.; Joaquin, Jeremiah Joven (July 23, 2019). \"Why friendly AIs won't be that friendly: a friendly reply to Muehlhauser and Bostrom\". AI &amp; Society . 35 (2): 505– 507. doi : 10.1007/s00146-019-00903-0 . ISSN 0951-5666 . S2CID 198190745 . ↑ Chan, Berman (March 4, 2020). \"The rise of artificial intelligence and the crisis of moral passivity\" . AI &amp; Society . 35 (4): 991– 993. doi : 10.1007/s00146-020-00953-9 . ISSN 1435-5655 . S2CID 212407078 . Archived from the original on February 10, 2023 . Retrieved January 21, 2023 . 1 2 Eliezer Yudkowsky (2004). \"Coherent Extrapolated Volition\" (PDF) . Singularity Institute for Artificial Intelligence. Archived (PDF) from the original on September 30, 2015 . Retrieved September 12, 2015 . 1 2 Hendry, Erica R. (January 21, 2014). \"What Happens When Artificial Intelligence Turns On Us?\" . Smithsonian Magazine . Archived from the original on July 19, 2014 . Retrieved July 15, 2014 . ↑ Baum, Seth D. (September 28, 2016). \"On the promotion of safe and socially beneficial artificial intelligence\". AI &amp; Society . 32 (4): 543– 551. doi : 10.1007/s00146-016-0677-0 . ISSN 0951-5666 . S2CID 29012168 . 1 2 3 4 Russell, Stuart (October 8, 2019). Human Compatible: Artificial Intelligence and the Problem of Control . United States: Viking. ISBN 978-0-525-55861-3 . OCLC 1083694322 . ↑ McGinnis, John O. (Summer 2010). \"Accelerating AI\" . Northwestern University Law Review . 104 (3): 1253– 1270. Archived from the original on December 1, 2014 . Retrieved July 16, 2014 . ↑ Winfield, Alan (August 9, 2014). \"Artificial intelligence will not turn into a Frankenstein's monster\" . The Guardian . Archived from the original on September 17, 2014 . Retrieved September 17, 2014 . ↑ Kornai, András (May 15, 2014). \"Bounding the impact of AGI\". Journal of Experimental &amp; Theoretical Artificial Intelligence . 26 (3). Informa UK Limited: 417– 438. doi : 10.1080/0952813x.2014.895109 . ISSN 0952-813X . S2CID 7067517 . ...the essence of AGIs is their reasoning facilities, and it is the very logic of their being that will compel them to behave in a moral fashion... The real nightmare scenario (is one where) humans find it advantageous to strongly couple themselves to AGIs, with no guarantees against self-deception. ↑ Keiper, Adam; Schulman, Ari N. (Summer 2011). \"The Problem with 'Friendly' Artificial Intelligence\" . The New Atlantis . No. 32. pp. 80– 89. Archived from the original on January 15, 2012 . Retrieved January 16, 2012 . ↑ Norvig, Peter; Russell, Stuart (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Pearson. ISBN 978-0-13-604259-4 . Further reading Yudkowsky, E. (2008). Artificial Intelligence as a Positive and Negative Factor in Global Risk . In Global Catastrophic Risks , Oxford University Press. Discusses Artificial Intelligence from the perspective of Existential risk . In particular, Sections 1-4 give background to the definition of Friendly AI in Section 5. Section 6 gives two classes of mistakes (technical and philosophical) which would both lead to the accidental creation of non-Friendly AIs. Sections 7-13 discuss further related issues. Omohundro, S. (2008). The Basic AI Drives Appeared in AGI-08 – Proceedings of the First Conference on Artificial General Intelligence. Mason, C. (2008). Human-Level AI Requires Compassionate Intelligence Archived 2022-01-09 at the Wayback Machine Appears in AAAI 2008 Workshop on Meta-Reasoning: Thinking About Thinking. Froding, B. and Peterson, M. (2021). Friendly AI Ethics and Information Technology, Vol. 23, pp. 207–214. External links Ethical Issues in Advanced Artificial Intelligence by Nick Bostrom What is Friendly AI? — A brief description of Friendly AI by the Machine Intelligence Research Institute. Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures — A near book-length description from the MIRI Critique of the MIRI Guidelines on Friendly AI — by Bill Hibbard Commentary on MIRI's Guidelines on Friendly AI — by Peter Voss. The Problem with 'Friendly' Artificial Intelligence — On the motives for and impossibility of FAI; by Adam Keiper and Ari N. Schulman. .mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd;color:inherit}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf;color:inherit}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf;color:inherit}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff;color:inherit}.mw-parser-output .navbox-even{background-color:#f7f7f7;color:inherit}.mw-parser-output .navbox-odd{background-color:transparent;color:inherit}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}} v t e Existential risk from artificial intelligence Concepts AGI AI alignment AI boom AI capability control AI safety AI takeover Consequentialism Effective accelerationism Ethics of artificial intelligence Existential risk from artificial intelligence Friendly artificial intelligence Instrumental convergence Vulnerable world hypothesis Intelligence explosion Jobpocalypse Longtermism Machine ethics Right to reality Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI Safe Superintelligence People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Ilya Sutskever Jaan Tallinn Max Tegmark Alan Turing Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Artificial Intelligence Act Do You Trust This Computer? Human Compatible Open letter on artificial intelligence (2015) Our Final Invention Roko's basilisk Statement on AI risk of extinction Superintelligence: Paths, Dangers, Strategies The Precipice If Anyone Builds It, Everyone Dies Category",
  "cached_at": "2025-10-25T19:39:30.506395"
}