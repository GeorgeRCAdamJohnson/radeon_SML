{
  "title": "Turing test",
  "summary": "The Turing test, originally called the imitation game by Alan Turing in 1949, is a test of a machine's ability to exhibit intelligent behaviour equivalent to that of a human. In the test, a human evaluator judges a text transcript of a natural-language conversation between a human and a machine. The evaluator tries to identify the machine, and the machine passes if the evaluator cannot reliably tell them apart. The results would not depend on the machine's ability to answer questions correctly, only on how closely its answers resembled those of a human. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).",
  "content": "Turing test Test of a machine's ability to imitate human intelligence .mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}} Not to be confused with Turing machine . \"Imitation game\" redirects here. For the film, see The Imitation Game . For other uses, see Turing test (disambiguation) . The \"standard interpretation\" of the Turing test, in which player C, the interrogator, is given the task of trying to determine which player – A or B – is a computer and which is a human. The interrogator is limited to using the responses to written questions to make the determination. [ 1 ] .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}} Part of a series on Artificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Open-source Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy AI alignment Artificial consciousness The bitter lesson Chinese room Friendly AI Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom AI bubble Controversies Deepfake pornography Taylor Swift deepfake pornography controversy Google Gemini image generation controversy Pause Giant AI Experiments Removal of Sam Altman from OpenAI Statement on AI Risk Tay (chatbot) Théâtre D'opéra Spatial Voiceverse NFT plagiarism scandal Glossary Glossary .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e The Turing test , originally called the imitation game by Alan Turing in 1949, [ 2 ] is a test of a machine's ability to exhibit intelligent behaviour equivalent to that of a human. In the test, a human evaluator judges a text transcript of a natural-language conversation between a human and a machine. The evaluator tries to identify the machine, and the machine passes if the evaluator cannot reliably tell them apart. The results would not depend on the machine's ability to answer questions correctly , only on how closely its answers resembled those of a human. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic). [ 3 ] The test was introduced by Turing in his 1950 paper \" Computing Machinery and Intelligence \" while working at the University of Manchester . [ 4 ] It opens with the words: \"I propose to consider the question, 'Can machines think? ' \" Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words\". [ 5 ] Turing describes the new form of the problem in terms of a three-person party game called the \"imitation game\", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: \"Are there imaginable digital computers which would do well in the imitation game ?\" [ 2 ] This question, Turing believed, was one that could actually be answered. In the remainder of the paper, he argued against the major objections to the proposition that \"machines can think\". [ 6 ] Since Turing introduced his test, it has been highly influential in the philosophy of artificial intelligence , resulting in substantial discussion and controversy, as well as criticism from philosophers like John Searle , who argue against the test's ability to detect consciousness . [ 7 ] [ 8 ] Since the mid- 2020s , several large language models such as ChatGPT have passed modern, rigorous variants of the Turing test. [ 9 ] [ 10 ] [ 11 ] History Philosophical background The question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind. René Descartes prefigures aspects of the Turing test in his 1637 Discourse on the Method when he writes: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} [H]ow many different automata or moving machines could be made by the industry of man ... For we can easily understand a machine's being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do. [ 12 ] Here Descartes notes that automata are capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. Descartes therefore prefigures the Turing test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. Descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the Turing test as such, even if he prefigures its conceptual framework and criterion. Denis Diderot formulates in his 1746 book Pensées philosophiques a Turing-test criterion, though with the important implicit limiting assumption maintained, of the participants being natural living beings, rather than considering created artifacts: If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation. This does not mean he agrees with this, but that it was already a common argument of materialists at that time. According to dualism, the mind is non-physical (or, at the very least, has non-physical properties ) [ 13 ] and, therefore, cannot be explained in purely physical terms. According to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially. [ 14 ] In 1936, philosopher Alfred Ayer considered the standard philosophical question of other minds : how do we know that other people have the same conscious experiences that we do? In his book, Language, Truth and Logic , Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: \"The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined\". [ 15 ] (This suggestion is very similar to the Turing test, but it is not certain that Ayer's popular philosophical classic was familiar to Turing.) In other words, a thing is not conscious if it fails the consciousness test. Cultural background A rudimentary idea of the Turing test appears in the 1726 novel Gulliver's Travels by Jonathan Swift . [ 16 ] [ 17 ] When Gulliver is brought before the king of Brobdingnag , the king thinks at first that Gulliver might be a \"a piece of clock-work (which is in that country arrived to a very great perfection) contrived by some ingenious artist\". Even when he hears Gulliver speaking, the king still doubts whether Gulliver was taught \"a set of words\" to make him \"sell at a better price\". Gulliver tells that only after \"he put several other questions to me, and still received rational answers\" the king became satisfied that Gulliver was not a machine. [ 18 ] Tests where a human judges whether a computer or an alien is intelligent were an established convention in science fiction by the 1940s, and it is likely that Turing would have been aware of these. [ 19 ] Stanley G. Weinbaum 's \" A Martian Odyssey \" (1934) provides an example of how nuanced such tests could be. [ 19 ] Earlier examples of machines or automatons attempting to pass as human include the Ancient Greek myth of Pygmalion who creates a sculpture of a woman that is animated by Aphrodite , Carlo Collodi 's novel The Adventures of Pinocchio , about a puppet who wants to become a real boy, and E. T. A. Hoffmann 's 1816 story \" The Sandman ,\" where the protagonist falls in love with an automaton. In all these examples, people are fooled by artificial beings that - up to a point - pass as human. [ 20 ] Alan Turing and the imitation game Researchers in the United Kingdom had been exploring \"machine intelligence\" for up to ten years prior to the founding of the field of artificial intelligence ( AI ) research in 1956. [ 21 ] It was a common topic among the members of the Ratio Club , an informal group of British cybernetics and electronics researchers that included Alan Turing. [ 22 ] Turing, in particular, had been running the notion of machine intelligence since at least 1941 [ 23 ] and one of the earliest-known mentions of \"computer intelligence\" was made by him in 1947. [ 24 ] In Turing's report, \"Intelligent Machinery,\" [ 25 ] he investigated \"the question of whether or not it is possible for machinery to show intelligent behaviour\" [ 26 ] and, as part of that investigation, proposed what may be considered the forerunner to his later tests: It is not difficult to devise a paper machine which will play a not very bad game of chess. [ 27 ] Now get three men A, B and C as subjects for the experiment. A and C are to be rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing. &amp;#91;&lt;i>[[Wikipedia:Citing_sources|&lt;span_title=\"This_citation_requires_a_reference_to_the_specific_page_or_range_of_pages_in_which_the_material_appears.&amp;#32;(May_2011)\">page&amp;nbsp;needed&lt;/span>]]&lt;/i>&amp;#93;&lt;/sup>_28-0' rel=\"dc:references\" typeof=\"mw:Transclusion mw:Extension/ref\" data-mw='{\"name\":\"ref\",\"attrs\":{\"group\":\"\",\"name\":\"FOOTNOTETuring1948[[Category:Wikipedia articles needing page number citations from May 2011]]&lt;sup class=\\\"noprint Inline-Template \\\" style=\\\"white-space:nowrap;\\\">&amp;#91;&lt;i>[[Wikipedia:Citing sources|&lt;span title=\\\"This citation requires a reference to the specific page or range of pages in which the material appears.&amp;#32;(May 2011)\\\">page&amp;nbsp;needed&lt;/span>]]&lt;/i>&amp;#93;&lt;/sup>\"},\"body\":{\"id\":\"mw-reference-text-cite_note-FOOTNOTETuring1948[[Category:Wikipedia_articles_needing_page_number_citations_from_May_2011]]&lt;sup_class=\\\"noprint_Inline-Template_\\\"_style=\\\"white-space:nowrap;\\\">&amp;#91;&lt;i>[[Wikipedia:Citing_sources|&lt;span_title=\\\"This_citation_requires_a_reference_to_the_specific_page_or_range_of_pages_in_which_the_material_appears.&amp;#32;(May_2011)\\\">page&amp;nbsp;needed&lt;/span>]]&lt;/i>&amp;#93;&lt;/sup>-28\"},\"parts\":[{\"template\":{\"target\":{\"wt\":\"sfn\",\"href\":\"./Template:Sfn\"},\"params\":{\"1\":{\"wt\":\"Turing\"},\"2\":{\"wt\":\"1948\"},\"p\":{\"wt\":\"{{page needed|date=May 2011}}\"}},\"i\":0}}]}'> &amp;#91;&lt;i>[[Wikipedia:Citing_sources|&lt;span_title=\"This_citation_requires_a_reference_to_the_specific_page_or_range_of_pages_in_which_the_material_appears.&amp;#32;(May_2011)\">page&amp;nbsp;needed&lt;/span>]]&lt;/i>&amp;#93;&lt;/sup>-28' id=\"mw1A\"> [ 28 ] \" Computing Machinery and Intelligence \" ( CITEREFTuring1950&lt;/span>\"}]]}' id=\"mw2g\">1950 ) was the first published paper by Turing to focus exclusively on machine intelligence. Turing begins the 1950 paper with the claim, \"I propose to consider the question 'Can machines think? ' \" [ 5 ] As he highlights, the traditional approach to such a question is to start with definitions , defining both the terms \"machine\" and \"think\". Turing chooses not to do so; instead, he replaces the question with a new one, \"which is closely related to it and is expressed in relatively unambiguous words\". [ 5 ] In essence he proposes to change the question from \"Can machines think?\" to \"Can machines do what we (as thinking entities) can do?\" [ 29 ] The advantage of the new question, Turing argues, is that it draws \"a fairly sharp line between the physical and intellectual capacities of a man\". [ 30 ] To demonstrate this approach Turing proposes a test inspired by a party game , known as the \"imitation game\", in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. In this game, both the man and the woman aim to convince the guests that they are the other. (Huma Shah argues that this two-human version of the game was presented by Turing only to introduce the reader to the machine-human question-answer test. [ 31 ] ) Turing described his new version of the game as follows: We now ask the question, \"What will happen when a machine takes the part of A in this game?\" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \"Can machines think?\" [ 30 ] Later in the paper, Turing suggests an \"equivalent\" alternative formulation involving a judge conversing only with a computer and a man. [ 32 ] While neither of these formulations precisely matches the version of the Turing test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in a BBC radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man. [ 33 ] Turing's paper considered nine putative objections, which include some of the major arguments against artificial intelligence that have been raised in the years since the paper was published (see \" Computing Machinery and Intelligence \"). [ 6 ] The Chinese room Main article: Chinese room John Searle 's 1980 paper Minds, Brains, and Programs proposed the \" Chinese room \" thought experiment and argued that the Turing test could not be used to determine if a machine could think. Searle noted that software (such as ELIZA) could pass the Turing test simply by manipulating symbols of which they had no understanding. Without understanding, they could not be described as \"thinking\" in the same sense people did. Therefore, Searle concluded, the Turing test could not prove that machines could think. [ 34 ] Much like the Turing test itself, Searle's argument has been both widely criticised [ 35 ] and endorsed. [ 36 ] Arguments such as Searle's and others working on the philosophy of mind sparked off a more intense debate about the nature of intelligence, the possibility of machines with a conscious mind and the value of the Turing test that continued through the 1980s and 1990s. [ 37 ] Loebner Prize Main article: Loebner Prize The Loebner Prize, now reported as defunct, [ 38 ] provided an annual platform for practical Turing tests with the first competition held in November 1991. [ 39 ] It was underwritten by Hugh Loebner . The Cambridge Center for Behavioral Studies in Massachusetts , United States, organised the prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing test despite 40 years of discussing it. [ 40 ] The first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing test and the value of pursuing it, in both the popular press [ 41 ] and academia. [ 42 ] The first contest was won by a mindless program with no identifiable intelligence that managed to fool naïve interrogators into making the wrong identification. This highlighted several of the shortcomings of the Turing test (discussed below ): The winner won, at least in part, because it was able to \"imitate human typing errors\"; [ 41 ] the unsophisticated interrogators were easily fooled; [ 42 ] and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research. [ 43 ] The silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour among that year's entries. Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AI Jabberwacky won in 2005 and 2006. The Loebner Prize tested conversational intelligence; winners were typically chatterbot programs, or Artificial Conversational Entities (ACE)s . Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic, [ 44 ] thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes. The final competition was in 2019, due to a lack of funding for the prize following Loebner's death in 2016. [ 45 ] CAPTCHA Main article: CAPTCHA CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) is one of the oldest concepts for artificial intelligence. The CAPTCHA system is commonly used online to tell humans and bots apart on the internet. It is based on the Turing test. Displaying distorted letters and numbers, it asks the user to identify the letters and numbers and type them into a field, which bots struggle to do. [ 46 ] [ 47 ] The reCaptcha is a CAPTCHA system owned by Google . The reCaptcha v1 and v2 both used to operate by asking the user to match distorted pictures or identify distorted letters and numbers. The reCaptcha v3 is designed to not interrupt users and run automatically when pages are loaded or buttons are clicked. This \"invisible\" CAPTCHA verification happens in the background and no challenges appear, which filters out most basic bots. [ 48 ] [ 49 ] Attempts Several early symbolic AI programs were controversially claimed to pass the Turing test, either by limiting themselves to scripted situations or by presenting \"excuses\" for poor reasoning and conversational abilities, such as mental illness or a poor grasp of English. [ 50 ] [ 51 ] [ 52 ] In 1966, Joseph Weizenbaum created a program called ELIZA , which mimicked a Rogerian psychotherapist . [ 53 ] The program would search the user's sentence for keywords before repeating them back to the user , providing the impression of a program listening and paying attention. [ 54 ] Weizenbaum thus succeeded by designing a context where a chatbot could mimic a person despite \"knowing almost nothing of the real world\". [ 51 ] Weizenbaum's program was able to fool some people into believing that they were talking to a real person. [ 51 ] Kenneth Colby created PARRY in 1972, a program modeled after the behaviour of paranoid schizophrenics . [ 55 ] [ 53 ] Psychiatrists asked to compare transcripts of conversations generated by the program to those of conversations by actual schizophrenics could only identify about 52 percent of cases correctly (a figure consistent with random guessing). [ 56 ] In 2001, three programmers developed Eugene Goostman , a chatbot portraying itself as a 13-year-old boy from Odesa who spoke English as a second language . This background was intentionally chosen so judges would forgive mistakes by the program. In a competition, 33% of judges thought Goostman was human. [ 46 ] [ 57 ] [ 58 ] Large language models Main article: Large language model Google LaMDA Main article: LaMDA In June 2022, Google 's LaMDA model received widespread coverage after claims about it having achieved sentience. Initially in an article in The Economist Google Research Fellow Blaise Agüera y Arcas said the chatbot had demonstrated a degree of understanding of social relationships. [ 59 ] Several days later, Google engineer Blake Lemoine claimed in an interview with the Washington Post that LaMDA had achieved sentience. Lemoine had been placed on leave by Google for internal assertions to this effect. Google had investigated the claims but dismissed them. [ 60 ] [ 61 ] ChatGPT Main article: ChatGPT OpenAI 's chatbot, ChatGPT, released in November 2022, is based on GPT-3.5 and GPT-4 large language models . Celeste Biever wrote in a Nature article that \"ChatGPT broke the Turing test\". [ 62 ] Stanford researchers reported that ChatGPT passes the test; they found that ChatGPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative\", [ 9 ] making it the first computer program to successfully do so. [ 10 ] In late March 2025, a study evaluated four systems (ELIZA, GPT-4o, LLaMa-3.1-405B, and GPT-4.5) in two randomized, controlled, and pre-registered Turing tests with independent participant groups. Participants engaged in simultaneous 5-minute conversations with another human participant and one of these systems, then judged which conversational partner they believed to be human. When instructed to adopt a humanlike persona, GPT-4.5 was identified as the human 73% of the time—significantly more often than the actual human participants. LLaMa-3.1, under the same conditions, was judged to be human 56% of the time, not significantly more or less often than the humans they were compared to. Baseline models (ELIZA and GPT-4o) achieved win rates significantly below chance (23% and 21%, respectively). These results provide the first empirical evidence that any artificial system passes a standard three-party Turing test. The findings have implications for debates about the nature of intelligence exhibited by Large Language Models (LLMs) and the social and economic impacts these systems are likely to have. [ 11 ] Versions The imitation game, as described by Alan Turing in \"Computing Machinery and Intelligence\". Player C, through a series of written questions, attempts to determine which of the other two players is a man, and which of the two is the woman. Player A, the man, tries to trick player C into making the wrong decision, while player B tries to help player C. Figure adapted from Saygin, 2000. [ 7 ] Saul Traiger argues that there are at least three primary versions of the Turing test, two of which are offered in \"Computing Machinery and Intelligence\" and one that he describes as the \"Standard Interpretation\". [ 63 ] While there is some debate regarding whether the \"Standard Interpretation\" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent, [ 63 ] and their strengths and weaknesses are distinct. [ 64 ] Turing's original article describes a simple party game involving three players. Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either sex. In the imitation game, player C is unable to see either player A or player B, and can communicate with them only through written notes. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one. [ 7 ] Turing then asks: \"What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?\" These questions replace our original, \"Can machines think?\" [ 30 ] The original imitation game test, in which the player A is replaced with a computer. The computer is now charged with the role of the man, while player B continues to attempt to assist the interrogator. Figure adapted from Saygin, 2000. [ 7 ] The second version appeared later in Turing's 1950 paper. Similar to the original imitation game test, the role of player A is performed by a computer. However, the role of player B is performed by a man rather than a woman. Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man? [ 30 ] In this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision. The standard interpretation is not included in the original paper, but is both accepted and debated. Common understanding has it that the purpose of the Turing test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could imitate a human. [ 7 ] While there is some dispute whether this interpretation was intended by Turing, Sterrett believes that it was [ 65 ] and thus conflates the second version with this one, while others, such as Traiger, do not [ 63 ] – this has nevertheless led to what can be viewed as the \"standard interpretation\". In this version, player A is a computer and player B a person of either sex. The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human. [ 66 ] The fundamental issue with the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. There are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable. Interpretations Controversy has arisen over which of the alternative formulations of the test Turing intended. [ 65 ] Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, despite Turing's remark, they are not equivalent. The test that employs the party game and compares frequencies of success is referred to as the \"Original Imitation Game Test\", whereas the test consisting of a human judge conversing with a human and a machine is referred to as the \"Standard Turing Test\", noting that Sterrett equates this with the \"standard interpretation\" rather than the second version of the imitation game. Sterrett agrees that the standard Turing test (STT) has the problems that its critics cite but feels that, in contrast, the original imitation game test (OIG test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. A man can fail the OIG test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG test requires the resourcefulness associated with intelligence and not merely \"simulation of human conversational behaviour\". The general structure of the OIG test could even be used with non-verbal versions of imitation games. [ 67 ] According to Huma Shah, Turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions. [ 68 ] Shah argues the imitation game which Turing described could be practicalized in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator. [ 31 ] Still other writers [ 69 ] have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game. Some writers argue that the imitation game is best understood by its social aspects. In his 1948 paper, Turing refers to intelligence as an \"emotional concept,\" and notes that The extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. If we are able to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine intelligence. With the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behaviour. [ 70 ] Following this remark and similar ones scattered throughout Turing's publications, Diane Proudfoot [ 71 ] claims that Turing held a response-dependence approach to intelligence, according to which an intelligent (or thinking) entity is one that appears intelligent to an average interrogator. Shlomo Danziger [ 72 ] promotes a socio-technological interpretation, according to which Turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve a change in society's attitude toward machines. According to this reading, Turing's celebrated 50-year prediction - that by the end of the 20th century his test will be passed by some machine - actually consists of two distinguishable predictions. The first is a technological prediction: I believe that in about fifty years' time it will be possible to programme computers ... to make them play the imitation game so well that an average interrogator will not have more than 70% chance of making the right identification after five minutes of questioning. [ 73 ] The second prediction Turing makes is a sociological one: I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. [ 73 ] Danziger claims further that for Turing, alteration of society's attitude towards machinery is a prerequisite for the existence of intelligent machines: Only when the term \"intelligent machine\" is no longer seen as an oxymoron the existence of intelligent machines would become logically possible. Saygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer. [ 74 ] The imitation game also includes a \"social hack\" not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not. [ 75 ] Should the interrogator know about the computer? A crucial piece of any laboratory test is that there should be a control. Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. He states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement. [ 30 ] When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation. [ 76 ] As Ayse Saygin, Peter Swirski, [ 77 ] and others have highlighted, this makes a big difference to the implementation and outcome of the test. [ 7 ] An experimental study looking at Gricean maxim violations using transcripts of Loebner's one-to-one (interrogator-hidden interlocutor) Prize for AI contests between 1994 and 1999, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved. [ 78 ] Strengths Tractability and simplicity The power and appeal of the Turing test derives from its simplicity. The philosophy of mind , psychology , and modern neuroscience have been unable to provide definitions of \"intelligence\" and \"thinking\" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of the philosophy of artificial intelligence cannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic attempt to answer a difficult philosophical question. Breadth of subject matter The format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. Turing wrote that \"the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include\". [ 79 ] John Haugeland adds that \"understanding the words is not enough; you have to understand the topic as well\". [ 80 ] To pass a well-designed Turing test, the machine must use natural language , reason , have knowledge and learn . The test can be extended to include video input, as well as a \"hatch\" through which objects can be passed: this would force the machine to demonstrate skilled use of well designed vision and robotics as well. Together, these represent almost all of the major problems that artificial intelligence research would like to solve. [ 81 ] The Feigenbaum test is designed to take advantage of the broad range of topics available to a Turing test. It is a limited form of Turing's question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry . Emphasis on emotional and aesthetic intelligence As a Cambridge honours graduate in mathematics, Turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipating a more recent approach to the subject . Instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant. Given the status of human sexual dimorphism as one of the most ancient of subjects , it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. The challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibility—both of which qualities are on display in this snippet of dialogue which Turing has imagined: Interrogator: Will X please tell me the length of his or her hair? Contestant: My hair is shingled, and the longest strands are about nine inches long. When Turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry: Interrogator: In the first line of your sonnet which reads, \"Shall I compare thee to a summer's day,\" would not \"a spring day\" do as well or better? Witness: It wouldn't scan . Interrogator: How about \"a winter's day\". That would scan all right. Witness: Yes, but nobody wants to be compared to a winter's day. Turing thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an AI run amok, [ 82 ] it has been suggested [ 83 ] that this focus perhaps represents a critical intuition on Turing's part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a \" friendly AI \". It is further noted, however, that whatever inspiration Turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a \"standard interpretation\" of the Turing test—i.e., one which focuses on a discursive intelligence only—must be regarded with some caution. Weaknesses Turing did not explicitly state that the Turing test could be used as a measure of \" intelligence \", or any other human quality. He wanted to provide a clear and understandable alternative to the word \"think\", which he could then use to reply to criticisms of the possibility of \"thinking machines\" and to suggest ways that research might move forward. Nevertheless, the Turing test has been proposed as a measure of a machine's \"ability to think\" or its \"intelligence\". This proposal has received criticism from both philosophers and computer scientists. The interpretation makes the assumption that an interrogator can determine if a machine is \"thinking\" by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the reliability of the interrogator's judgement, the value of comparing the machine with a human, and the value of comparing only behaviour. Because of these and other considerations, some AI researchers have questioned the relevance of the test to their field. Naïveté of interrogators In practice, the test's results can easily be dominated not by the computer's intelligence, but by the attitudes, skill, or naïveté of the questioner. Numerous experts in the field, including cognitive scientist Gary Marcus , insist that the Turing test only shows how easy it is to fool humans and is not an indication of machine intelligence. [ 84 ] Turing doesn't specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term \"average interrogator\": \"[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning\". [ 73 ] Chatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the \"interrogators\" are not even aware of the possibility that they are interacting with computers. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required. [ 53 ] Early Loebner Prize competitions used \"unsophisticated\" interrogators who were easily fooled by the machines. [ 42 ] Since 2004, the Loebner Prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. Nonetheless, some of these experts have been deceived by the machines. [ 85 ] One interesting feature of the Turing test is the frequency of the confederate effect , when the confederate (tested) humans are misidentified by the interrogators as machines. It has been suggested that what interrogators expect as human responses is not necessarily typical of humans. As a result, some individuals can be categorised as machines. This can therefore work in favour of a competing machine. The humans are instructed to \"act themselves\", but sometimes their answers are more like what the interrogator expects a machine to say. [ 86 ] This raises the question of how to ensure that the humans are motivated to \"act human\". Human intelligence vs. intelligence in general The Turing test does not directly test whether the computer behaves intelligently. It tests only whether the computer behaves like a human being. Since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways: Some human behaviour is unintelligent The Turing test requires that the machine be able to execute all human behaviours, regardless of whether they are intelligent. It even tests for behaviours that may not be considered intelligent at all, such as the susceptibility to insults, [ 87 ] the temptation to lie or, simply, a high frequency of typing mistakes . If a machine cannot imitate these unintelligent behaviours in detail it fails the test. This objection was raised by The Economist , in an article entitled \" artificial stupidity \" published shortly after the first Loebner Prize competition in 1992. The article noted that the first Loebner winner's victory was due, at least in part, to its ability to \"imitate human typing errors\". [ 41 ] Turing himself had suggested that programs add errors into their output, so as to be better \"players\" of the game. [ 88 ] Some intelligent behaviour is inhuman The Turing test does not test for highly intelligent behaviours, such as the ability to solve difficult problems or come up with original insights. In fact, it specifically requires deception on the part of the machine: if the machine is more intelligent than a human being it must deliberately avoid appearing too intelligent. If it were to solve a computational problem that is practically impossible for a human to solve, then the interrogator would know the program is not human, and the machine would fail the test. Because it cannot measure intelligence that is beyond the ability of humans, the test cannot be used to build or evaluate systems that are more intelligent than humans. Because of this, several test alternatives that would be able to evaluate super-intelligent systems have been proposed. [ 89 ] Consciousness vs. the simulation of consciousness Main article: Chinese room See also: Synthetic intelligence The Turing test is concerned strictly with how the subject acts – the external behaviour of the machine. In this regard, it takes a behaviourist or functionalist approach to the study of the mind. The example of ELIZA suggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all. [ 53 ] John Searle has argued that external behaviour cannot be used to determine if a machine is \"actually\" thinking or merely \"simulating thinking\". [ 34 ] His Chinese room argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a mind , consciousness , or intentionality . (Intentionality is a philosophical term for the power of thoughts to be \"about\" something.) Turing anticipated this line of criticism in his original paper, [ 90 ] writing: I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with",
  "cached_at": "2025-10-25T19:39:38.429117"
}