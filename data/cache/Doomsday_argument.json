{
  "title": "Doomsday argument",
  "summary": "The doomsday argument (DA), or Carter catastrophe, is a probabilistic argument that aims to predict the total number of humans who will ever live. It argues that if a human's birth rank is randomly sampled from the set of all humans who will ever live, it is improbable that one would be at the extreme beginning. This implies that the total number of humans is unlikely to be much larger than the number of humans born so far.",
  "content": "Doomsday argument Doomsday scenario on human births .mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}} Not to be confused with the Doomsday equation or the Doomsday rule . .mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}@media print{body.ns-0 .mw-parser-output .ambox{display:none!important}} This article relies excessively on references to primary sources . Please improve this article by adding secondary or tertiary sources . Find sources: \"Doomsday argument\" – news · newspapers · books · scholar · JSTOR ( August 2016 ) ( Learn how and when to remove this message ) World population from 10,000 BC to AD 2000 The doomsday argument ( DA ), or Carter catastrophe , is a probabilistic argument that aims to predict the total number of humans who will ever live. It argues that if a human's birth rank is randomly sampled from the set of all humans who will ever live, it is improbable that one would be at the extreme beginning. This implies that the total number of humans is unlikely to be much larger than the number of humans born so far. The doomsday argument was originally proposed by the astrophysicist Brandon Carter in 1983, [ 1 ] leading to the initial name of the Carter catastrophe. The argument was subsequently championed by the philosopher John A. Leslie and has since been independently conceived by J. Richard Gott [ 2 ] and Holger Bech Nielsen . [ 3 ] Summary The premise of the argument is as follows: suppose that the total number of human beings who will ever exist is fixed. If so, the likelihood of a randomly selected person existing at a particular time in history would be proportional to the total population at that time. Given this, the argument posits that a person alive today should adjust their expectations about the future of the human race because their existence provides information about the total number of humans that will ever live. If the total number of humans who were born or will ever be born is denoted by N {\\textstyle N} , then the Copernican principle suggests that any one human is equally likely to find themselves in any position n {\\textstyle n} of the total population N {\\textstyle N} . f {\\textstyle f} is uniformly distributed on (0,1) even after learning the absolute position n {\\textstyle n} . For example, there is a 95% chance that f {\\textstyle f} is in the interval (0.05,1), that is 0.05\"}}' id=\"mwLw\"> 0.05}\"> f > 0.05 {\\textstyle f>0.05} 0.05}\"/> . In other words, one can assume with 95% certainty that any individual human would be within the last 95% of all the humans ever to be born. If the absolute position n {\\textstyle n} is known, this argument implies a 95% confidence upper bound for N {\\textstyle N} obtained by rearranging 0.05\"}}' id=\"mwMg\"> 0.05}\"> n / N > 0.05 {\\textstyle n/N>0.05} 0.05}\"/> to give N &lt; 20 n {\\textstyle N&lt;20n} . If Leslie's figure [ 4 ] is used, then approximately 60 billion humans have been born so far, so it can be estimated that there is a 95% chance that the total number of humans N {\\textstyle N} will be less than 20 × {\\textstyle \\times } 60 billion = 1.2 trillion. Assuming that the world population stabilizes at 10 billion and a life expectancy of 80 years , it can be estimated that the remaining 1140 billion humans will be born in 9120 years. Depending on the projection of the world population in the forthcoming centuries, estimates may vary, but the argument states that it is unlikely that more than 1.2 trillion humans will ever live. Aspects Assume, for simplicity, that the total number of humans who will ever be born is 60 billion ( N 1 ), or 6,000 billion ( N 2 ). [ 5 ] If there is no prior knowledge of the position that a currently living individual, X , has in the history of humanity, one may instead compute how many humans were born before X , and arrive at say 59,854,795,447, which would necessarily place X among the first 60 billion humans who have ever lived. It is possible to sum the probabilities for each value of N and, therefore, to compute a statistical 'confidence limit' on N . For example, taking the numbers above, it is 99% certain that N is smaller than 6 trillion. Note that as remarked above, this argument assumes that the prior probability for N is flat, or 50% for N 1 and 50% for N 2 in the absence of any information about X . On the other hand, it is possible to conclude, given X , that N 2 is more likely than N 1 if a different prior is used for N . More precisely, Bayes' theorem tells us that P( N | X ) = P( X | N )P( N )/P( X ), and the conservative application of the Copernican principle tells us only how to calculate P( X | N ). Taking P( X ) to be flat, we still have to assume the prior probability P( N ) that the total number of humans is N . If we conclude that N 2 is much more likely than N 1 (for example, because producing a larger population takes more time, increasing the chance that a low probability but cataclysmic natural event will take place in that time), then P( X | N ) can become more heavily weighted towards the bigger value of N . A further, more detailed discussion, as well as relevant distributions P( N ), are given below in the Rebuttals section. The doomsday argument does not say that humanity cannot or will not exist indefinitely. It does not put any upper limit on the number of humans that will ever exist nor provide a date for when humanity will become extinct . An abbreviated form of the argument does make these claims, by confusing probability with certainty. However, the actual conclusion for the version used above is that there is a 95% chance of extinction within 9,120 years and a 5% chance that some humans will still be alive at the end of that period. (The precise numbers vary among specific doomsday arguments.) Variations This argument has generated a philosophical debate, and no consensus has yet emerged on its solution. The variants described below produce the DA by separate derivations. Gott's formulation: \"vague prior\" total population Gott specifically proposes the functional form for the prior distribution of the number of people who will ever be born ( N ). Gott's DA used the vague prior distribution : P ( N ) = k N {\\displaystyle P(N)={\\frac {k}{N}}} . where P(N) is the probability prior to discovering n , the total number of humans who have yet been born. The constant, k , is chosen to normalize the sum of P( N ). The value chosen is not important here, just the functional form (this is an improper prior , so no value of k gives a valid distribution, but Bayesian inference is still possible using it.) Since Gott specifies the prior distribution of total humans, P(N) , Bayes' theorem and the principle of indifference alone give us P(N|n) , the probability of N humans being born if n is a random draw from N : P ( N ∣ n ) = P ( n ∣ N ) P ( N ) P ( n ) . {\\displaystyle P(N\\mid n)={\\frac {P(n\\mid N)P(N)}{P(n)}}.} This is Bayes' theorem for the posterior probability of the total population ever born of N , conditioned on population born thus far of n . Now, using the indifference principle: P ( n ∣ N ) = 1 N {\\displaystyle P(n\\mid N)={\\frac {1}{N}}} . The unconditioned n distribution of the current population is identical to the vague prior N probability density function, [ note 1 ] so: P ( n ) = k n {\\displaystyle P(n)={\\frac {k}{n}}} , giving P ( N | n ) for each specific N (through a substitution into the posterior probability equation): P ( N ∣ n ) = n N 2 {\\displaystyle P(N\\mid n)={\\frac {n}{N^{2}}}} . The easiest way to produce the doomsday estimate with a given confidence (say 95%) is to pretend that N is a continuous variable (since it is very large) and integrate over the probability density from N = n to N = Z . (This will give a function for the probability that N ≤ Z ): P ( N ≤ Z ) = ∫ N = n N = Z P ( N | n ) d N {\\displaystyle P(N\\leq Z)=\\int _{N=n}^{N=Z}P(N|n)\\,dN} = Z − n Z {\\displaystyle ={\\frac {Z-n}{Z}}} Defining Z = 20 n gives: P ( N ≤ 20 n ) = 19 20 {\\displaystyle P(N\\leq 20n)={\\frac {19}{20}}} . This is the simplest Bayesian derivation of the doomsday argument: The chance that the total number of humans that will ever be born ( N ) is greater than twenty times the total that have been is below 5% The use of a vague prior distribution seems well-motivated as it assumes as little knowledge as possible about N , given that some particular function must be chosen. It is equivalent to the assumption that the probability density of one's fractional position remains uniformly distributed even after learning of one's absolute position ( n ). Gott's \"reference class\" in his original 1993 paper was not the number of births, but the number of years \"humans\" had existed as a species, which he put at 200,000 . Also, Gott tried to give a 95% confidence interval between a minimum survival time and a maximum. Because of the 2.5% chance that he gives to underestimating the minimum, he has only a 2.5% chance of overestimating the maximum. This equates to 97.5% confidence that extinction occurs before the upper boundary of his confidence interval, which can be used in the integral above with Z = 40 n , and n = 200,000 years: P ( N ≤ 40 [ 200000 ] ) = 39 40 {\\displaystyle P(N\\leq 40[200000])={\\frac {39}{40}}} This is how Gott produces a 97.5% confidence of extinction within N ≤ 8,000,000 years. The number he quoted was the likely time remaining, N − n = 7.8 million years. This was much higher than the temporal confidence bound produced by counting births, because it applied the principle of indifference to time. (Producing different estimates by sampling different parameters in the same hypothesis is Bertrand's paradox .) Similarly, there is a 97.5% chance that the present lies in the first 97.5% of human history, so there is a 97.5% chance that the total lifespan of humanity will be at least N ≥ 200000 × 40 39 ≈ 205100 years {\\displaystyle N\\geq 200000\\times {\\frac {40}{39}}\\approx 205100~{\\text{years}}} ; In other words, Gott's argument gives a 95% confidence that humans will go extinct between 5,100 and 7.8 million years in the future. Gott has also tested this formulation against the Berlin Wall and Broadway and off-Broadway plays. [ 6 ] Leslie's argument differs from Gott's version in that he does not assume a vague prior probability distribution for N . Instead, he argues that the force of the doomsday argument resides purely in the increased probability of an early doomsday once you take into account your birth position, regardless of your prior probability distribution for N . He calls this the probability shift . Heinz von Foerster argued that humanity's abilities to construct societies, civilizations and technologies do not result in self-inhibition. Rather, societies' success varies directly with population size. Von Foerster found that this model fits some 25 data points from the birth of Jesus to 1958, with only 7% of the variance left unexplained. Several follow-up letters (1961, 1962, ...) were published in Science showing that von Foerster's equation was still on track. The data continued to fit up until 1973. The most remarkable thing about von Foerster's model was it predicted that the human population would reach infinity or a mathematical singularity, on Friday, November 13, 2026. In fact, von Foerster did not imply that the world population on that day could actually become infinite. The real implication was that the world population growth pattern followed for many centuries prior to 1960 was about to come to an end and be transformed into a radically different pattern. Note that this prediction began to be fulfilled just in a few years after the \"doomsday\" argument was published. [ note 2 ] Reference classes The reference class from which n is drawn, and of which N is the ultimate size, is a crucial point of contention in the doomsday argument argument. The \"standard\" doomsday argument hypothesis skips over this point entirely, merely stating that the reference class is the number of \"people\". Given that you are human, the Copernican principle might be used to determine if you were born exceptionally early, however the term \"human\" has been heavily contested on practical and philosophical reasons. According to Nick Bostrom , consciousness is (part of) the discriminator between what is in and what is out of the reference class, and therefore extraterrestrial intelligence might have a significant impact on the calculation. [ citation needed ] The following sub-sections relate to different suggested reference classes, each of which has had the standard doomsday argument applied to it. SSSA: Sampling from observer-moments Nick Bostrom , considering observation selection effects , has produced a Self-Sampling Assumption (SSA): \"that you should think of yourself as if you were a random observer from a suitable reference class\". If the \"reference class\" is the set of humans to ever be born, this gives N &lt; 20 n with 95% confidence (the standard doomsday argument). However, he has refined this idea to apply to observer-moments rather than just observers. He has formalized this as: [ 7 ] The strong self-sampling assumption (SSSA): Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class. An application of the principle underlying SSSA (though this application is nowhere expressly articulated by Bostrom), is: If the minute in which you read this article is randomly selected from every minute in every human's lifespan, then (with 95% confidence) this event has occurred after the first 5% of human observer-moments. If the mean lifespan in the future is twice the historic mean lifespan, this implies 95% confidence that N &lt; 10 n (the average future human will account for twice the observer-moments of the average historic human). Therefore, the 95th percentile extinction-time estimate in this version is 4560 years. Counterarguments This section's tone or style may not reflect the encyclopedic tone used on Wikipedia . See Wikipedia's guide to writing better articles for suggestions. ( November 2010 ) ( Learn how and when to remove this message ) We are in the earliest 5%, a priori One counterargument to the doomsday argument agrees with its statistical methods but disagrees with its extinction-time estimate. This position requires justifying why the observer cannot be assumed to be randomly selected from the set of all humans ever to be born, which implies that this set is not an appropriate reference class. By disagreeing with the doomsday argument, it implies that the observer is within the first 5% of humans to be born. By analogy, if one is a member of 50,000 people in a collaborative project, the reasoning of the doomsday argument implies that there will never be more than a million members of that project, within a 95% confidence interval. However, if one's characteristics are typical of an early adopter , rather than typical of an average member over the project's lifespan, then it may not be reasonable to assume one has joined the project at a random point in its life. For instance, the mainstream of potential users will prefer to be involved when the project is nearly complete. However, if one were to enjoy the project's incompleteness, it is already known that he or she is unusual, before the discovery of his or her early involvement. If one has measurable attributes that set one apart from the typical long-run user, the project doomsday argument can be refuted based on the fact that one could expect to be within the first 5% of members, a priori . The analogy to the total-human-population form of the argument is that confidence in a prediction of the distribution of human characteristics that places modern and historic humans outside the mainstream implies that it is already known, before examining n , that it is likely to be very early in N . This is an argument for changing the reference class. For example, if one is certain that 99% of humans who will ever live will be cyborgs , but that only a negligible fraction of humans who have been born to date are cyborgs, one could be equally certain that at least one hundred times as many people remain to be born as have been. Robin Hanson 's paper sums up these criticisms of the doomsday argument: [ 8 ] .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} All else is not equal; we have good reasons for thinking we are not randomly selected humans from all who will ever live. Human extinction is distant, a posteriori The a posteriori observation that extinction level events are rare could be offered as evidence that the doomsday argument's predictions are implausible; typically, extinctions of dominant species happen less often than once in a million years. Therefore, it is argued that human extinction is unlikely within the next ten millennia. (Another probabilistic argument , drawing a different conclusion than the doomsday argument.) In Bayesian terms, this response to the doomsday argument says that our knowledge of history (or ability to prevent disaster) produces a prior marginal for N with a minimum value in the trillions. If N is distributed uniformly from 10 12 to 10 13 , for example, then the probability of N &lt; 1,200 billion inferred from n = 60 billion will be extremely small. This is an equally impeccable Bayesian calculation, rejecting the Copernican principle because we must be 'special observers' since there is no likely mechanism for humanity to go extinct within the next hundred thousand years. This response is accused of overlooking the technological threats to humanity's survival , to which earlier life was not subject, and is specifically rejected by most [ by whom? – Discuss ] academic critics of the doomsday argument (arguably excepting Robin Hanson ). The prior N distribution may make n very uninformative Robin Hanson argues that N ' s prior may be exponentially distributed : [ 8 ] N = e U ( 0 , q ] c {\\displaystyle N={\\frac {e^{U(0,q]}}{c}}} Here, c and q are constants. If q is large, then our 95% confidence upper bound is on the uniform draw, not the exponential value of N . The simplest way to compare this with Gott's Bayesian argument is to flatten the distribution from the vague prior by having the probability fall off more slowly with N (than inverse proportionally). This corresponds to the idea that humanity's growth may be exponential in time with doomsday having a vague prior probability density function in time . This would mean that N , the last birth, would have a distribution looking like the following: Pr ( N ) = k N α , 0 &lt; α &lt; 1. {\\displaystyle \\Pr(N)={\\frac {k}{N^{\\alpha }}},0&lt;\\alpha &lt;1.} This prior N distribution is all that is required (with the principle of indifference) to produce the inference of N from n , and this is done in an identical way to the standard case, as described by Gott (equivalent to α {\\displaystyle \\alpha } = 1 in this distribution): Pr ( n ) = ∫ N = n N = ∞ Pr ( n ∣ N ) Pr ( N ) d N = ∫ n ∞ k N ( α + 1 ) d N = k α n α {\\displaystyle \\Pr(n)=\\int _{N=n}^{N=\\infty }\\Pr(n\\mid N)\\Pr(N)\\,dN=\\int _{n}^{\\infty }{\\frac {k}{N^{(\\alpha +1)}}}\\,dN={\\frac {k}{{\\alpha }n^{\\alpha }}}} Substituting into the posterior probability equation): Pr ( N ∣ n ) = α n α N ( 1 + α ) . {\\displaystyle \\Pr(N\\mid n)={\\frac {{\\alpha }n^{\\alpha }}{N^{(1+\\alpha )}}}.} Integrating the probability of any N above xn : xn) = \\\\int_{N=xn}^{N=\\\\infty} \\\\Pr(N\\\\mid n)\\\\,dN = \\\\frac{1}{x^{\\\\alpha}}.\"}}' id=\"mwAXI\"> xn)=\\int _{N=xn}^{N=\\infty }\\Pr(N\\mid n)\\,dN={\\frac {1}{x^{\\alpha }}}.}\"> Pr ( N > x n ) = ∫ N = x n N = ∞ Pr ( N ∣ n ) d N = 1 x α . {\\displaystyle \\Pr(N>xn)=\\int _{N=xn}^{N=\\infty }\\Pr(N\\mid n)\\,dN={\\frac {1}{x^{\\alpha }}}.} xn)=\\int _{N=xn}^{N=\\infty }\\Pr(N\\mid n)\\,dN={\\frac {1}{x^{\\alpha }}}.}\"/> For example, if x = 20, and α {\\displaystyle \\alpha } = 0.5, this becomes: 20n) = \\\\frac{1}{\\\\sqrt{20}} \\\\simeq 22.3\\\\%. \"}}' id=\"mwAXg\"> 20n)={\\frac {1}{\\sqrt {20}}}\\simeq 22.3\\%.}\"> Pr ( N > 20 n ) = 1 20 ≃ 22.3 % . {\\displaystyle \\Pr(N>20n)={\\frac {1}{\\sqrt {20}}}\\simeq 22.3\\%.} 20n)={\\frac {1}{\\sqrt {20}}}\\simeq 22.3\\%.}\"/> Therefore, with this prior, the chance of a trillion births is well over 20%, rather than the 5% chance given by the standard DA. If α {\\displaystyle \\alpha } is reduced further by assuming a flatter prior N distribution, then the limits on N given by n become weaker. An α {\\displaystyle \\alpha } of one reproduces Gott's calculation with a birth reference class, and α {\\displaystyle \\alpha } around 0.5 could approximate his temporal confidence interval calculation (if the population were expanding exponentially). As α → 0 {\\displaystyle \\alpha \\to 0} (gets smaller) n becomes less and less informative about N . In the limit this distribution approaches an (unbounded) uniform distribution , where all values of N are equally likely. This is Page et al.'s \"Assumption 3\", which they find few reasons to reject, a priori . (Although all distributions with α ≤ 1 {\\displaystyle \\alpha \\leq 1} are improper priors, this applies to Gott's vague-prior distribution also, and they can all be converted to produce proper integrals by postulating a finite upper population limit.) Since the probability of reaching a population of size 2 N is usually thought of as the chance of reaching N multiplied by the survival probability from N to 2 N it follows that Pr( N ) must be a monotonically decreasing function of N , but this doesn't necessarily require an inverse proportionality. [ 8 ] Infinite expectation Another objection to the doomsday argument is that the expected total human population is actually infinite . [ 9 ] The calculation is as follows: The total human population N = n / f , where n is the human population to date and f is our fractional position in the total. We assume that f is uniformly distributed on (0,1] . The expectation of N is E ( N ) = ∫ 0 1 n f d f = n [ ln ⁡ ( f ) ] 0 1 = n ln ⁡ ( 1 ) − n ln ⁡ ( 0 ) = + ∞ . {\\displaystyle E(N)=\\int _{0}^{1}{n \\over f}\\,df=n[\\ln(f)]_{0}^{1}=n\\ln(1)-n\\ln(0)=+\\infty .} For a similar example of counterintuitive infinite expectations, see the St. Petersburg paradox . Self-indication assumption: The possibility of not existing at all Main article: Self-indication assumption doomsday argument rebuttal One objection is that the possibility of a human existing at all depends on how many humans will ever exist ( N ). If this is a high number, then the possibility of their existing is higher than if only a few humans will ever exist. Since they do indeed exist, this is evidence that the number of humans that will ever exist is high. [ 10 ] This objection, originally by Dennis Dieks (1992), [ 11 ] is now known by Nick Bostrom 's name for it: the \" Self-Indication Assumption objection\". It can be shown that some SIAs prevent any inference of N from n (the current population). [ 12 ] The SIA has been defended by Matthew Adelstein, arguing that all alternatives to the SIA imply the soundness of the doomsday argument, and other even stranger conclusions. [ 13 ] Caves' rebuttal The Bayesian argument by Carlton M. Caves states that the uniform distribution assumption is incompatible with the Copernican principle , not a consequence of it. [ 14 ] Caves gives a number of examples to argue that Gott's rule is implausible. For instance, he says, imagine stumbling into a birthday party, about which you know nothing: Your friendly enquiry about the age of the celebrant elicits the reply that she is celebrating her ( t p =) 50th birthday. According to Gott, you can predict with 95% confidence that the woman will survive between [50]/39 = 1.28 years and 39[×50] = 1,950 years into the future. Since the wide range encompasses reasonable expectations regarding the woman's survival, it might not seem so bad, till one realizes that [Gott's rule] predicts that with probability 1/2 the woman will survive beyond 100 years old and with probability 1/3 beyond 150. Few of us would want to bet on the woman's survival using Gott's rule. (See Caves' online paper below .) Cave's example example exposes a weakness in J. Richard Gott 's \"Copernicus method\" DA: it does not specify when the \"Copernicus method\" can be applied. But this criticism is less effective against more refined versions of the argument. Epistemological refinements of Gott's argument by philosophers such as Nick Bostrom specify that: Knowing the absolute birth rank ( n ) must give no information on the total population ( N ). Careful DA variants specified with this rule aren't shown implausible by Caves' \"Old Lady\" example above, because the woman's age is given prior to the estimate of her lifespan. Since human age gives an estimate of survival time (via actuarial tables) Caves' Birthday party age-estimate could not fall into the class of DA problems defined with this proviso. To produce a comparable \"Birthday Party Example\" of the carefully specified Bayesian DA, we would need to completely exclude all prior knowledge of likely human life spans; in principle this could be done (e.g.: hypothetical Amnesia chamber). However, this would remove the modified example from everyday experience. To keep it in the everyday realm the lady's age must be hidden prior to the survival estimate being made. (Although this is no longer exactly the DA, it is much more comparable to it.) Without knowing the lady's age, the DA reasoning produces a rule to convert the birthday ( n ) into a maximum lifespan with 50% confidence ( N ). Gott's Copernicus method rule is simply: Prob ( N &lt; 2 n ) = 50%. How accurate would this estimate turn out to be? Western demographics are now fairly uniform across ages, so a random birthday ( n ) could be (very roughly) approximated by a U(0, M ] draw where M is the maximum lifespan in the census. In this 'flat' model, everyone shares the same lifespan so N = M . If n happens to be less than ( M )/2 then Gott's 2 n estimate of N will be under M , its true figure. The other half of the time 2 n underestimates M , and in this case (the one Caves highlights in his example) the subject will die before the 2 n estimate is reached. In this \"flat demographics\" model Gott's 50% confidence figure is proven right 50% of the time. Self-referencing doomsday argument rebuttal Main article: Self-referencing doomsday argument rebuttal Some philosophers have suggested that only people who have contemplated the doomsday argument (DA) belong in the reference class \" human \". If that is the appropriate reference class, Carter defied his own prediction when he first described the argument (to the Royal Society ). An attendant could have argued thus: Presently, only one person in the world understands the Doomsday argument, so by its own logic there is a 95% chance that it is a minor problem which will only ever interest twenty people, and I should ignore it. Jeff Dewynne and Professor Peter Landsberg suggested that this line of reasoning will create a paradox for the doomsday argument: [ 9 ] If a member of the Royal Society did pass such a comment, it would indicate that they understood the DA sufficiently well that in fact 2 people could be considered to understand it, and thus there would be a 5% chance that 40 or more people would actually be interested. Also, of course, ignoring something because you only expect a small number of people to be interested in it is extremely short sighted—if this approach were to be taken, nothing new would ever be explored, if we assume no a priori knowledge of the nature of interest and attentional mechanisms. Conflation of future duration with total duration Various authors have argued that the doomsday argument rests on an incorrect conflation of future duration with total duration. This occurs in the specification of the two time periods as \"doom soon\" and \"doom deferred\" which means that both periods are selected to occur after the observed value of the birth order. A rebuttal in Pisaturo (2009) [ 15 ] argues that the doomsday argument relies on the equivalent of this equation: P ( H T S | D p X ) / P ( H T L | D p X ) = [ P ( H F S | X ) / P ( H F L | X ) ] ⋅ [ P ( D p | H T S X ) / P ( D p | H T L X ) ] {\\displaystyle P(H_{TS}|D_{p}X)/P(H_{TL}|D_{p}X)=[P(H_{FS}|X)/P(H_{FL}|X)]\\cdot [P(D_{p}|H_{TS}X)/P(D_{p}|H_{TL}X)]} , where: X = the prior information; D p = the data that past duration is t p ; H FS = the hypothesis that the future duration of the phenomenon will be short; H FL = the hypothesis that the future duration of the phenomenon will be long; H TS = the hypothesis that the total duration of the phenomenon will be short—i.e., that t t , the phenomenon's total longevity, = t TS ; H TL = the hypothesis that the total duration of the phenomenon will be long—i.e., that t t , the phenomenon's total longevity, = t TL , with t TL > t TS . Pisaturo then observes: Clearly, this is an invalid application of Bayes' theorem, as it conflates future duration and total duration. Pisaturo takes numerical examples based on two possible corrections to this equation: considering only future durations and considering only total durations. In both cases, he concludes that the doomsday argument's claim, that there is a \"Bayesian shift\" in favor of the shorter future duration, is fallacious. This argument is also echoed in O'Neill (2014). [ 16 ] In this work O'Neill argues that a unidirectional \"Bayesian Shift\" is an impossibility within the standard formulation of probability theory and is contradictory to the rules of probability. As with Pisaturo, he argues that the doomsday argument conflates future duration with total duration by specification of doom times that occur after the observed birth order. According to O'Neill: The reason for the hostility to the doomsday argument and its assertion of a \"Bayesian shift\" is that many people who are familiar with probability theory are implicitly aware of the absurdity of the claim that one can have an automatic unidirectional shift in beliefs regardless of the actual outcome that is observed. This is an example of the \"reasoning to a foregone conclusion\" that arises in certain kinds of failures of an underlying inferential mechanism. An examination of the inference problem used in the argument shows that this suspicion is indeed correct, and the doomsday argument is invalid. (pp. 216-217) Confusion over the meaning of confidence intervals Gelman and Robert [ 17 ] assert that the doomsday argument confuses frequentist confidence intervals with Bayesian credible intervals . Suppose that every individual knows their number n and uses it to estimate an upper bound on N . Every individual has a different estimate, and these estimates are constructed so that 95% of them contain the true value of N and the other 5% do not. This, say Gelman and Robert, is the defining property of a frequentist lower-tailed 95% confidence interval. But, they say, \"this does not mean that there is a 95% chance that any particular interval will contain the true value.\" That is, while 95% of the confidence intervals will contain the true value of N , this is not the same as N being contained in the confidence interval with 95% probability. The latter is a different property and is the defining characteristic of a Bayesian credible interval. Gelman and Robert conclude: the Doomsday argument is the ultimate triumph of the idea, beloved among Bayesian educators, that our students and clients do not really understand Neyman–Pearson confidence intervals and inevitably give them the intuitive Bayesian interpretation. See also Anthropic principle Human overpopulation German tank problem Global catastrophic risk Doomsday event Fermi paradox Measure problem (cosmology) Mediocrity principle Quantum suicide and immortality Simulated reality Survival analysis Survivalism Technological singularity Notes .mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman} ↑ The only probability density functions that must be specified a priori are: Pr( N ) - the ultimate number of people that will be born, assumed by J. Richard Gott to have a vague prior distribution, Pr( N ) = k / N Pr( n | N ) - the chance of being born in any position based on a total population N - all DA forms assume the Copernican principle , making Pr( n | N ) = 1/ N From these two distributions, the doomsday argument proceeds to create a Bayesian inference on the distribution of N from n , through Bayes' rule , which requires P( n ); to produce this, integrate over all the possible values of N which might contain an individual born n th (that is, wherever N > n ): P ( n ) = ∫ N = n N = ∞ P ( n ∣ N ) P ( N ) d N = ∫ n ∞ k N 2 d N {\\displaystyle P(n)=\\int _{N=n}^{N=\\infty }P(n\\mid N)P(N)\\,dN=\\int _{n}^{\\infty }{\\frac {k}{N^{2}}}\\,dN} = k n . {\\displaystyle ={\\frac {k}{n}}.} This is why the marginal distribution of n and N are identical in the case of P( N ) = k / N ↑ See, for example, Introduction to Social Macrodynamics by Andrey Korotayev et al. References ↑ .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}} Brandon Carter ; McCrea, W. H. (1983). \"The anthropic principle and its implications for biological evolution\". Philosophical Transactions of the Royal Society of London . A310 (1512): 347– 363. Bibcode : 1983RSPTA.310..347C . doi : 10.1098/rsta.1983.0096 . S2CID 92330878 . ↑ J. Richard Gott, III (1993). \"Implications of the Copernican principle for our future prospects\". Nature . 363 (6427): 315– 319. Bibcode : 1993Natur.363..315G . doi : 10.1038/363315a0 . S2CID 4252750 . ↑ Holger Bech Nielsen (1989). \"Random dynamics and relations between the number of fermion generations and the fine structure constants\". Acta Physica Polonica . B20 : 427– 468. ↑ Oliver, Jonathan; Korb, Kevin (1998). \"A Bayesian Analysis of the Doomsday Argument\". Philosophy . CiteSeerX 10.1.1.49.5899 . ↑ Korb, K. (1998). \"A refutation of the doomsday argument\". Mind . 107 (426): 403– 410. doi : 10.1093/mind/107.426.403 . ↑ Timothy Ferris (July 12, 1999). \"How to Predict Everything\" . The New Yorker . Retrieved September 3, 2010 . ↑ Bostrom, Nick (2005). \"Self-Location and Observation Selection Theory\" . anthropic-principle.com . Retrieved 2023-07-02 . 1 2 3 \"Critiquing the Doomsday Argument\" . mason.gmu.edu . Retrieved 2023-06-17 . 1 2 Monton, Bradley; Roush, Sherri (2001-11-20). \"Gott's Doomsday Argument\" . philsci-archive.pitt.edu . Retrieved 2023-06-17 . ↑ Olum, Ken D. (2002). \"The doomsday argument and the number of possible observers\". The Philosophical Quarterly . 52 (207): 164. arXiv : gr-qc/0009081 . doi : 10.1111/1467-9213.00260 . S2CID 14707647 . ↑ Dieks, Dennis (2005-01-13). \"Reasoning About the Future: Doom and Beauty\" . philsci-archive.pitt.edu . Retrieved 2023-06-17 . ↑ Bostrom, Nick (2002). Anthropic Bias: Observational Selection Effects in Science and Philosophy . New York &amp; London: Routledge. pp. 124– 126. ISBN 0-415-93858-9 . ↑ Adelstein, Matthew (2024). \"Alternatives to the self-indication assumption are doomed\" . Synthese . 204 23: 1– 17. doi : 10.1007/s11229-024-04686-w . ↑ Caves, Carlton M. (2008). \"Predicting future duration from present age: Revisiting a critical assessment of Gott's rule\". arXiv : 0806.3538 [ astro-ph ]. ↑ Ronald Pisaturo (2009). \"Past Longevity as Evidence for the Future\". Philosophy of Science . 76 : 73– 100. doi : 10.1086/599273 . S2CID 122207511 . ↑ Ben O'Neill (2014). \"Assessing the 'Bayesian Shift' in the Doomsday Argument\". Journal of Philosophy . 111 (4): 198– 218. doi : 10.5840/jphil2014111412 . ↑ Andrew Gelman; Christian P. Robert (2013). \" 'Not Only Defended But Also Applied': The Perceived Absurdity of Bayesian Inference\". The American Statistician . 67 (4): 1– 5. arXiv : 1006.5366 . doi : 10.1080/00031305.2013.760987 . S2CID 10833752 . Further reading John A. Leslie , The End of the World: The Science and Ethics of Human Extinction , Routledge, 1998, ISBN 0-41518447-9 . J. R. Gott III , Future Prospects Discussed , Nature, vol. 368, p. 108, 1994. This argument plays a central role in Stephen Baxter 's science fiction book, Manifold: Time , Del Rey Books, 2000, ISBN 0-345-43076-X . The same principle plays a major role in the Dan Brown novel, Inferno , Corgy Books, ISBN 978-0-552-16959-2 Poundstone, William , The Doomsday Calculation: How an Equation that Predicts the Future Is Transforming Everything We Know About Life and the Universe . 2019 Little, Brown Spark. Description &amp; arrow/scrollable preview. Also summarised in Poundstone's essay, \"Math Says Humanity May Have Just 760 Years Left\" . The Wall Street Journal , updated June 27, 2019. ISBN 9783164440707 External links This article's use of external links may not follow Wikipedia's policies or guidelines . Please improve this article by removing excessive or inappropriate external links, and converting useful links where appropriate into footnote references . ( November 2023 ) ( Learn how and when to remove this message ) The Doomsday argument category on PhilPapers A non-mathematical, unpartisan introduction to the DA Nick Bostrom's response to Korb and Oliver Nick Bostrom's annotated collection of references Kopf, Krtouš &amp; Page's early (1994) refutation based on the SIA , which they called \"Assumption 2\". The Doomsday argument and the number of possible observers by Ken Olum In 1993 J. Richard Gott used his \"Copernicus method\" to predict the lifetime of Broadway shows. One part of this paper uses the same reference class as an empirical counter-example to Gott's method. A Critique of the Doomsday Argument by Robin Hanson A Third Route to the Doomsday Argument by Paul Franceschi , Journal of Philosophical Research , 2009, vol. 34, pp. 263–278 Chambers' Ussherian Corollary Objection Caves' Bayesian critique of Gott's argument. C. M. Caves, \"Predicting future duration from present age: A critical assessment\", Contemporary Physics 41, 143-153 (2000). C.M. Caves, \"Predicting future duration from present age: Revisiting a critical assessment of Gott's rule. \"Infinitely Long Afterlives and the Doomsday Argument\" by John Leslie shows that Leslie has recently modified his analysis and conclusion (Philosophy 83 (4) 2008 pp. 519–524): Abstract—A recent book of mine defends three distinct varieties of immortality. One of them is an infinitely lengthy afterlife; however, any hopes of it might seem destroyed by something like Brandon Carter's 'doomsday argument' against viewing ourselves as extremely early humans. The apparent difficulty might be overcome in two ways. First, if the world is non-deterministic then anything on the lines of the doomsday argument may prove unable to deliver a strongly pessimistic conclusion. Secondly, anything on those lines may break down when an infinite sequence of experiences is in question. Mark Greenberg, \"Apocalypse Not Just Now\" in London Review of Books Laster : A simple webpage applet giving the min &amp; max survival times of anything with 50% and 95% confidence requiring only that you input how old it is. It is designed to use the same mathematics as J. Richard Gott 's form of the DA, and was programmed by sustainable development researcher Jerrad Pierce. PBS Space Time The Doomsday Argument .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd;color:inherit}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf;color:inherit}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf;color:inherit}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff;color:inherit}.mw-parser-output .navbox-even{background-color:#f7f7f7;color:inherit}.mw-parser-output .navbox-odd{background-color:transparent;color:inherit}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}} .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e Global catastrophic risks Future of the Earth Future of an expanding universe Ultimate fate of the universe Human extinction risk estimates Technological Chemical warfare Cyberattack Cyberwarfare Cyberterrorism Cybergeddon Ransomware Gray goo Nanoweapons Kinetic bombardment Kinetic energy weapon Nuclear warfare Mutual assured destruction Dead Hand Doomsday Clock Doomsday device Antimatter weapon Electromagnetic pulse (EMP) Safety of high-energy particle collision experiments Micro black hole Strangelet Synthetic intelligence / Artificial intelligence AI takeo",
  "cached_at": "2025-10-25T19:37:37.198663"
}