{
  "title": "Chinese room",
  "summary": "The Chinese room argument holds that a computer executing a program cannot have a mind, understanding, or consciousness, regardless of how intelligently or human-like the program may make the computer behave. The argument was presented in a 1980 paper by the philosopher John Searle entitled \"Minds, Brains, and Programs\" and published in the journal Behavioral and Brain Sciences. Similar arguments had been made by Gottfried Wilhelm Leibniz (1714), Ned Block (1978) and others. Searle's version has been widely discussed in the years since. The centerpiece of Searle's argument is a thought experiment known as the Chinese room.",
  "content": "Chinese room Thought experiment on artificial intelligence .mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}} For the British video game development studio, see The Chinese Room . .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"} .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}} Part of a series on Artificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety Approaches Machine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Open-source Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy AI alignment Artificial consciousness The bitter lesson Chinese room Friendly AI Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom AI bubble Controversies Deepfake pornography Taylor Swift deepfake pornography controversy Google Gemini image generation controversy Pause Giant AI Experiments Removal of Sam Altman from OpenAI Statement on AI Risk Tay (chatbot) Théâtre D'opéra Spatial Voiceverse NFT plagiarism scandal Glossary Glossary .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}} v t e The Chinese room argument holds that a computer executing a program cannot have a mind , understanding , or consciousness , [ a ] regardless of how intelligently or human-like the program may make the computer behave. The argument was presented in a 1980 paper by the philosopher John Searle entitled \"Minds, Brains, and Programs\" and published in the journal Behavioral and Brain Sciences . [ 1 ] Similar arguments had been made by Gottfried Wilhelm Leibniz (1714), Ned Block (1978) and others. Searle's version has been widely discussed in the years since. [ 2 ] The centerpiece of Searle's argument is a thought experiment known as the Chinese room . [ 3 ] The argument is directed against the philosophical positions of functionalism and computationalism , [ 4 ] which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls the strong AI hypothesis : [ b ] \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" [ c ] Although its proponents originally presented the argument in reaction to statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of intelligent behavior a machine can display. [ 5 ] The argument applies only to digital computers running programs and does not apply to machines in general. [ 6 ] While widely discussed, the argument has been subject to significant criticism and remains controversial among philosophers of mind and AI researchers. [ 7 ] [ 8 ] Chinese room thought experiment Suppose that artificial intelligence research has succeeded in programming a computer to behave as if it understands Chinese. The machine accepts Chinese characters as input, carries out each instruction of the program step by step, and then produces Chinese characters as output. The machine does this so perfectly that no one can tell that they are communicating with a machine and not a hidden Chinese speaker. [ 6 ] The questions at issue are these: does the machine actually understand the conversation, or is it just simulating the ability to understand the conversation? Does the machine have a mind in exactly the same sense that people do, or is it just acting as if it had a mind? [ 6 ] Now suppose that Searle is in a room with an English version of the program, along with sufficient pencils, paper, erasers and filing cabinets. Chinese characters are slipped in under the door, he follows the program step-by-step, which eventually instructs him to slide other Chinese characters back out under the door. If the computer had passed the Turing test this way, it follows that Searle would do so as well, simply by running the program by hand. [ 6 ] Searle can see no essential difference between the roles of the computer CPU&lt;/em>\"}},\"i\":0}}]}'> [ d ] and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that makes them appear to understand. However, Searle would not be able to understand the conversation. Therefore, he argues, it follows that the computer would not be able to understand the conversation either. [ 6 ] Searle argues that, without \"understanding\" (or \" intentionality \"), we cannot describe what the machine is doing as \"thinking\" and, since it does not think, it does not have a \"mind\" in the normal sense of the word. Therefore, he concludes that the strong AI hypothesis is false: a computer running a program that simulates a mind would not have a mind in the same sense that human beings have a mind. [ 6 ] History Gottfried Leibniz made a similar argument in 1714 against mechanism (the idea that everything that makes up a human being could, in principle, be explained in mechanical terms. In other words, that a person, including their mind, is merely a very complex machine). Leibniz used the thought experiment of expanding the brain until it was the size of a mill. [ 9 ] Leibniz found it difficult to imagine that a \"mind\" capable of \"perception\" could be constructed using only mechanical processes. [ e ] Peter Winch made the same point in his book The Idea of a Social Science and its Relation to Philosophy (1958), where he provides an argument to show that \"a man who understands Chinese is not a man who has a firm grasp of the statistical probabilities for the occurrence of the various words in the Chinese language\" (p. 108). Soviet cyberneticist Anatoly Dneprov made an essentially identical argument in 1961, in the form of the short story \" The Game \". In it, a stadium of people act as switches and memory cells implementing a program to translate a sentence of Portuguese, a language that none of them know. [ 10 ] The game was organized by a \"Professor Zarubin\" to answer the question \"Can mathematical machines think?\" Speaking through Zarubin, Dneprov writes \"the only way to prove that machines can think is to turn yourself into a machine and examine your thinking process\" and he concludes, as Searle does, \"We've proven that even the most perfect simulation of machine thinking is not the thinking process itself.\" In 1974, Lawrence H. Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called the China brain , also the \"Chinese Nation\" or the \"Chinese Gym\". [ 11 ] John Searle in December 2005 Searle's version appeared in his 1980 paper \"Minds, Brains, and Programs\", published in Behavioral and Brain Sciences . [ 1 ] It eventually became the journal's \"most influential target article\", [ 2 ] generating an enormous number of commentaries and responses in the ensuing decades, and Searle has continued to defend and refine the argument in multiple papers, popular articles and books. David Cole writes that \"the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear in the past 25 years\". [ 12 ] Most of the discussion consists of attempts to refute it. \"The overwhelming majority\", notes Behavioral and Brain Sciences editor Stevan Harnad , [ f ] \"still think that the Chinese Room Argument is dead wrong\". [ 13 ] The sheer volume of the literature that has grown up around it inspired Pat Hayes to comment that the field of cognitive science ought to be redefined as \"the ongoing research program of showing Searle's Chinese Room Argument to be false\". [ 14 ] Searle's argument has become \"something of a classic in cognitive science\", according to Harnad. [ 13 ] Varol Akman agrees, and has described the original paper as \"an exemplar of philosophical clarity and purity\". [ 15 ] Philosophy Although the Chinese Room argument was originally presented in reaction to the statements of artificial intelligence researchers, philosophers have come to consider it as an important part of the philosophy of mind . It is a challenge to functionalism and the computational theory of mind , [ g ] and is related to such questions as the mind–body problem , the problem of other minds , the symbol grounding problem, and the hard problem of consciousness . [ a ] Strong AI Searle identified a philosophical position he calls \" strong AI \" : as if&lt;/em> they were intelligent—and strong AI—the assertions that do so are &lt;em>actually&lt;/em> consciously thinking (not just &lt;em>simulating&lt;/em> thinking).\\\"{{sfn|Russell|Norvig|2021|p=981}}}}\\n\"}},\"i\":0}}]}' id=\"mw0g\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds. [ c ] The definition depends on the distinction between simulating a mind and actually having one. Searle writes that \"according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind.\" [ 22 ] The claim is implicit in some of the statements of early AI researchers and analysts. For example, in 1957, the economist and psychologist Herbert A. Simon declared that \"there are now in the world machines that think, that learn and create\". [ 23 ] Simon, together with Allen Newell and Cliff Shaw , after having completed the first program that could do formal reasoning (the Logic Theorist ), claimed that they had \"solved the venerable mind–body problem, explaining how a system composed of matter can have the properties of mind.\" [ 24 ] John Haugeland wrote that \"AI wants only the genuine article: machines with minds , in the full and literal sense. This is not science fiction, but real science, based on a theoretical conception as deep as it is daring: namely, we are, at root, computers ourselves .\" [ 25 ] Searle also ascribes the following claims to advocates of strong AI: AI systems can be used to explain the mind; [ 20 ] The study of the brain is irrelevant to the study of the mind; [ h ] and The Turing test is adequate for establishing the existence of mental states. [ i ] Strong AI as computationalism or functionalism In more recent presentations of the Chinese room argument, Searle has identified \"strong AI\" as \"computer functionalism \" (a term he attributes to Daniel Dennett ). [ 4 ] [ 30 ] Functionalism is a position in modern philosophy of mind that holds that we can define mental phenomena (such as beliefs, desires, and perceptions) by describing their functions in relation to each other and to the outside world. Because a computer program can accurately represent functional relationships as relationships between symbols, a computer can have mental phenomena if it runs the right program, according to functionalism. Stevan Harnad argues that Searle's depictions of strong AI can be reformulated as \"recognizable tenets of computationalism , a position (unlike \"strong AI\") that is actually held by many thinkers, and hence one worth refuting.\" [ 31 ] Computationalism [ j ] is the position in the philosophy of mind which argues that the mind can be accurately described as an information-processing system. Each of the following, according to Harnad, is a \"tenet\" of computationalism: [ 34 ] Mental states are computational states (which is why computers can have mental states and help to explain the mind); Computational states are implementation-independent —in other words, it is the software that determines the computational state, not the hardware (which is why the brain, being hardware, is irrelevant); and that Since implementation is unimportant, the only empirical data that matters is how the system functions; hence the Turing test is definitive. Recent philosophical discussions have revisited the implications of computationalism for artificial intelligence. Goldstein and Levinstein explore whether large language models (LLMs) like ChatGPT can possess minds, focusing on their ability to exhibit folk psychology, including beliefs, desires, and intentions. The authors argue that LLMs satisfy several philosophical theories of mental representation, such as informational, causal, and structural theories, by demonstrating robust internal representations of the world. However, they highlight that the evidence for LLMs having action dispositions necessary for belief-desire psychology remains inconclusive. Additionally, they refute common skeptical challenges, such as the \" stochastic parrots \" argument and concerns over memorization, asserting that LLMs exhibit structured internal representations that align with these philosophical criteria. [ 35 ] David Chalmers suggests that while current LLMs lack features like recurrent processing and unified agency, advancements in AI could address these limitations within the next decade, potentially enabling systems to achieve consciousness. This perspective challenges Searle's original claim that purely \"syntactic\" processing cannot yield understanding or consciousness, arguing instead that such systems could have authentic mental states. [ 36 ] Strong AI vs. biological naturalism Searle holds a philosophical position he calls \" biological naturalism \": that consciousness [ a ] and understanding require specific biological machinery that is found in brains. He writes \"brains cause minds\" [ 37 ] and that \"actual human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains\". [ 37 ] Searle argues that this machinery (known in neuroscience as the \" neural correlates of consciousness \") must have some causal powers that permit the human experience of consciousness. [ 38 ] Searle's belief in the existence of these powers has been criticized. Searle does not disagree with the notion that machines can have consciousness and understanding, because, as he writes, \"we are precisely such machines\". [ 6 ] Searle holds that the brain is, in fact, a machine, but that the brain gives rise to consciousness and understanding using specific machinery. If neuroscience is able to isolate the mechanical process that gives rise to consciousness, then Searle grants that it may be possible to create machines that have consciousness and understanding. However, without the specific machinery required, Searle does not believe that consciousness can occur. Biological naturalism implies that one cannot determine if the experience of consciousness is occurring merely by examining how a system functions, because the specific machinery of the brain is essential. Thus, biological naturalism is directly opposed to both behaviorism and functionalism (including \"computer functionalism\" or \"strong AI\"). [ 39 ] Biological naturalism is similar to identity theory (the position that mental states are \"identical to\" or \"composed of\" neurological events); however, Searle has specific technical objections to identity theory. [ 40 ] [ k ] Searle's biological naturalism and strong AI are both opposed to Cartesian dualism , [ 39 ] the classical idea that the brain and mind are made of different \"substances\". Indeed, Searle accuses strong AI of dualism, writing that \"strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter\". [ 26 ] Consciousness Searle's original presentation emphasized understanding—that is, mental states with intentionality —and did not directly address other closely related ideas such as \"consciousness\". However, in more recent presentations, Searle has included consciousness as the real target of the argument. [ 4 ] Computational models of consciousness are not sufficient by themselves for consciousness. The computational model for consciousness stands to consciousness in the same way the computational model of anything stands to the domain being modelled. Nobody supposes that the computational model of rainstorms in London will leave us all wet. But they make the mistake of supposing that the computational model of consciousness is somehow conscious. It is the same mistake in both cases. [ 41 ] — John R. Searle, Consciousness and Language , p. 16 David Chalmers writes, \"it is fairly clear that consciousness is at the root of the matter\" of the Chinese room. [ 42 ] Colin McGinn argues that the Chinese room provides strong evidence that the hard problem of consciousness is fundamentally insoluble. The argument, to be clear, is not about whether a machine can be conscious, but about whether it (or anything else for that matter) can be shown to be conscious. It is plain that any other method of probing the occupant of a Chinese room has the same difficulties in principle as exchanging questions and answers in Chinese. It is simply not possible to divine whether a conscious agency or some clever simulation inhabits the room. [ 43 ] Searle argues that this is only true for an observer outside of the room. The whole point of the thought experiment is to put someone inside the room, where they can directly observe the operations of consciousness. Searle claims that from his vantage point within the room there is nothing he can see that could imaginably give rise to consciousness, other than himself, and clearly he does not have a mind that can speak Chinese. In Searle's words, \"the computer has nothing more than I have in the case where I understand nothing\". [ 44 ] Applied ethics Sitting in the combat information center aboard a warship —proposed as a real-life analog to the Chinese room Patrick Hew used the Chinese Room argument to deduce requirements from military command and control systems if they are to preserve a commander's moral agency . He drew an analogy between a commander in their command center and the person in the Chinese Room, and analyzed it under a reading of Aristotle's notions of \"compulsory\" and \"ignorance\" . Information could be \"down converted\" from meaning to symbols, and manipulated symbolically, but moral agency could be undermined if there was inadequate 'up conversion' into meaning. Hew cited examples from the USS Vincennes incident . [ 45 ] Computer science The Chinese room argument is primarily an argument in the philosophy of mind, and both major computer scientists and artificial intelligence researchers consider it irrelevant to their fields. [ 5 ] However, several concepts developed by computer scientists are essential to understanding the argument, including symbol processing , Turing machines , Turing completeness , and the Turing test. Strong AI vs. AI research Searle's arguments are not usually considered an issue for AI research. The primary mission of artificial intelligence research is only to create useful systems that act intelligently and it does not matter if the intelligence is \"merely\" a simulation. AI researchers Stuart J. Russell and Peter Norvig wrote in 2021: \"We are interested in programs that behave intelligently. Individual aspects of consciousness—awareness, self-awareness, attention—can be programmed and can be part of an intelligent machine. The additional project making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" [ 5 ] Searle does not disagree that AI research can create machines that are capable of highly intelligent behavior. The Chinese room argument leaves open the possibility that a digital machine could be built that acts more intelligently than a person, but does not have a mind or intentionality in the same way that brains do. Searle's \"strong AI hypothesis\" should not be confused with \"strong AI\" as defined by Ray Kurzweil and other futurists, [ 46 ] [ 21 ] who use the term to describe machine intelligence that rivals or exceeds human intelligence—that is, artificial general intelligence , human level AI or superintelligence . Kurzweil is referring primarily to the amount of intelligence displayed by the machine, whereas Searle's argument sets no limit on this. Searle argues that a superintelligent machine would not necessarily have a mind and consciousness. Turing test Main article: Turing test The \"standard interpretation\" of the Turing Test, in which player C, the interrogator, is given the task of trying to determine which player—A or B—is a computer and which is a human. The interrogator is limited to using the responses to written questions to make the determination. Image adapted from Saygin, et al. 2000. [ 47 ] The Chinese room implements a version of the Turing test. [ 48 ] Alan Turing introduced the test in 1950 to help answer the question \"can machines think?\" In the standard version, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test. Turing then considered each possible objection to the proposal \"machines can think\", and found that there are simple, obvious answers if the question is de-mystified in this way. He did not, however, intend for the test to measure for the presence of \"consciousness\" or \"understanding\". He did not believe this was relevant to the issues that he was addressing. He wrote: I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper. [ 48 ] To Searle, as a philosopher investigating in the nature of mind and consciousness, these are the relevant mysteries. The Chinese room is designed to show that the Turing test is insufficient to detect the presence of consciousness, even if the room can behave or function as a conscious mind would. Symbol processing Main article: Physical symbol system Computers manipulate physical objects in order to carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a physical symbol system . It is also equivalent to the formal systems used in the field of mathematical logic . Searle emphasizes the fact that this kind of symbol manipulation is syntactic (borrowing a term from the study of grammar ). The computer manipulates the symbols using a form of syntax, without any knowledge of the symbol's semantics (that is, their meaning ). Newell and Simon had conjectured that a physical symbol system (such as a digital computer) had all the necessary machinery for \"general intelligent action\", or, as it is known today, artificial general intelligence . They framed this as a philosophical position, the physical symbol system hypothesis : \"A physical symbol system has the necessary and sufficient means for general intelligent action.\" [ 49 ] [ 50 ] The Chinese room argument does not refute this, because it is framed in terms of \"intelligent action\", i.e. the external behavior of the machine, rather than the presence or absence of understanding, consciousness and mind. Twenty-first century AI programs (such as \" deep learning \") do mathematical operations on huge matrixes of unidentified numbers and bear little resemblance to the symbolic processing used by AI programs at the time Searle wrote his critique in 1980. Nils Nilsson describes systems like these as \"dynamic\" rather than \"symbolic\". Nilsson notes that these are essentially digitized representations of dynamic systems—the individual numbers do not have a specific semantics, but are instead samples or data points from a dynamic signal, and it is the signal being approximated which would have semantics. Nilsson argues it is not reasonable to consider these signals as \"symbol processing\" in the same sense as the physical symbol systems hypothesis. [ 51 ] Chinese room and Turing completeness See also: Turing completeness and Church–Turing thesis The Chinese room has a design analogous to that of a modern computer. It has a Von Neumann architecture , which consists of a program (the book of instructions), some memory (the papers and file cabinets), a machine that follows the instructions (the man), and a means to write symbols in memory (the pencil and eraser). A machine with this design is known in theoretical computer science as \" Turing complete \", because it has the necessary machinery to carry out any computation that a Turing machine can do, and therefore it is capable of doing a step-by-step simulation of any other digital machine, given enough memory and time. Turing writes, \"all digital computers are in a sense equivalent.\" [ 52 ] The widely accepted Church–Turing thesis holds that any function computable by an effective procedure is computable by a Turing machine. The Turing completeness of the Chinese room implies that it can do whatever any other digital computer can do (albeit much, much more slowly). Thus, if the Chinese room does not or can not contain a Chinese-speaking mind, then no other digital computer can contain a mind. Some replies to Searle begin by arguing that the room, as described, cannot have a Chinese-speaking mind. Arguments of this form, according to Stevan Harnad , are \"no refutation (but rather an affirmation)\" [ 53 ] of the Chinese room argument, because these arguments actually imply that no digital computers can have a mind. [ 28 ] There are some critics, such as Hanoch Ben-Yami, who argue that the Chinese room cannot simulate all the abilities of a digital computer, such as being able to determine the current time. [ 54 ] Complete argument Searle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first version in 1984. The version given below is from 1990. [ 55 ] [ l ] The Chinese room thought experiment is intended to prove point A3. [ m ] He begins with three axioms: (A1) \"Programs are formal (syntactic).\" A program uses syntax to manipulate symbols and pays no attention to the semantics of the symbols. It knows where to put the symbols and how to move them around, but it does not know what they stand for or what they mean. For the program, the symbols are just physical objects like any others. (A2) \"Minds have mental contents (semantics).\" Unlike the symbols used by a program, our thoughts have meaning: they represent things and we know what it is they represent. (A3) \"Syntax by itself is neither constitutive of nor sufficient for semantics.\" This is what the Chinese room thought experiment is intended to prove: the Chinese room has syntax (because there is a man in there moving symbols around). The Chinese room has no semantics (because, according to Searle, there is no one or nothing in the room that understands what the symbols mean). Therefore, having syntax is not enough to generate semantics. Searle posits that these lead directly to this conclusion: (C1) Programs are neither constitutive of nor sufficient for minds. This should follow without controversy from the first three: Programs don't have semantics. Programs have only syntax, and syntax is insufficient for semantics. Every mind has semantics. Therefore no programs are minds. This much of the argument is intended to show that artificial intelligence can never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is the computational theory of mind correct? [ g ] He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds: (A4) Brains cause minds. Searle claims that we can derive \"immediately\" and \"trivially\" [ 56 ] that: (C2) Any other system capable of causing minds would have to have causal powers (at least) equivalent to those of brains. Brains must have something that causes a mind to exist. Science has yet to determine exactly what it is, but it must exist, because minds exist. Searle calls it \"causal powers\". \"Causal powers\" is whatever the brain uses to create a mind. If anything else can cause a mind to exist, it must have \"equivalent causal powers\". \"Equivalent causal powers\" is whatever else that could be used to make a mind. And from this he derives the further conclusions: (C3) Any artifact that produced mental phenomena, any artificial brain, would have to be able to duplicate the specific causal powers of brains, and it could not do that just by running a formal program. This follows from C1 and C2: Since no program can produce a mind, and \"equivalent causal powers\" produce minds, it follows that programs do not have \"equivalent causal powers.\" (C4) The way that human brains actually produce mental phenomena cannot be solely by virtue of running a computer program. Since programs do not have \"equivalent causal powers\", \"equivalent causal powers\" produce minds, and brains produce minds, it follows that brains do not use programs to produce minds. Refutations of Searle's argument take a number of different forms (see below). Computationalists and functionalists reject A3, arguing that \"syntax\" (as Searle describes it) can have \"semantics\" if the syntax has the right functional structure. Eliminative materialists reject A2, arguing that minds don't actually have \"semantics\"—that thoughts and other mental phenomena are inherently meaningless but nevertheless function as if they had meaning. Replies Replies to Searle's argument may be classified according to what they claim to show: [ n ] Those which identify who speaks Chinese Those which demonstrate how meaningless symbols can become meaningful Those which suggest that the Chinese room should be redesigned in some way Those which contend that Searle's argument is misleading Those which argue that the argument makes false assumptions about subjective conscious experience and therefore proves nothing Some of the arguments (robot and brain simulation, for example) fall into multiple categories. Systems and virtual mind replies: finding the mind These replies attempt to answer the question: since the man in the room does not speak Chinese, where is the mind that does? These replies address the key ontological issues of mind versus body and simulation vs. reality. All of the replies that identify the mind in the room are versions of \"the system reply\". System reply The basic version of the system reply argues that it is the \"whole system\" that understands Chinese. [ 61 ] [ o ] While the man understands only English, when he is combined with the program, scratch paper, pencils and file cabinets, they form a system that can understand Chinese. \"Here, understanding is not being ascribed to the mere individual; rather it is being ascribed to this whole system of which he is a part\" Searle explains. [ 29 ] Searle notes that (in this simple version of the reply) the \"system\" is nothing more than a collection of ordinary physical objects; it grants the power of understanding and consciousness to \"the conjunction of that person and bits of paper\" [ 29 ] without making any effort to explain how this pile of objects has become a conscious, thinking being. Searle argues that no reasonable person should be satisfied with the reply, unless they are \"under the grip of an ideology;\" [ 29 ] In order for this reply to be remotely plausible, one must take it for granted that consciousness can be the product of an information processing \"system\", and does not require anything resembling the actual biology of the brain. Searle then responds by simplifying this list of physical objects: he asks what happens if the man memorizes the rules and keeps track of everything in his head? Then the whole system consists of just one object: the man himself. Searle argues that if the man does not understand Chinese then the system does not understand Chinese either because now \"the system\" and \"the man\" both describe exactly the same object. [ 29 ] Critics of Searle's response argue that the program has allowed the man to have two minds in one head. [ who? ] If we assume a \"mind\" is a form of information processing, then the theory of computation can account for two computations occurring at once, namely (1) the computation for universal programmability (which is the function instantiated by the person and note-taking materials independently from any particular program contents) and (2) the computation of the Turing machine that is described by the program (which is instantiated by everything including the specific program). [ 63 ] The theory of computation thus formally explains the open possibility that the second computation in the Chinese Room could entail a human-equivalent semantic understanding of the Chinese inputs. The focus belongs on the program's Turing machine rather than on the person's. [ 64 ] However, from Searle's perspective, this argument is circular. The question at issue is whether consciousness is a form of information processing, and this reply requires that we make that assumption. More sophisticated versions of the systems reply try to identify more precisely what \"the system\" is and they differ in exactly how they describe it. According to these replies, [ who? ] the \"mind that speaks Chinese\" could be such things as: the \"software\", a \"program\", a \"running program\", a simulation of the \"neural correlates of consciousness\", the \"functional system\", a \"simulated mind\", an \" emergent property\", or \"a virtual mind\". Virtual mind reply Marvin Minsky suggested a version of the system reply known as the \"virtual mind reply\". [ p ] The term \" virtual \" is used in computer science to describe an object that appears to exist \"in\" a computer (or computer network) only because software makes it appear to exist. The objects \"inside\" computers (including files, folders, and so on) are all \"virtual\", except for the computer's electronic components. Similarly, Minsky proposes that a computer may contain a \"mind\" that is virtual in the same sense as virtual machines , virtual communities and virtual reality . To clarify the distinction between the simple systems reply given above and virtual mind reply, David Cole notes that two simulations could be running on one system at the same time: one speaking Chinese and one speaking Korean. While there is only one system, there can be multiple \"virtual minds,\" thus the \"system\" cannot be the \"mind\". [ 68 ] Searle responds that such a mind is at best a simulation, and writes: \"No one supposes that computer simulations of a five-alarm fire will burn the neighborhood down or that a computer simulation of a rainstorm will leave us all drenched.\" [ 69 ] Nicholas Fearn responds that, for some things, simulation is as good as the real thing. \"When we call up the pocket calculator function on a desktop computer, the image of a pocket calculator appears on the screen. We don't complain that it isn't really a calculator, because the physical attributes of the device do not matter.\" [ 70 ] The question is, is the human mind like the pocket calculator, essentially composed of information, where a perfect simulation of the thing just is the thing? Or is the mind like the rainstorm, a thing in the world that is more than just its simulation, and not realizable in full by a computer simulation? For decades, this question of simulation has led AI researchers and philosophers to consider whether the term \" synthetic intelligence \" is more appropriate than the common description of such intelligences as \"artificial.\" These replies provide an explanation of exactly who it is that understands Chinese. If there is something besides the man in the room that can understand Chinese, Searle cannot argue that (1) the man does not understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that \"strong AI\" is false. [ q ] These replies, by themselves, do not provide any evidence that strong AI is true, however. They do not show that the system (or the virtual mind) understands Chinese, other than the hypothetical premise that it passes the Turing test. Searle argues that, if we are to consider Strong AI remotely plausible, the Chinese Room is an example that requires explanation, and it is difficult or impossible to explain how consciousness might \"emerge\" from the room or how the system would have consciousness. As Searle writes \"the systems reply simply begs the question by insisting that the system must understand Chinese\" [ 29 ] and thus is dodging the question or hopelessly circular. Robot and semantics replies: finding the meaning As far as the person in the room is concerned, the symbols are just meaningless \"squiggles.\" But if the Chinese room really \"understands\" what it is saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns about intentionality , symbol grounding and syntax vs. semantics . Robot reply Suppose that instead of a room, the program was placed into a robot that could wander around and interact with its environment. This would allow a \" causal connection\" between the symbols and things they represent. [ 72 ] [ r ] Hans Moravec comments: \"If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world.\" [ 74 ] [ s ] Searle's reply is to suppose that, unbeknownst to the individual in the Chinese room, some of the inputs came directly from a camera mounted on a robot, and some of the outputs were used to manipulate the arms and legs of the robot. Nevertheless, the person in the room is still just following the rules, and does not know what the symbols mean. Searle writes \"he doesn't see what comes into the robot's eyes.\" [ 76 ] Derived meaning Some respond that the room, as Searle describes it, is connected to the world: through the Chinese speakers that it is \"talking\" to and through the programmers who designed the knowledge base in his file cabinet. The symbols Searle manipulates are already meaningful, they are just not meaningful to him. [ 77 ] [ t ] Searle says that the symbols only have a \"derived\" meaning, like the meaning of words in books. The meaning of the symbols depends on the conscious understanding of the Chinese speakers and the programmers outside the room. The room, like a book, has no understanding of its own. [ u ] Contextualist reply Some have argued that the meanings of the symbols would come from a vast \"background\" of commonsense knowledge encoded in the program and the filing cabinets. This would provide a \"context\" that would give the symbols their meaning. [ 75 ] [ v ] Searle agrees that this background exists, but he does not agree that it can be built into programs. Hubert Dreyfus has also criticized the idea that the \"background\" can be represented symbolically. [ 80 ] To each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are syntactic and this can never explain to him what the symbols stand for. Searle writes \"syntax is insufficient for semantics.\" [ 81 ] [ w ] However, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean to Searle, what is important is what they mean to the virtual mind. While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors that roboticists can supply. Brain simulation and connectionist replies: redesigning the room These arguments are all versions of the systems reply that identify a particular kind of system as being important; they identify some special technology that would create conscious understanding in a machine. (The \"robot\" and \"commonsense knowledge\" replies above also specify a certain kind of system as being important.) Brain simulator reply Suppose that the program simulated in fine detail the action of every neuron in the brain of a Chinese speaker. [ 83 ] [ x ] This strengthens the intuition that there would be no significant difference between the operation of the program and the operation of a live human brain. Searle replies that such a simulation does not reproduce the important features of the brain—its causal and intentional states. He is adamant that \"human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains.\" [ 26 ] Moreover, he argues: [I]magine that instead of a monolingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes. Now, where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly does not understand Chinese, and neither do the water pipes, and if we are tempted to adopt what I think is the absurd view that somehow the conjunction of man and water pipes understands, remember that in principle the man can internalize the formal structure of the water pipes and do all the \"neuron firings\" in his imagination. &amp;#91;&lt;i>[[Wikipedia:Citing_sources|&lt;span_title=\"This_citation_requires_a_reference_to_the_specific_page_or_range_of_pages_in_which_the_material_appears.&amp;#32;(January_2019)\">page&amp;nbsp;needed&lt;/span>]]&lt;/i>&amp;#93;&lt;/sup>_109-0' rel=\"dc:references\" typeof=\"mw:Extension/ref\" data-mw='{\"name\":\"ref\",\"attrs\":{\"group\":\"\",\"name\":\"FOOTNOTESearle1980[[Category:Wikipedia articles needing page number citations from January 2019]]&lt;sup class=\\\"noprint Inline-Template \\\" style=\\\"white-space:nowrap;\\",
  "cached_at": "2025-10-25T19:39:37.343406"
}