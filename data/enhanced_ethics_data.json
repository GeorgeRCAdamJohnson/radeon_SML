[
  {
    "source": "Wikipedia",
    "title": "Ethics of artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
    "content": "Challenges related to the responsible development and use of AI\n\n\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nThe ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[1] This includes algorithmic biases, fairness,[2] automated decision-making,[3] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[4] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1]\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n\n\nMachine ethics[edit]\nMain articles: Machine ethics and AI alignment\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[5][6][7][8] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[9]\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[10] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[10] Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[11] Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[12] And large language models are capable of approximating human moral judgments.[13] Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nIn Moral Machines: Teaching Robots Right from Wrong,[14] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms,[15] while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".[16]\n\nRobot ethics[edit]\nMain article: Robot ethics\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots.[17] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can also be entirely software.[18] Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benef",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "AI alignment",
    "url": "https://en.wikipedia.org/wiki/AI_alignment",
    "content": "AI conformance to the intended objective\nSome of this article's listed sources may not be reliable. Please help improve this article by looking for better, more reliable sources. Unreliable citations may be challenged and removed. (October 2025) (Learn how and when to remove this message)\n\n\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nIn the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[1]\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.[1][2] AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[1][3]\nAdvanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[1][4][5] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.[6][7] Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[8][9]\nToday, some of these issues affect existing commercial systems such as LLMs,[10][11][12] robots,[13] autonomous vehicles,[14] and social media recommendation engines.[10][5][15] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[16][3][2]\nMany prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned.[17][5] These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind.[18][19][20] These risks remain debated.[21]\nAI alignment is a subfield of AI safety, the study of how to build safe AI systems.[22][23] Other subfields of AI safety include robustness, monitoring, and capability control.[24] Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking.[24] Alignment research has connections to interpretability research,[25][26] (adversarial) robustness,[27] anomaly detection, calibrated uncertainty,[25] formal verification,[28] preference learning,[29][30][31] safety-critical engineering,[32] game theory,[33] algorithmic fairness,[27][34] and social sciences.[35][36]\n\n\nObjectives in AI[edit]\nMain article: Intelligent agent § Objective function\nProgrammers provide an AI system such as AlphaZero with an \"objective function\",[a] in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal \"model\" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize[b] the value[c] of its objective function.[37] For example, when AlphaZero is trained on chess, it has a simple objective function of \"+1 if AlphaZero wins, −1 if AlphaZero loses\". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain t",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Algorithmic bias",
    "url": "https://en.wikipedia.org/wiki/Algorithmic_bias",
    "content": "Technological phenomenon with social implications\n\n\nA flow chart showing the decisions made by a recommendation engine, c. 2001[1]\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nAlgorithmic bias describes systematic and repeatable harmful tendency in a computerized sociotechnical system to create \"unfair\" outcomes, such as \"privileging\" one category over another in ways different from the intended function of the algorithm.[2]\nBias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm.[3]  For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.[4] This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024).\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.[5]\nAlgorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice,[6] healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\nA 2021 survey identified multiple forms of algorithmic bias, including historical, representation, and measurement biases, each of which can contribute to unfair outcomes.[7]\n\n\nDefinitions[edit]\nA 1969 diagram for how a simple computer program makes decisions, illustrating a very simple algorithm\nAlgorithms are difficult to define,[8] but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output.[9]: 13  For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intellige",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Machine ethics",
    "url": "https://en.wikipedia.org/wiki/Machine_ethics",
    "content": "Moral behaviours of man-made machines\nMachine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence (AI), otherwise known as AI agents.[1] Machine ethics differs from other ethical fields related to engineering and technology. It should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with technology's grander social effects.[2]\n\n\nDefinitions[edit]\nJames H. Moor, one of the pioneering theoreticians in the field of computer ethics, defines four kinds of ethical robots. As an extensive researcher on the studies of philosophy of artificial intelligence, philosophy of mind, philosophy of science, and logic, Moor defines machines as ethical impact agents, implicit ethical agents, explicit ethical agents, or full ethical agents. A machine can be more than one type of agent.[3]\n\nEthical impact agents: These are machine systems that carry an ethical impact whether intended or not. At the same time, they have the potential to act unethically. Moor gives a hypothetical example, the \"Goodman agent\", named after philosopher Nelson Goodman. The Goodman agent compares dates but has the millennium bug. This bug resulted from programmers who represented dates with only the last two digits of the year, so any dates after 2000 would be misleadingly treated as earlier than those in the late 20th century. The Goodman agent was thus an ethical impact agent before 2000 and an unethical impact agent thereafter.\nImplicit ethical agents: For the consideration of human safety, these agents are programmed to have a fail-safe, or a built-in virtue. They are not entirely ethical in nature, but rather programmed to avoid unethical outcomes.\nExplicit ethical agents: These are machines capable of processing scenarios and acting on ethical decisions, machines that have algorithms to act ethically.\nFull ethical agents: These are similar to explicit ethical agents in being able to make ethical decisions. But they also have human metaphysical features (i.e., have free will, consciousness, and intentionality).\n(See artificial systems and moral responsibility.)\n\nHistory[edit]\nBefore the 21st century the ethics of machines had largely been the subject of science fiction, mainly due to computing and artificial intelligence (AI) limitations. Although the definition of \"machine ethics\" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI magazine article \"A Question of Responsibility\":One thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. Thus, as computers and robots become more and more intelligent, it becomes imperative that we think carefully and explicitly about what those built-in values are. Perhaps what we need is, in fact, a theory and practice of machine ethics, in the spirit of Asimov's three laws of robotics.[4]\nIn 2004, Towards Machine Ethics[5] was presented at the AAAI Workshop on Agent Organizations: Theory and Practice.[6] Theoretical foundations for machine ethics were laid out.\nAt the AAAI Fall 2005 Symposium on Machine Ethics, researchers met for the first time to consider implementation of an ethical dimension in autonomous systems.[7] A variety of perspectives of this nascent field can be found in the collected edition Machine Ethics[8] that stems from that symposium.\nIn 2007, AI magazine published \"Machine Ethics: Creating an Ethical Intelligent Agent\",[9] an article that discussed the importance of machine ethics, the need for machines that represent ethical principles explicitly, and challenges facing those working on machine ethics. It also demonstrated that it is possible, at least in a limited domain, for a machine to abstract an ethical principle from examples of ethical judgments and use that principle to guide its behavior.\nIn 2009, Oxford University Press published Moral Machines, Teaching Robots Right from Wrong,[10] which it advertised as \"the first book to examine the challenge of building artificial moral agents, probing deeply into the nature of human decision making and ethics.\" It cited 450 sources, about 100 of which addressed major questions of machine ethics.\nIn 2011, Cambridge University Press published a collection of essays about machine ethics edited by Michael and Susan Leigh Anderson,[8] who also edited a special issue of IEEE Intelligent Systems on the topic in 2006.[11] The collection focuses on the challenges of adding ethical principles to machines.[12]\nIn 2014, the US Office of Naval Research announced that it would distribute $7.5 million in grants over five years to university researchers to study questions of machine ethics as applied to autonomous",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Artificial general intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
    "content": "Type of AI with wide-ranging abilities\nNot to be confused with Generative artificial intelligence or Artificial superintelligence.\n\n\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nArtificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.[1][2]\nSome researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved.[3] Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin.[4]\nUnlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.[5]\nCreating AGI is a primary goal of AI research and of companies such as OpenAI,[6] Google,[7] xAI,[8] and Meta.[9] A 2020 survey identified 72 active AGI research and development projects across 37 countries.[10]\nThe timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all.[11][12][13] There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI.[3] AGI is a common topic in science fiction and futures studies.[14][15]\nContention exists over whether AGI represents an existential risk.[16][17][18] Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority.[19][20] Others find the development of AGI to be in too remote a stage to present such a risk.[21][22]\n\n\nTerminology[edit]\nAGI is also known as strong AI,[23][24] full AI,[25] human-level AI,[26] human-level intelligent AI, or general intelligent action.[27]\nSome academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness.[a] In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities.[28][24] Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.[a]\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans,[29] while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.[30]\nA framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers.[31] They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman.[31] For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%.[31] They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans).[31] Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).[32]\n\nCharacteristics[edit]\nMain article: Artificial intel",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "AI safety",
    "url": "https://en.wikipedia.org/wiki/AI_safety",
    "content": "Research area on making AI safe and beneficial\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvteAI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.[1][2]\nBeyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.[3]\n\n\nMotivations[edit]\nScholars discuss current risks from critical systems failures,[4] bias,[5] and AI-enabled surveillance,[6] as well as emerging risks like technological unemployment, digital manipulation,[7] weaponization,[8] AI-enabled cyberattacks[9] and bioterrorism.[10] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents,[11] or from AI enabling perpetually stable dictatorships.[12]\n\nExistential safety[edit]\nSee also: Existential risk from artificial general intelligence\nSome have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\".[13] Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".[14]\nAI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[15][16][17] – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI.[15] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".[18]\n\nHistory[edit]\nRisks from AI began to be seriously discussed at the start of the computer age:\n\nMoreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.— Norbert Wiener (1949)[19]\nIn 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines.[20]\nFrom 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".[21]\nIn 2011, Roman Yampolskiy introduced the term \"AI safety engineering\"[22] at the Philosophy and Theory of Artificial Intelligence conference,[23] listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".[24]\nIn 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create vario",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Explainable artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence",
    "content": "AI whose outputs can be understood by humans\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nWithin artificial intelligence (AI), explainable AI (XAI), often overlapping with interpretable AI or explainable machine learning (XML), is a field of research that explores methods that provide humans with the ability of intellectual oversight over AI algorithms.[1][2] The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms,[3] to make them more understandable and transparent.[4] This addresses users' requirement to assess safety and scrutinize the automated decision making in applications.[5] XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.[6][7]\nXAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason.[8] XAI may be an implementation of the social right to explanation.[9] Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions.[10] XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on.[11] This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.[12]\n\n\nBackground[edit]\nMachine learning (ML) algorithms used in AI can be categorized as white-box or black-box.[13] White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts.[14] XAI algorithms follow the three principles of transparency, interpretability, and explainability.\n\nA model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\"[15]\nInterpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans.[16][17][18]\nExplainability is a concept that is recognized as important, but a consensus definition is not yet available;[15] one possibility is \"the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\".[19]\nIn summary, Interpretability refers to the user's ability to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than entire models. All these concepts aim to enhance the comprehensibility and usability of AI systems.[20]\nIf algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.[21]\nSometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions.[22] Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image[23] and text[24] prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms.[11] Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Fairness (machine learning)",
    "url": "https://en.wikipedia.org/wiki/Fairness_(machine_learning)",
    "content": "Measurement of algorithmic bias\nThis article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these messages)\n\nThis article contains instructions or advice. Wikipedia is not a guidebook; please help rewrite such content to be encyclopedic or move it to Wikiversity, Wikibooks, or Wikivoyage. (December 2019)\nThis article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (December 2019) (Learn how and when to remove this message)\n\n (Learn how and when to remove this message)\n\nFairness in machine learning (ML) refers to the various attempts to correct algorithmic bias in automated decision processes based on ML models. Decisions made by such models after a learning process may be considered unfair if they were based on variables considered sensitive (e.g., gender, ethnicity, sexual orientation, or disability).\nAs is the case with many ethical concepts, definitions of fairness and bias can be controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives.\nSince machine-made decisions may be skewed by a range of factors, they might be considered unfair with respect to certain groups or individuals. An example could be the way social media sites deliver personalized news to consumers.\n\n\nContext[edit]\nDiscussion about fairness in machine learning is a relatively recent topic. Since 2016 there has been a sharp increase in research into the topic.[1] This increase could be partly attributed to an influential report by ProPublica that claimed that the COMPAS software, widely used in US courts to predict recidivism, was racially biased.[2] One topic of research and discussion is the definition of fairness, as there is no universal definition, and different definitions can be in contradiction with each other, which makes it difficult to judge machine learning models.[3] Other research topics include the origins of bias, the types of bias, and methods to reduce bias.[4]\nIn recent years tech companies have made tools and manuals on how to detect and reduce bias in machine learning. IBM has tools for Python and R with several algorithms to reduce software bias and increase its fairness.[5][6] Google has published guidelines and tools to study and combat bias in machine learning.[7][8] Facebook have reported their use of a tool, Fairness Flow, to detect bias in their AI.[9] However, critics have argued that the company's efforts are insufficient, reporting little use of the tool by employees as it cannot be used for all their programs and even when it can, use of the tool is optional.[10]\nIt is important to note that the discussion about quantitative ways to test fairness and unjust discrimination in decision-making predates by several decades the rather recent debate on fairness in machine learning.[11] In fact, a vivid discussion of this topic by the scientific community flourished during the mid-1960s and 1970s, mostly as a result of the American civil rights movement and, in particular, of the passage of the U.S. Civil Rights Act of 1964. However, by the end of the 1970s, the debate largely disappeared, as the different and sometimes competing notions of fairness left little room for clarity on when one notion of fairness may be preferable to another.\n\nLanguage Bias[edit]\nLanguage bias refers a type of statistical sampling bias tied to the language of a query that leads to \"a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository.\"[better source needed][12] Luo et al.[12] show that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent. Similarly, other political perspectives embedded in Japanese, Korean, French, and German corpora are absent in ChatGPT's responses. ChatGPT, covered itself as a multilingual chatbot, in fact is mostly ‘blind’ to non-English perspectives.[12]\n\nGender Bias[edit]\nGender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on trad",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "AI governance",
    "url": "https://en.wikipedia.org/wiki/AI_governance",
    "content": "Guidelines and laws to regulate AI\nPart of a series onLegal aspects of computing\n\nInformation privacy law\nFile sharing\nComputer trespass\nData mining\nHyperlinking and framing\nRegulation of algorithms\nRegulation of AI\nSoftware law\nSoftware licenses\nSpamming\n\nvte\nRegulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI). It is part of the broader regulation of algorithms.[1][2] The regulatory and policy landscape for AI is an emerging issue in jurisdictions worldwide, including for international organizations without direct enforcement power like the IEEE or the OECD.[3]\nSince 2016, numerous AI ethics guidelines have been published in order to maintain social control over the technology.[4] Regulation is deemed necessary to both foster AI innovation and manage associated risks.\nFurthermore, organizations deploying AI have a central role to play in creating and implementing trustworthy AI, adhering to established principles, and taking accountability for mitigating risks.[5]\nRegulating AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.[6][7]\n\n\nBackground[edit]\nAccording to Stanford University's 2025 AI Index, legislative mentions of AI rose 21.3% across 75 countries since 2023, marking a ninefold increase since 2016. The U.S. federal agencies introduced 59 AI-related regulations in 2024—more than double the number in 2023.[8][9] In 2024, nearly 700 AI-related bills were introduced across 45 states, up from 191 in 2023.[10]\nThere is currently no broad consensus on the degree or mechanics of AI regulation. Several prominent figures in the field, including Elon Musk, Sam Altman, Dario Amodei, and Demis Hassabis have publicly called for immediate regulation of AI.[11][12][13][14] In 2023, following ChatGPT-4's creation, Elon Musk and others signed an open letter urging a moratorium on the training of more powerful AI systems.[15] Others, such as Mark Zuckerberg and Marc Andreessen, have warned about the risk of preemptive regulation stifling innovation.[16][17]\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[8] Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[18] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[19][20]\nIn 2023 the United Kingdom started a series of international summits on AI with the AI Safety Summit. It was followed by the AI Seoul Summit in 2024, and the AI Action Summit in Paris in 2025.[21]\n\nPerspectives[edit]\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI.[22] Public administration and policy considerations generally focus on the technical and economic implications and on trustworthy and human-centered AI systems,[23] regulation of artificial superintelligence,[24] the risks and biases of machine-learning algorithms, the explainability of model outputs,[25] and the tension between open source AI and unchecked AI use.[26][27][28]\nThere have been both hard law and soft law proposals to regulate AI.[29] Some legal scholars have noted that hard law approaches to AI regulation have substantial challenges.[30][31] Among the challenges, AI technology is rapidly evolving leading to a \"pacing problem\" where traditional laws and regulations often cannot keep up with emerging applications and their associated risks and benefits.[30][31] Similarly, the diversity of AI applications challenges existing regulatory agencies, which often have limited jurisdictional scope.[30] As an alternative, some legal scholars argue that soft law approaches to AI regulation are promising, as they offer greater flexibility to adapt to emerging technologies and the evolving nature of AI applications.[30][31] However, soft law approaches often lack substantial enforcement potential.[30][32]\nCason Schmit, Megan Doerr, and Jennifer Wagner proposed the creation of a quasi-governmental regulator by leveraging intellectual property rights (i.e., copyleft licensing) in certain AI objects (i.e., AI models and training datasets) and delegating enforcement rights to a designated enforcement entity.[33] They argue that AI can be licensed under terms that require adherence to specified ethical practices and codes of conduct. (e.g., soft law principles).[33]\nProminent youth organizations focused on AI, namely Encode Justice, have also issued comprehensive agendas calling for more stringent AI regulations and public-private partnerships.[34][35]\nAI regulation could derive from basic principles. A 2020 Berk",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Responsible AI",
    "url": "https://en.wikipedia.org/wiki/Responsible_AI",
    "content": "Challenges related to the responsible development and use of AI\n\n\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nThe ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[1] This includes algorithmic biases, fairness,[2] automated decision-making,[3] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[4] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1]\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n\n\nMachine ethics[edit]\nMain articles: Machine ethics and AI alignment\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[5][6][7][8] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[9]\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[10] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[10] Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[11] Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[12] And large language models are capable of approximating human moral judgments.[13] Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nIn Moral Machines: Teaching Robots Right from Wrong,[14] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms,[15] while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".[16]\n\nRobot ethics[edit]\nMain article: Robot ethics\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots.[17] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can also be entirely software.[18] Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benef",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "AI ethics",
    "url": "https://en.wikipedia.org/wiki/AI_ethics",
    "content": "Challenges related to the responsible development and use of AI\n\n\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nThe ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[1] This includes algorithmic biases, fairness,[2] automated decision-making,[3] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[4] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1]\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n\n\nMachine ethics[edit]\nMain articles: Machine ethics and AI alignment\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[5][6][7][8] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[9]\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[10] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[10] Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[11] Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[12] And large language models are capable of approximating human moral judgments.[13] Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nIn Moral Machines: Teaching Robots Right from Wrong,[14] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms,[15] while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".[16]\n\nRobot ethics[edit]\nMain article: Robot ethics\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots.[17] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can also be entirely software.[18] Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benef",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Robot ethics",
    "url": "https://en.wikipedia.org/wiki/Robot_ethics",
    "content": "Ethical problems related to robots\nRobot ethics, sometimes known as \"roboethics\", concerns ethical problems that occur with robots, such as whether robots pose a threat to humans in the long or short run, whether some uses of robots are problematic (such as in healthcare or as \"killer robots\" in war), and how robots should be designed such that they act \"ethically\" (this last concern is also called machine ethics). Alternatively, roboethics refers specifically to the ethics of human behavior towards robots, as robots become increasingly advanced.[1]\nRobot ethics is a sub-field of the ethics of technology. It is closely related to legal and socio-economic concerns. Serious academic discussions about robot ethics started around 2000, and involve several disciplines, mainly robotics, computer science, artificial intelligence, philosophy, ethics, theology, biology, physiology, cognitive science, neurosciences, law, sociology, psychology, and industrial design.[2]\n\n\nHistory and events[edit]\nSee also: History of robotsLaws of robotics\nIsaac Asimov\n\nThree Laws of Robotics\nin popular culture\n\n\nRelated topics\n\nRoboethics\nEthics of AI\nMachine ethics\n\nvte\nOne of the first publications directly addressing and setting the foundation for robot ethics was \"Runaround\", a science fiction short story written by Isaac Asimov in 1942, which featured his well-known Three Laws of Robotics. These three laws were continuously altered by Asimov, and a fourth – or \"zeroth\" – law was eventually added to precede the first three, in the context of his science fiction works. The short term \"roboethics\" was most likely coined by Gianmarco Veruggio.[3]\nRoboethics was also highlighted in 2004 with the First International Symposium on Roboethics.[4] In discussions with students and non-specialists, Gianmarco Veruggio and Fiorella Operto thought that a good debate could push people to take an active part in the education of public opinion, make them comprehend the positive uses of the new technology, and prevent its abuse. Anthropologist Daniela Cerqui identified three main ethical positions emerging from the two days of debate: those who see robotics as purely technical and disclaim ethical responsibility, those interested in short-term ethical questions (such as compliance with existing conventions), and those interested in long-term ethical questions (including the digital divide).[5]\n\nSophia, a humanoid robot, obtained Saudi Arabian citizenship in 2017.[6]\nSome other important events include:\n\n2004: the Fukuoka World Robot Declaration.[7]\n2017: in the Future Investment Summit in Riyadh, a robot named Sophia (and referred to with female pronouns) is granted Saudi Arabian citizenship, becoming the first robot ever to have a nationality.[8][6] This attracts controversy due to legal ambiguity, for instance over whether Sophia can vote or marry, or whether a deliberate system shutdown is to be considered murder. Additionally, news outlets contrasted it with the limited rights that Saudi women have.[9][10]\n2017: The European Parliament passed a resolution addressed to the European Commission concerning Civil Law Rules on Robotics.[11]\nComputer scientist Virginia Dignum noted in a March 2018 issue of Ethics and Information Technology that the general societal attitude toward artificial intelligence (AI) has, in the modern era, shifted away from viewing AI as a tool and toward viewing it as an intelligent \"team-mate\". In the same article, she assessed that, with respect to AI, ethical thinkers have three goals, each of which she argues can be achieved in the modern era with careful thought and implementation.[12][13][14][15][16] The three ethical goals are as follows:\n\nEthics by Design (the technical/algorithmic integration of ethical reasoning capabilities as part of the behavior of artificial autonomous system, see machine ethics);\nEthics in Design (the regulatory and engineering methods that support the analysis and evaluation of the ethical implications of AI systems as these integrate or replace traditional social structures); and\nEthics for design (the codes of conduct, standards and certification processes that ensure the integrity of developers and users as they research, design, construct, employ and manage artificial intelligent systems, see § Law below).[17]\nIn popular culture[edit]\nMain article: Artificial intelligence in fiction\nIllustration of HAL 9000, a fictional artificial intelligence character from the film 2001: A Space Odyssey\nRoboethics as a science or philosophical topic has been a common theme in science fiction literature and films. One film that could be argued to be ingrained in pop culture that depicts the dystopian future use of robotic AI is The Matrix, depicting a future where humans and conscious sentient AI struggle for control of planet Earth, resulting in the destruction of most of the human race. An animated film based on The Matrix, the Animatrix, focused heavily on the potential ethical issues and insecuriti",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Algorithmic accountability",
    "url": "https://en.wikipedia.org/wiki/Algorithmic_accountability",
    "content": "Responsibility to design impartial algorithms\nAlgorithmic accountability refers to the allocation of responsibility for the consequences of real-world actions influenced by algorithms used in decision-making processes.[1]\nIdeally, algorithms should be designed to eliminate bias from their decision-making outcomes. This means they ought to evaluate only relevant characteristics of the input data, avoiding distinctions based on attributes that are generally inappropriate in social contexts, such as an individual's ethnicity in legal judgments. However, adherence to this principle is not always guaranteed, and there are instances where individuals may be adversely affected by algorithmic decisions. Responsibility for any harm resulting from a machine's decision may lie with the algorithm itself or with the individuals who designed it, particularly if the decision resulted from bias or flawed data analysis inherent in the algorithm's design.[2]\n\n\nAlgorithm usage[edit]\nAlgorithms are widely utilized across various sectors of society that incorporate computational techniques in their control systems. These applications span numerous industries, including but not limited to medical, transportation, and payment services.[3] In these contexts, algorithms perform functions such as:[4]\n\nApproving or denying credit card applications;\nCounting votes in elections;\nApproving or denying immigrant visas;\nDetermining which taxpayers will be audited on their income taxes;\nManaging systems that control self-driving cars on a highway;\nScoring individuals as potential criminals for use in legal proceedings.\nHowever, the implementation of these algorithms can be complex and opaque. Generally, algorithms function as \"black boxes,\" meaning that the specific processes an input undergoes during execution are often not transparent, with users typically only seeing the resulting output.[5] This lack of transparency raises concerns about potential biases within the algorithms, as the parameters influencing decision-making may not be well understood. The outputs generated can lead to perceptions of bias, especially if individuals in similar circumstances receive different results. According to Nicholas Diakopoulos:\n\nBut these algorithms can make mistakes. They have biases. Yet they sit in opaque black boxes, their inner workings, their inner “thoughts” hidden behind layers of complexity. We need to get inside that black box, to understand how they may be exerting power on us, and to understand where they might be making unjust mistakes\nWisconsin Supreme Court case[edit]\nAlgorithms are prevalent across various fields and significantly influence decisions that affect the population at large. Their underlying structures and parameters often remain unknown to those impacted by their outcomes. A notable case illustrating this issue is a recent ruling by the Wisconsin Supreme Court concerning \"risk assessment\" algorithms used in criminal justice.[3] The court determined that scores generated by such algorithms, which analyze multiple parameters from individuals, should not be used as a determining factor for arresting an accused individual. Furthermore, the court mandated that all reports submitted to judges must include information regarding the accuracy of the algorithm used to compute these scores.\nThis ruling is regarded as a noteworthy development in how society should manage software that makes consequential decisions, highlighting the importance of reliability, particularly in complex settings like the legal system. The use of algorithms in these contexts necessitates a high degree of impartiality in processing input data. However, experts note that there is still considerable work to be done to ensure the accuracy of algorithmic results. Questions about the transparency of data processing continue to arise, which raises issues regarding the appropriateness of the algorithms and the intentions of their designers.[citation needed]\n\nControversies[edit]\nA notable instance of potential algorithmic bias is highlighted in an article by The Washington Post[6] regarding the ride-hailing service Uber. An analysis of collected data revealed that estimated waiting times for users varied based on the neighborhoods in which they resided. Key factors influencing these discrepancies included the predominant ethnicity and average income of the area.\nSpecifically, neighborhoods with a majority white population and higher economic status tended to have shorter waiting times, while those with more diverse ethnic compositions and lower average incomes experienced longer waits. It’s important to clarify that this observation reflects a correlation identified in the data, rather than a definitive cause-and-effect relationship. No value judgments are made regarding the behavior of the Uber app in these cases.\nIn a separate analysis published in the \"Direito Digit@l\" column on the Migalhas website, authors Coriolano Almeida Camargo and Marcelo Crespo examine t",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "AI transparency",
    "url": "https://en.wikipedia.org/wiki/AI_transparency",
    "content": "\n\nLook for AI transparency on one of Wikipedia's sister projects:\n\n\n\n\nWiktionary (dictionary)\n\n\n\nWikibooks (textbooks)\n\n\n\nWikiquote (quotations)\n\n\n\nWikisource (library)\n\n\n\nWikiversity (learning resources)\n\n\n\nCommons (media)\n\n\n\nWikivoyage (travel guide)\n\n\n\nWikinews (news source)\n\n\n\nWikidata (linked database)\n\n\n\nWikispecies (species directory)\n\n\n\nWikipedia does not have an article with this exact name. Please search for AI transparency in Wikipedia to check for alternative titles or spellings.\nYou need to log in or create an account and be autoconfirmed to create new articles. Alternatively, you can use the article wizard to submit a draft for review, or request a new article.\nSearch for \"AI transparency\" in existing articles.\nLook for pages within Wikipedia that link to this title.\n\n\nOther reasons this message may be displayed:\n\nIf a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.\nTitles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.\nIf the page has been deleted, check the deletion log, and see Why was the page I created deleted?\n\n\n\nRetrieved from \"https://en.wikipedia.org/wiki/AI_transparency\"",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Digital ethics",
    "url": "https://en.wikipedia.org/wiki/Digital_ethics",
    "content": "Branch of ethics\nNot to be confused with Cyberethics or Computer ethics.\nInformation ethics has been defined as \"the branch of ethics that focuses on the relationship between the creation, organization, dissemination, and use of information, and the ethical standards and moral codes governing human conduct in society\".[1] It examines the morality that comes from information as a resource, a product, or as a target.[2] It provides a critical framework for considering moral issues concerning informational privacy, moral agency (e.g. whether artificial agents may be moral), new environmental issues (especially how agents should behave in the infosphere), problems arising from the life-cycle (creation, collection, recording, distribution, processing, etc.) of information (especially ownership and copyright, digital divide, and digital rights). It is very vital to understand that librarians, archivists, information professionals among others, really understand the importance of knowing how to disseminate proper information as well as being responsible with their actions when addressing information.[3]\nInformation ethics has evolved to relate to a range of fields such as computer ethics,[4] medical ethics, journalism[5] and the philosophy of information. As the use and creation of information and data form the foundation of machine learning, artificial intelligence and many areas of mathematics, information ethics also plays a central role in the ethics of artificial intelligence, big data ethics and ethics in mathematics.\n\n\nHistory[edit]\nThe term information ethics was first coined by Robert Hauptman and used in the book Ethical Challenges in Librarianship. \nThe field of information ethics has a relatively short but progressive history having been recognized in the United States for nearly 20 years.[6] The origins of the field are in librarianship though it has now expanded to the consideration of ethical issues in other domains including computer science, the internet, media, journalism, management information systems, and business.[6]\nEvidence of scholarly work on this subject can be traced to the 1980s, when an article authored by Barbara J. Kostrewski and Charles Oppenheim and published in the Journal of Information Science, discussed issues relating to the field including confidentiality, information biases, and quality control.[6] Another scholar, Robert Hauptman, has also written extensively about information ethics in the library field and founded the Journal of Information Ethics in 1992.[7]\nOne of the first schools to introduce an Information Ethics course was the University of Pittsburgh in 1990. The course was a master's level course on the concept of Information Ethics. Soon after, Kent State University also introduced a master's level course called \"Ethical Concerns For Library and Information Professionals.\" Eventually, the term \"Information Ethics\" became more associated with the computer science and information technology disciplines in university. Still however, it is uncommon for universities to devote entire courses to the subject. Due to the nature of technology, the concept of information ethics has spread to other realms in the industry. Thus, concepts such as \"cyberethics,\" a concept which discusses topics such as the ethics of artificial intelligence and its ability to reason, and media ethics which applies to concepts such as lies, censorship, and violence in the press. Therefore, due to the advent of the internet, the concept of information ethics has been spread to other fields other than librarianship now that information has become so readily available. Information has become more relevant now than ever now that the credibility of information online is more blurry than print articles due to the ease of publishing online articles. All of these different concepts have been embraced by the International Center for Information Ethics (ICIE), established by Rafael Capurro in 1999.[8]\nDilemmas regarding the life of information are becoming increasingly important in a society that is defined as \"the information society\". The explosion of so much technology has brought information ethics to a forefront in ethical considerations. Information transmission and literacy are essential concerns in establishing an ethical foundation that promotes fair, equitable, and responsible practices. Information ethics broadly examines issues related to ownership, access, privacy, security, and community. It is also concerned with relational issues such as \"the relationship between information and the good of society, the relationship between information providers and the consumers of information\".[9]\nInformation technology affects common issues such as copyright protection, intellectual freedom, accountability, privacy, and security. Many of these issues are difficult or impossible to resolve due to fundamental tensions between Western moral philosophies (based on rules, democracy, individual rights, and pe",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Computational ethics",
    "url": "https://en.wikipedia.org/wiki/Computational_ethics",
    "content": "Moral behaviours of man-made machines\nMachine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence (AI), otherwise known as AI agents.[1] Machine ethics differs from other ethical fields related to engineering and technology. It should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with technology's grander social effects.[2]\n\n\nDefinitions[edit]\nJames H. Moor, one of the pioneering theoreticians in the field of computer ethics, defines four kinds of ethical robots. As an extensive researcher on the studies of philosophy of artificial intelligence, philosophy of mind, philosophy of science, and logic, Moor defines machines as ethical impact agents, implicit ethical agents, explicit ethical agents, or full ethical agents. A machine can be more than one type of agent.[3]\n\nEthical impact agents: These are machine systems that carry an ethical impact whether intended or not. At the same time, they have the potential to act unethically. Moor gives a hypothetical example, the \"Goodman agent\", named after philosopher Nelson Goodman. The Goodman agent compares dates but has the millennium bug. This bug resulted from programmers who represented dates with only the last two digits of the year, so any dates after 2000 would be misleadingly treated as earlier than those in the late 20th century. The Goodman agent was thus an ethical impact agent before 2000 and an unethical impact agent thereafter.\nImplicit ethical agents: For the consideration of human safety, these agents are programmed to have a fail-safe, or a built-in virtue. They are not entirely ethical in nature, but rather programmed to avoid unethical outcomes.\nExplicit ethical agents: These are machines capable of processing scenarios and acting on ethical decisions, machines that have algorithms to act ethically.\nFull ethical agents: These are similar to explicit ethical agents in being able to make ethical decisions. But they also have human metaphysical features (i.e., have free will, consciousness, and intentionality).\n(See artificial systems and moral responsibility.)\n\nHistory[edit]\nBefore the 21st century the ethics of machines had largely been the subject of science fiction, mainly due to computing and artificial intelligence (AI) limitations. Although the definition of \"machine ethics\" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI magazine article \"A Question of Responsibility\":One thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. Thus, as computers and robots become more and more intelligent, it becomes imperative that we think carefully and explicitly about what those built-in values are. Perhaps what we need is, in fact, a theory and practice of machine ethics, in the spirit of Asimov's three laws of robotics.[4]\nIn 2004, Towards Machine Ethics[5] was presented at the AAAI Workshop on Agent Organizations: Theory and Practice.[6] Theoretical foundations for machine ethics were laid out.\nAt the AAAI Fall 2005 Symposium on Machine Ethics, researchers met for the first time to consider implementation of an ethical dimension in autonomous systems.[7] A variety of perspectives of this nascent field can be found in the collected edition Machine Ethics[8] that stems from that symposium.\nIn 2007, AI magazine published \"Machine Ethics: Creating an Ethical Intelligent Agent\",[9] an article that discussed the importance of machine ethics, the need for machines that represent ethical principles explicitly, and challenges facing those working on machine ethics. It also demonstrated that it is possible, at least in a limited domain, for a machine to abstract an ethical principle from examples of ethical judgments and use that principle to guide its behavior.\nIn 2009, Oxford University Press published Moral Machines, Teaching Robots Right from Wrong,[10] which it advertised as \"the first book to examine the challenge of building artificial moral agents, probing deeply into the nature of human decision making and ethics.\" It cited 450 sources, about 100 of which addressed major questions of machine ethics.\nIn 2011, Cambridge University Press published a collection of essays about machine ethics edited by Michael and Susan Leigh Anderson,[8] who also edited a special issue of IEEE Intelligent Systems on the topic in 2006.[11] The collection focuses on the challenges of adding ethical principles to machines.[12]\nIn 2014, the US Office of Naval Research announced that it would distribute $7.5 million in grants over five years to university researchers to study questions of machine ethics as applied to autonomous",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Technology ethics",
    "url": "https://en.wikipedia.org/wiki/Technology_ethics",
    "content": "Ethical questions specific to the technology age\nThe ethics of technology is a sub-field of ethics addressing ethical questions specific to the technology age, the transitional shift in society wherein personal computers and subsequent devices provide for the quick and easy transfer of information. Technology ethics is the application of ethical thinking to growing concerns as new technologies continue to rise in prominence.\nThe topic has evolved as technologies have developed. Technology poses an ethical dilemma on producers and consumers alike. \nThe subject of technoethics, or the ethical implications of technology, have been studied by different philosophers such as Hans Jonas and Mario Bunge.\n\n\nTechnoethics[edit]\nTechnoethics (TE) is an interdisciplinary research area that draws on theories and methods from multiple knowledge domains (such as communications, social sciences, information studies, technology studies, applied ethics, and philosophy) to provide insights on ethical dimensions of technological systems and practices for advancing a technological society.[1]\nTechnoethics views technology and ethics as socially embedded enterprises and focuses on discovering the ethical uses for technology, protecting against the misuse of technology,[2][3] and devising common principles to guide new advances in technological development and application to benefit society. Typically, scholars in technoethics have a tendency to conceptualize technology and ethics as interconnected and embedded in life and society.[4] Technoethics denotes a broad range of ethical issues revolving around technology – from specific areas of focus affecting professionals working with technology to broader social, ethical, and legal issues concerning the role of technology in society and everyday life.[1]\nTechnoethical perspectives are constantly in transition as technology advances in areas unseen by creators and as users change the intended uses of new technologies. Humans cannot be separated from these technologies because it is an inherent part of consciousness.[5] The short term and longer term ethical considerations for technologies engage the creator, producer, user, and governments.\nWith the increasing impact emerging technologies have on society, the importance of assessing ethical and social issues constantly becomes more important.[6] While such technologies provide opportunities for novel applications and the potential to transform the society on a global scale, their rise is accompanied by new ethical challenges and problems that must be considered.[7] This becomes more difficult with the increasing pace at which technology is progressing and the increasing impact it has on the societal understanding by seemingly outrunning human control.[8] The concept of technoethics focuses on expanding the knowledge of existing research in the areas of technology and ethics in order to provide a holistic construct for the different aspects and subdisciplines of ethics related to technology-related human activity like economics, politics, globalization, and scientific research.[6] It is also concerned with the rights and responsibilities that designers and developers have regarding the outcomes of the respective technology.[6][9] This is of particular importance with the emergence of algorithmic technology capable of making decisions autonomously and the related issues of developer or data bias influencing these decisions.[9] To work against the manifestation of these biases, the balance between human and technology accountability for ethical failure has to be carefully evaluated and has shifted the view from technology as a merely positive tool towards the perception of technology as inherently neutral.[8][10] Technoethics thus has to focus on both sides of the human technology equation when confronted with upcoming technology innovations and applications.[9]\nWith technology continuing to advance over time, there are new technoethical issues that come into play. For instance, discussions on genetically modified organisms (GMOs) have brought about a huge concern for technology, ethics, and safety.[11] There is also a huge question of whether or not artificial intelligence (AI) should be trusted and relied upon. These are just some examples of how the advancements in technology will affect the ethical values of humans in the future.\nTechnoethics finds application in various areas of technology. The following key areas are mentioned in the literature:[6]\n\nComputer ethics: Focuses on the use of technology in areas including visual technology, artificial intelligence, and robotics.\nEngineering ethics: Dealing with professional standards of engineers and their moral responsibilities to the public.\nInternet ethics and cyberethics: Concerning the guarding against unethical Internet activity.\nMedia and communication technoethics: Concerning ethical issues and responsibilities when using mass media and communication technology.\nProfessional tec",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Computer ethics",
    "url": "https://en.wikipedia.org/wiki/Computer_ethics",
    "content": "Philosophy concept in computing\nNot to be confused with Cyberethics or Information ethics.\nComputer ethics is a part of practical philosophy concerned with how computing professionals should make decisions regarding professional and social conduct.[1]\nMargaret Anne Pierce, a professor in the Department of Mathematics and Computers at Georgia Southern University has categorized the ethical decisions related to computer technology and usage into three primary influences:[2]\n\nThe individual's own personal [ethical] code.\nAny informal code of ethical conduct that exists in the work place.\nExposure to formal codes of ethics.\n\nFoundation[edit]\nComputer ethics was first coined by Walter Maner,[1] a professor at Bowling Green State University. Maner noticed ethical concerns that were brought up during his Medical Ethics course at Old Dominion University became more complex and difficult when the use of technology and computers became involved.[3] The conceptual foundations of computer ethics are investigated by information ethics, a branch of philosophical ethics promoted, among others, by Luciano Floridi.[4]\n\nHistory[edit]\nThe concept of computer ethics originated in the 1940s with MIT professor Norbert Wiener, the American mathematician and philosopher. While working on anti-aircraft artillery during World War II, Wiener and his fellow engineers developed a system of communication between the part of a cannon that tracked a warplane, the part that performed calculations to estimate a trajectory, and the part responsible for firing.[1] Wiener termed the science of such information feedback systems, \"cybernetics,\" and he discussed this new field with its related ethical concerns in his 1948 book, Cybernetics.[1][5] In 1950, Wiener's second book, The Human Use of Human Beings, delved deeper into the ethical issues surrounding information technology and laid out the basic foundations of computer ethics.[5]\nA bit later during the same year, the world's first computer crime was committed. A programmer was able to use a bit of computer code to stop his banking account from being flagged as overdrawn. However, there were no laws in place at that time to stop him, and as a result he was not charged.[6][unreliable source?] To make sure another person did not follow suit, an ethics code for computers was needed.\nIn 1973, the Association for Computing Machinery (ACM) adopted its first code of ethics.[1] SRI International's Donn Parker,[7] an author on computer crimes, led the committee that developed the code.[1]\nIn 1976, medical teacher and researcher Walter Maner noticed that ethical decisions are much harder to make when computers are added. He noticed a need for a different branch of ethics for when it came to dealing with computers. The term \"computer ethics\" was thus invented.[1][5]\nIn 1976 Joseph Weizenbaum made his second significant addition to the field of computer ethics. He published a book titled Computer Power and Human Reason,[8] which talked about how artificial intelligence is good for the world; however it should never be allowed to make the most important decisions as it does not have human qualities such as wisdom. By far the most important point he makes in the book is the distinction between choosing and deciding. He argued that deciding is a computational activity while making choices is not and thus the ability to make choices is what makes us humans.\nAt a later time during the same year Abbe Mowshowitz, a professor of Computer Science at the City College of New York, published an article titled \"On approaches to the study of social issues in computing.\" This article identified and analyzed technical and non-technical biases in research on social issues present in computing.\nDuring 1978, the Right to Financial Privacy Act was adopted by the United States Congress, drastically limiting the government's ability to search bank records.[9]\nDuring the next year Terrell Ward Bynum, the professor of philosophy at Southern Connecticut State University as well as Director of the Research Center on Computing and Society there, developed curriculum for a university course on computer ethics.[10] Bynum was also editor of the journal Metaphilosophy.[1] In 1983 the journal held an essay contest on the topic of computer ethics and published the winning essays in its best-selling 1985 special issue, “Computers and Ethics.”[1]\nIn 1984, the United States Congress passed the Small Business Computer Security and Education Act, which created a Small Business Administration advisory council to focus on computer security related to small businesses.[11]\nIn 1985, James H. Moor, professor of philosophy at Dartmouth College in New Hampshire, published an essay called \"What is Computer Ethics?\"[5] In this essay Moor states the computer ethics includes the following: \"(1) identification of computer-generated policy vacuums, (2) clarification of conceptual muddles, (3) formulation of policies for the use of computer technology, a",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Information ethics",
    "url": "https://en.wikipedia.org/wiki/Information_ethics",
    "content": "Branch of ethics\nNot to be confused with Cyberethics or Computer ethics.\nInformation ethics has been defined as \"the branch of ethics that focuses on the relationship between the creation, organization, dissemination, and use of information, and the ethical standards and moral codes governing human conduct in society\".[1] It examines the morality that comes from information as a resource, a product, or as a target.[2] It provides a critical framework for considering moral issues concerning informational privacy, moral agency (e.g. whether artificial agents may be moral), new environmental issues (especially how agents should behave in the infosphere), problems arising from the life-cycle (creation, collection, recording, distribution, processing, etc.) of information (especially ownership and copyright, digital divide, and digital rights). It is very vital to understand that librarians, archivists, information professionals among others, really understand the importance of knowing how to disseminate proper information as well as being responsible with their actions when addressing information.[3]\nInformation ethics has evolved to relate to a range of fields such as computer ethics,[4] medical ethics, journalism[5] and the philosophy of information. As the use and creation of information and data form the foundation of machine learning, artificial intelligence and many areas of mathematics, information ethics also plays a central role in the ethics of artificial intelligence, big data ethics and ethics in mathematics.\n\n\nHistory[edit]\nThe term information ethics was first coined by Robert Hauptman and used in the book Ethical Challenges in Librarianship. \nThe field of information ethics has a relatively short but progressive history having been recognized in the United States for nearly 20 years.[6] The origins of the field are in librarianship though it has now expanded to the consideration of ethical issues in other domains including computer science, the internet, media, journalism, management information systems, and business.[6]\nEvidence of scholarly work on this subject can be traced to the 1980s, when an article authored by Barbara J. Kostrewski and Charles Oppenheim and published in the Journal of Information Science, discussed issues relating to the field including confidentiality, information biases, and quality control.[6] Another scholar, Robert Hauptman, has also written extensively about information ethics in the library field and founded the Journal of Information Ethics in 1992.[7]\nOne of the first schools to introduce an Information Ethics course was the University of Pittsburgh in 1990. The course was a master's level course on the concept of Information Ethics. Soon after, Kent State University also introduced a master's level course called \"Ethical Concerns For Library and Information Professionals.\" Eventually, the term \"Information Ethics\" became more associated with the computer science and information technology disciplines in university. Still however, it is uncommon for universities to devote entire courses to the subject. Due to the nature of technology, the concept of information ethics has spread to other realms in the industry. Thus, concepts such as \"cyberethics,\" a concept which discusses topics such as the ethics of artificial intelligence and its ability to reason, and media ethics which applies to concepts such as lies, censorship, and violence in the press. Therefore, due to the advent of the internet, the concept of information ethics has been spread to other fields other than librarianship now that information has become so readily available. Information has become more relevant now than ever now that the credibility of information online is more blurry than print articles due to the ease of publishing online articles. All of these different concepts have been embraced by the International Center for Information Ethics (ICIE), established by Rafael Capurro in 1999.[8]\nDilemmas regarding the life of information are becoming increasingly important in a society that is defined as \"the information society\". The explosion of so much technology has brought information ethics to a forefront in ethical considerations. Information transmission and literacy are essential concerns in establishing an ethical foundation that promotes fair, equitable, and responsible practices. Information ethics broadly examines issues related to ownership, access, privacy, security, and community. It is also concerned with relational issues such as \"the relationship between information and the good of society, the relationship between information providers and the consumers of information\".[9]\nInformation technology affects common issues such as copyright protection, intellectual freedom, accountability, privacy, and security. Many of these issues are difficult or impossible to resolve due to fundamental tensions between Western moral philosophies (based on rules, democracy, individual rights, and pe",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Bioethics",
    "url": "https://en.wikipedia.org/wiki/Bioethics",
    "content": "Study of the ethical issues emerging from advances in biology and medicine\nThis article is about the discipline. For the journal, see Bioethics (journal).\n\n\nPart of a series onMedical ethics\nValues\nPrimum non nocere\nInformed consent\nBeneficence\nConfidentiality\nAutonomy\n\nConflicts\nCapacity\nConflict of interest\nSeparation of prescribing and dispensing\nHealth care rationing\nInvoluntary treatment\nDuty to warn\nDuty to protect\nCases\n\nFrameworks\nAdvance healthcare directive\nSurrogate decision-maker\n\nRelated Concepts\nMedical profession\nMedical law\nBioethics\n\nCritiques\nMedical sociology\nvte\nBioethics is both a field of study and professional practice, interested in ethical issues related to health (primarily focused on the human, but also increasingly includes animal ethics), including those emerging from advances in biology, medicine, and technologies. It proposes the discussion about moral discernment in society (what decisions are \"good\" or \"bad\" and why) and it is often related to medical policy and practice, but also to broader questions as environment, well-being and public health. Bioethics is concerned with the ethical questions that arise in the relationships among life sciences, biotechnology, medicine, politics, law, theology and philosophy. It includes the study of values relating to primary care, other branches of medicine (\"the ethics of the ordinary\"), ethical education in science, animal, and environmental ethics, and public health.\n\n\nEtymology[edit]\nThe term bioethics (Greek bios, \"life\"; ethos, \"moral nature, behavior\"[1]) was coined in 1927 by Fritz Jahr in an article about a \"bioethical imperative\" regarding the use of animals and plants in scientific research.[2] In 1970, the American biochemist, and oncologist Van Rensselaer Potter used the term to describe the relationship between the biosphere and a growing human population. Potter's work laid the foundation for global ethics, a discipline centered around the link between biology, ecology, medicine, and human values.[3][4] Sargent Shriver, the spouse of Eunice Kennedy Shriver, claimed that he had invented the term \"bioethics\" in the living room of his home in Bethesda, Maryland, in 1970. He stated that he thought of the word after returning from a discussion earlier that evening at Georgetown University, where he discussed with others a possible Kennedy family sponsorship of an institute focused around the \"application of moral philosophy to concrete medical dilemmas\".[5]\n\nPurpose and scope[edit]\nThe discipline of bioethics has addressed a wide swathe of human inquiry; ranging from debates over the boundaries of lifestyles (e.g. abortion, euthanasia), surrogacy, the allocation of scarce health care resources (e.g. organ donation, health care rationing), to the right to refuse medical care for religious or cultural reasons. Bioethicists disagree among themselves over the precise limits of their discipline, debating whether the field should concern itself with the ethical evaluation of all questions involving biology and medicine, or only a subset of these questions.[6] Some bioethicists would narrow ethical evaluation only to the morality of medical treatments or technological innovations, and the timing of medical treatment of humans. Others would increase the scope of moral assessment to encompass the morality of all moves that would possibly assist or damage organisms successful of feeling fear.\nThe scope of bioethics has evolved past mere biotechnology to include topics such as cloning, gene therapy, life extension, human genetic engineering, astroethics and life in space,[7][8] and manipulation of basic biology through altered DNA, XNA and proteins.[9] These (and other) developments may affect future evolution and require new principles that address life at its core, such as biotic ethics that values life itself at its basic biological processes and structures, and seeks their propagation.[10] Moving beyond the biological, issues raised in public health such as vaccination and resource allocation have also encouraged the development of novel ethics frameworks[11] to address such challenges. A study published in 2022 based on the corpus of full papers from eight main bioethics journals demonstrated the heterogeneity of this field by distinguishing 91 topics that have been discussed in these journals over the past half a century.[12]\n\nPrinciples[edit]\nHippocrates Refusing the Gifts of Artaxerxes by Anne-Louis Girodet-Trioson\nOne of the first areas addressed by modern bioethicists was human experimentation. According to the Declaration of Helsinki (1964) published by the World Medical Association, the essential principles in medical research involving human subjects are autonomy, beneficence, non-maleficence, and justice. \nThe autonomy of individuals to make decisions while assuming responsibility for them and respecting the autonomy of others ought to be respected. For people unable to exercise their autonomy, special measures ought to be t",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Medical ethics",
    "url": "https://en.wikipedia.org/wiki/Medical_ethics",
    "content": "System of moral principles of the practice of medicine\nFor the 1803 textbook, see Medical Ethics (book). For the journal, see Journal of Medical Ethics.\nPart of a series onMedical ethics\nValues\nPrimum non nocere\nInformed consent\nBeneficence\nConfidentiality\nAutonomy\n\nConflicts\nCapacity\nConflict of interest\nSeparation of prescribing and dispensing\nHealth care rationing\nInvoluntary treatment\nDuty to warn\nDuty to protect\nCases\n\nFrameworks\nAdvance healthcare directive\nSurrogate decision-maker\n\nRelated Concepts\nMedical profession\nMedical law\nBioethics\n\nCritiques\nMedical sociology\nvte\nMedical ethics is an applied branch of ethics which analyzes the practice of clinical medicine and related scientific research.[1] Medical ethics is based on a set of values that professionals can refer to in the case of any confusion or conflict. These values include the respect for autonomy, non-maleficence, beneficence, and justice.[2] Such tenets may allow doctors, care providers, and families to create a treatment plan and work towards the same common goal.[3] These four values are not ranked in order of importance or relevance and they all encompass values pertaining to medical ethics.[4] However, a conflict may arise leading to the need for hierarchy in an ethical system, such that some moral elements overrule others with the purpose of applying the best moral judgement to a difficult medical situation.[5] Medical ethics is particularly relevant in decisions regarding involuntary treatment and involuntary commitment.\nThere are several codes of conduct. The Hippocratic Oath discusses basic principles for medical professionals.[5] This document dates back to the fifth century BCE.[6] Both The Declaration of Helsinki (1964) and The Nuremberg Code (1947) are two well-known and well respected documents contributing to medical ethics. Other important markings in the history of medical ethics include Roe v. Wade[why?] in 1973 and the development of hemodialysis in the 1960s. With hemodialysis now available, but a limited number of dialysis machines to treat patients, an ethical question arose on which patients to treat and which ones not to treat, and which factors to use in making such a decision.[7] More recently, new techniques for gene editing aiming at treating, preventing, and curing diseases utilizing gene editing, are raising important moral questions about their applications in medicine and treatments as well as societal impacts on future generations.[8][9]\nAs this field continues to develop and change throughout history, the focus remains on fair, balanced, and moral thinking across all cultural and religious backgrounds around the world.[10][11] The field of medical ethics encompasses both practical application in clinical settings and scholarly work in philosophy, history, and sociology.\nMedical ethics encompasses beneficence, autonomy, and justice as they relate to conflicts such as euthanasia, patient confidentiality, informed consent, and conflicts of interest in healthcare.[12][13][14] In addition, medical ethics and culture are interconnected as different cultures implement ethical values differently, sometimes placing more emphasis on family values and downplaying the importance of autonomy. This leads to an increasing need for culturally sensitive physicians and ethical committees in hospitals and other healthcare settings.[10][11][15]\n\n\nMedical ethics relationships[edit]\nMedical ethics defines relationships in the following directions:\n\na medical worker — a patient;\na medical worker — a healthy person (relatives);\na medical worker — a medical worker.\nMedical ethics includes provisions on medical confidentiality, medical errors, iatrogenesis, duties of the doctor and the patient.\nMedical ethics is closely related to bioethics, but these are not identical concepts. Since the science of bioethics arose in an evolutionary way in the continuation of the development of medical ethics, it covers a wider range of issues.[16]\nMedical ethics is also related to the law. But ethics and law are not identical concepts. More often than not, ethics implies a higher standard of behavior than the law dictates.[17]\n\nHistory[edit]\nSee also: List of medical ethics cases\nA 12th-century Byzantine manuscript of the Hippocratic Oath\nAMA Code of Medical EthicsThe term medical ethics first dates back to 1803, when English author and physician Thomas Percival published a document describing the requirements and expectations of medical professionals within medical facilities. The Code of Ethics was then adapted by the American Medical Association in 1847, relying heavily on Percival's words.[18] Over the years in 1903, 1912, and 1947, revisions have been made to the original document.[18] The practice of medical ethics is widely accepted and practiced throughout the world.[4]\nHistorically, Western medical ethics may be traced to guidelines on the duty of physicians in antiquity, such as the Hippocratic Oath, and early Christian teachings. The",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Research ethics",
    "url": "https://en.wikipedia.org/wiki/Research_ethics",
    "content": "Ethical practice in scientific research\nYou can help expand this article with text translated from the corresponding article in Portuguese.  (January 2024) Click [show] for important translation instructions.\nView a machine-translated version of the Portuguese article.\nMachine translation, like DeepL or Google Translate, is a useful starting point for translations, but translators must revise errors as necessary and confirm that the translation is accurate, rather than simply copy-pasting machine-translated text into the English Wikipedia.\nConsider adding a topic to this template: there are already 540 articles in the main category, and specifying|topic= will aid in categorization.\nDo not translate text that appears unreliable or low-quality. If possible, verify the text with references provided in the foreign-language article.\nYou must provide copyright attribution in the edit summary accompanying your translation by providing an interlanguage link to the source of your translation. A model attribution edit summary is Content in this edit is translated from the existing Portuguese Wikipedia article at [[:pt:Ética na ciência]]; see its history for attribution.\nYou may also add the template {{Translated|pt|Ética na ciência}} to the talk page.\nFor more guidance, see Wikipedia:Translation.\nPart of a series onResearch\nResearch design\nEthics\nProposal\nQuestion\nWriting\nArgument\nReferencing\n\nResearch strategy\nInterdisciplinary\nMultimethodology\nQualitative\nArt-based\nQuantitative\n\nPhilosophical schools\nAntipositivism\n Constructivism\nCritical rationalism\nEmpiricism\nFallibilism\nPositivism\nPostpositivism\nPragmatism\nRealism\nCritical realism\nSubtle realism\n\nMethodology\nAction research\nArt methodology\nCritical theory\nGrounded theory\nHermeneutics\nHistoriography\nHuman subject research\nNarrative inquiry\nPhenomenology\nPragmatism\nScientific method\n\nMethods\nAnalysis\nCase study\nContent analysis\nDescriptive statistics\nDiscourse analysis\nEthnography\nAutoethnography\nExperiment\nField experiment\nSocial experiment\nQuasi-experiment\nField research\nHistorical method\nInferential statistics\nInterviews\nMapping\nCultural mapping\nPhenomenography\nSecondary research\nBibliometrics\nLiterature review\nMeta-analysis\nScoping review\nSystematic review\nScientific modelling\nSimulation\nSurvey\n\nTools and software\nArgument technology\nGIS software\nLIS software\nBibliometrics\nReference management\nScience software\nQualitative data analysis\nSimulation\nStatistics\n\nPhilosophy portalvte\nResearch ethics is a discipline within the study of applied ethics. Its scope ranges from general scientific integrity and misconduct to the treatment of human and animal subjects. The social responsibilities of scientists and researchers are not traditionally included and are less well defined.[1]\nThe discipline is most developed in medical research. Beyond the issues of falsification, fabrication, and plagiarism that arise in every scientific field, research design in human subject research and animal testing are the areas that raise ethical questions most often.\nThe list of historic cases includes many large-scale violations and crimes against humanity such as Nazi human experimentation and the Tuskegee syphilis experiment which led to international codes of research ethics. No approach has been universally accepted,[2][3][4] but typically cited codes are the 1947 Nuremberg Code, the 1964 Declaration of Helsinki, and the 1978 Belmont Report.\nToday, research ethics committees, such as those of the US, UK, and EU, govern and oversee the responsible conduct of research. One major goal being to reduce questionable research practices.\nResearch in other fields such as social sciences, information technology, biotechnology, or engineering may generate ethical concerns.[2][3][5][6][7][8]\n\n\nHistory[edit]\nThis section needs expansion. You can help by adding to it.  (March 2024)\nSee also: Unethical human experimentation, Ethics committee § History, and List of medical ethics cases\nThe list of historic cases includes many large scale violations and crimes against humanity such as Nazi human experimentation and the Tuskegee syphilis experiment which led to international codes of research ethics.[2][3][4] Medical ethics developed out of centuries of general malpractice and science motivated only by results. Medical ethics in turn led to today's more broad understanding in bioethics.[9]\n\nScientific conduct[edit]\nScientific integrity[edit]\nThis section is an excerpt from Scientific integrity.[edit]\nResearch integrity or scientific integrity is an aspect of research ethics that deals with best practice or rules of professional practice of scientists.\nFirst introduced in the 19th century by Charles Babbage, the concept of research integrity came to the fore in the late 1970s. A series of publicized scandals in the United States led to heightened debate on the ethical norms of sciences and the limitations of the self-regulation processes implemented by scientific communities and institutions. Formali",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Engineering ethics",
    "url": "https://en.wikipedia.org/wiki/Engineering_ethics",
    "content": "Moral principles within the field of engineering\nEngineering ethics is the field concerned with the system of moral principles that apply to the practice of engineering. The field examines and sets the obligations by engineers to society, to their clients, and to the profession. As a scholarly discipline, it is closely related to subjects such as the philosophy of science, the philosophy of engineering, and the ethics of technology.\n\n\nBackground and origins[edit]\nUp to the 19th century and growing concerns[edit]\nThe first Tay Bridge collapsed in 1879. At least sixty were killed.\nAs engineering rose as a distinct profession during the 19th century, engineers saw themselves as either independent professional practitioners or technical employees of large enterprises. There was considerable tension between the two sides as large industrial employers fought to maintain control of their employees.[1]\nIn the United States growing professionalism gave rise to the development of four founding engineering societies: The American Society of Civil Engineers (ASCE) (1851), the American Institute of Electrical Engineers (AIEE) (1884),[2] the American Society of Mechanical Engineers (ASME) (1880), and the American Institute of Mining Engineers (AIME) (1871).[3] ASCE and AIEE were more closely identified with the engineer as learned professional, where ASME, to an extent, and AIME almost entirely, identified with the view that the engineer is a technical employee.[4]\nEven so, at that time ethics was viewed as a personal rather than a broad professional concern.[5][6]: 6 \n\nTurn of the  20th century and turning point[edit]\nThe Boston molasses disaster provided a strong impetus for the establishment of professional licensing and codes of ethics in the United States.When the 19th century drew to a close and the 20th century began, there had been series of significant structural failures, including some spectacular bridge failures, notably the Ashtabula River Railroad Disaster (1876), Tay Bridge Disaster (1879), and the Quebec Bridge collapse (1907). These had a profound effect on engineers and forced the profession to confront shortcomings in technical and construction practice, as well as ethical standards.[7]\nOne response was the development of formal codes of ethics by three of the four founding engineering societies. AIEE adopted theirs in 1912. ASCE and ASME did so in 1914.[8] AIME did not adopt a code of ethics in its history.[4]\nConcerns for professional practice and protecting the public highlighted by these bridge failures, as well as the Boston molasses disaster (1919), provided impetus for another movement that had been underway for some time: to require formal credentials (Professional Engineering licensure in the US) as a requirement to practice. This involves meeting some combination of educational, experience, and testing requirements.[9]\nIn 1950, the Association of German Engineers developed an oath for all its members titled 'The Confession of the Engineers', directly hinting at the role of engineers in the atrocities committed during World War II.[10][11][12]\nOver the following decades most American states and Canadian provinces either required engineers to be licensed, or passed special legislation reserving title rights to organization of professional engineers.[13] The Canadian model is to require all persons working in fields of engineering that posed a risk to life, health, property, the public welfare and the environment to be licensed, and all provinces required licensing by the 1950s.\nThe US model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed.[14] This has perpetuated the split between professional engineers and those in private industry.[15] Professional societies have adopted generally uniform codes of ethics.\n\nRecent developments[edit]\nWilliam LeMessurier's response to design deficiencies uncovered after construction of the Citigroup Center is often cited as an example of ethical conduct.\nEfforts to promote ethical practice continue. In addition to the professional societies and chartering organizations efforts with their members, the Canadian Iron Ring and American Order of the Engineer trace their roots to the 1907 Quebec Bridge collapse. Both require members to swear an oath to uphold ethical practice and wear a symbolic ring as a reminder.\nIn the United States, the National Society of Professional Engineers released in 1946 its Canons of Ethics for Engineers and Rules of Professional Conduct, which evolved to the current Code of Ethics, adopted in 1964. These requests ultimately led to the creation of the Board of Ethical Review in 1954. Ethics cases rarely have easy answers, but th",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Professional ethics",
    "url": "https://en.wikipedia.org/wiki/Professional_ethics",
    "content": "Principles and rules which guide professional activity\nNot to be confused with Professional Ethics (journal).\n\"Professional Misconduct\" redirects here. For the 2023 album, see Professional Misconduct (album).\nA 12th-century Byzantine manuscript of the Hippocratic oath.\nProfessional ethics  encompass the personal and corporate standards of behavior expected of professionals.[1]\nThe word professionalism originally applied to vows of a religious order. By no later than the year 1675, the term had seen secular application and was applied to the three learned professions: divinity, law, and medicine.[2]  The term professionalism was also used for the military profession around this same time.\nProfessionals and those working in acknowledged professions exercise specialist knowledge and skill. How the use of this knowledge should be governed when providing a service to the public can be considered a moral issue and is termed \"professional ethics\".[3]\nOne of the earliest examples of professional ethics is the Hippocratic oath to which medical doctors still adhere to this day.\n\n\nComponents[edit]\nSome professional organizations may define their ethical approach in terms of a number of discrete components.[4] Typically these include honesty, trustworthiness, transparency, accountability, confidentiality, objectivity, respect, obedience to the law, and loyalty.\n\nImplementation[edit]\nMost professionals have internally enforced codes of practice that members of the profession must follow to prevent exploitation of the client and to preserve the integrity and reputation of the profession. This is not only for the benefit of the client but also for the benefit of those belonging to that profession. Disciplinary codes allow the profession to define a standard of conduct and ensure that individual practitioners meet this standard, by disciplining them from the professional body if they do not practice accordingly. This allows those professionals who act with a conscience to practice in the knowledge that they will not be undermined commercially by those who have fewer ethical qualms. It also maintains the public’s trust in the profession, encouraging the public to continue seeking their services.\n\nInternal regulation[edit]\nIn cases where professional bodies regulate their own ethics, there are possibilities for such bodies to become self-serving and fail to follow their own ethical code when dealing with renegade members. This is particularly true of professions in which they have almost a complete monopoly on a particular area of knowledge. For example, until recently, the English courts deferred to the professional consensus on matters relating to their practice that lay outside case law and legislation.[5]\nNew UK research shows that lawyers “are sometimes too inclined to engage in professionally questionable, and potentially even illegal, actions without fully reflecting on the legal rules and interests engaged”. It found the potential for the rule of law to be challenged by certain forms of lawyer conduct was “widespread and significant”.[6] Recent events (2023) in the USA seem to reflect similar issues.\n\nStatutory regulation[edit]\nIn many countries there is some statutory regulation of professional ethical standards such as the statutory bodies that regulate nursing and midwifery in England and Wales.[7] Failure to comply with these standards can thus become a matter for the courts.\n\nExamples[edit]\nFor example, a lay member of the public should not be held responsible for failing to act to save a car crash victim because they could not give an appropriate emergency treatment, though, they are responsible for attempting to get help for the victim. This is because they do not have the relevant knowledge and experience. In contrast, a fully trained doctor (with the correct equipment) would be capable of making the correct diagnosis and carrying out appropriate procedures. Failure of a doctor to help at all in such a situation would generally be regarded as negligent and unethical. Though, if a doctor helps and makes a mistake that is considered negligent and unethical, there could be egregious repercussions. An untrained person would only be considered to be negligent for failing to act if they did nothing at all to help and is protected by the \"Good Samaritan\" laws if they unintentionally caused more damage and possible loss of life.\nA business may approach a professional engineer to certify the safety of a project which is not safe. While one engineer may refuse to certify the project on moral grounds, the business may find a less scrupulous engineer who will be prepared to certify the project for a bribe, thus saving the business the expense of redesigning.[8]\nSome corporations have tried to burnish their ethical image by creating whistle-blower protections, such as anonymity. In the case of Citi, they call this the \"Ethics Hotline\",[9] though it is unclear whether firms such as Citi take offences reported to these hotl",
    "category": "AI Ethics"
  },
  {
    "source": "Wikipedia",
    "title": "Applied ethics",
    "url": "https://en.wikipedia.org/wiki/Applied_ethics",
    "content": "Practical application of moral considerations\n\"Practical ethics\" redirects here. For the book, see Practical Ethics.\n\n\nThis article includes a list of general references, but it lacks sufficient corresponding inline citations. Please help to improve this article by introducing more precise citations. (September 2011) (Learn how and when to remove this message)\nApplied ethics is the practical aspect of moral considerations. It is ethics with respect to real-world actions and their moral considerations in private and public life, the professions, health, technology, law, and leadership.[1] For example, bioethics is concerned with identifying the best approach to moral issues in the life sciences, such as euthanasia, the allocation of scarce health resources, or the use of human embryos in research.[2][3][4] Environmental ethics is concerned with ecological issues such as the responsibility of government and corporations to clean up pollution.[5] Business ethics includes the duties of whistleblowers to the public and to their employers.[6]\n\n\nHistory[edit]\nApplied ethics has expanded the study of ethics beyond the realms of academic philosophical discourse.[7] The field of applied ethics, as it appears today, emerged from debate surrounding rapid medical and technological advances in the early 1970s and is now established as a subdiscipline of moral philosophy. However, applied ethics is, by its very nature, a multi-professional subject because it requires specialist understanding of the potential ethical issues in fields like medicine, business or information technology. Nowadays, ethical codes of conduct exist in almost every profession.[8]\nAn applied ethics approach to the examination of moral dilemmas can take many different forms but one of the most influential and most widely utilised approaches in bioethics and health care ethics is the four-principle approach developed by Tom Beauchamp and James Childress.[9] The four-principle approach, commonly termed principlism, entails consideration and application of four prima facie ethical principles: autonomy, non-maleficence, beneficence, and justice.\n\nUnderpinning theory[edit]\nApplied ethics is distinguished from normative ethics, which concerns standards for right and wrong behavior, and from meta-ethics, which concerns the nature of ethical properties, statements, attitudes, and judgments.[10]\nWhilst these three areas of ethics appear to be distinct, they are also interrelated. The use of an applied ethics approach often draws upon these normative ethical theories:\n\nConsequentialist ethics, which hold that the rightness of acts depends only on their consequences.[11] The paradigmatic consequentialist theory is utilitarianism, which classically holds that whether an act is morally right depends on whether it maximizes net aggregated psychological wellbeing. This theory's main developments came from Jeremy Bentham and John Stuart Mill who distinguished between act and rule utilitarianism. Notable later developments were made by Henry Sidgwick who introduced the significance of motive or intent, and R. M. Hare who introduced[12] the significance of preference in utilitarian decision-making. Other forms of consequentialism include prioritarianism.\nDeontological ethics, which hold that acts have an inherent rightness or wrongness regardless of their context or consequences. This approach is epitomized by Immanuel Kant's notion of the categorical imperative, which was the centre of Kant's ethical theory based on duty. Another key deontological theory is natural law, which was heavily developed by Thomas Aquinas and is an important part of the Catholic Church's teaching on morals. Threshold deontology holds that rules ought to govern up to a point despite adverse consequences; but when the consequences become so dire that they cross a stipulated threshold, consequentialism takes over.[13]\nVirtue ethics, derived from Aristotle's and Confucius' notions, which asserts that the right action will be that chosen by a suitably 'virtuous' agent.\nNormative ethical theories can clash when trying to resolve real-world ethical dilemmas. One approach attempting to overcome the divide between consequentialism and deontology is case-based reasoning, also known as casuistry. Casuistry does not begin with theory, rather it starts with the immediate facts of a real and concrete case. While casuistry makes use of ethical theory, it does not view ethical theory as the most important feature of moral reasoning. Casuists, like Albert Jonsen and Stephen Toulmin (The Abuse of Casuistry, 1988), challenge the traditional paradigm of applied ethics. Instead of starting from theory and applying theory to a particular case, casuists start with the particular case itself and then ask what morally significant features (including both theory and practical considerations) ought to be considered for that particular case. In their observations of medical ethics committees, Jonsen and Toulmin note t",
    "category": "AI Ethics"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Popular Mechanics",
    "url": "https://en.wikipedia.org/wiki/Popular_Mechanics",
    "content": "American science magazine\nThis article is about the magazine. For other uses, see Popular Mechanics (disambiguation).\n\n\nPopular MechanicsPopular Mechanics first cover (January 11, 1902)CategoriesAutomotive, DIY, Science, TechnologyFrequencySix print issues/yearCirculation401,507[1]Total circulation(2024)17.5M[2]\n17.9M digital\n0.4 print\n\n[3]First issueJanuary 11, 1902; 123 years ago (1902-01-11)CompanyHearstCountryUnited StatesBased inNew York City, New YorkLanguageEnglishWebsitewww.popularmechanics.com ISSN0032-4558\nPopular Mechanics (often abbreviated as PM or PopMech) is a magazine of popular science and technology, featuring automotive, home, outdoor, electronics, science, do it yourself, and technology topics. Military topics, aviation and transportation of all types, space, tools and gadgets are commonly featured.[4]\nIt was founded in 1902 by Henry Haven Windsor, who was the editor and—as owner of the Popular Mechanics Company—the publisher. For decades, the tagline of the monthly magazine was \"Written so you can understand it.\" In 1958, PM was purchased by the Hearst Corporation, now Hearst Communications.[5]\nIn 2013, the US edition changed from twelve to ten issues per year, and in 2014 the tagline was changed to \"How your world works.\"[6] The magazine added a podcast in recent years, including regular features Most Useful Podcast Ever and How Your World Works.[7]\n\n\nHistory[edit]\nCover of April 1924 issue, 25 cents (equivalent to $4.59 in 2024)\nPopular Mechanics was founded as a weekly in Chicago by Henry Haven Windsor, with the first issue dated January 11, 1902. His concept was that it would explain \"the way the world works\" in plain language, with photos and illustrations to aid comprehension.[5] For decades, its tagline was: \"Written so you can understand it.\"[8]\nIn September 1902, the magazine, formerly a weekly, became a monthly. The Popular Mechanics Company was owned by the Windsor family and printed in Chicago until the Hearst Corporation purchased the magazine in 1958. In 1962, the editorial offices moved to New York City.[9]\nIn 2020, Popular Mechanics relocated to Easton, Pennsylvania, along with the two additional brands in the Hearst Enthusiast Group (Bicycling and Runner's World).[10][11] That location has also included Popular Mechanics' testing facility, called the Test Zone.[12]\nFrom the first issue, the magazine featured a large illustration of a technological subject, a look that evolved into the magazine's characteristic full-page, full-color illustration and a small 6.5-by-9.5-inch (170 mm × 240 mm) trim size beginning with the July 1911 issue. It maintained the small format until 1975 when it switched to a larger standard trim size.[clarification needed][13]\nIn 1915, Popular Mechanics adopted full-color cover illustrations, and the look was widely imitated by later technology magazines.[13]\nAfter World War II ended, in 1945, a number of international editions were introduced, starting with a French edition, followed b",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Popular Science",
    "url": "https://en.wikipedia.org/wiki/Popular_Science",
    "content": "American popular science website\nThis article is about the magazine. For the general concept of interpreting science for a broad audience, see popular science. For the 1935–1949 film series, see Popular Science (film series). For similar magazines, see List of science magazines.\n\n\nPopular ScienceGeneral ManagerAdam MorathCategoriesInterdisciplinaryFrequencyFully digitalTotal circulation(June 2014)1,321,075[1]FoundedMay 1872; 153 years ago (1872-05) (as The Popular Science Monthly)Final issueApril 27, 2021 (print)CompanyRecurrent VenturesCountryUnited StatesBased inNew York, New YorkWebsitepopsci.comISSN0161-7370OCLC488612811\nPopular Science (also known as PopSci) is an American popular science website, covering science and technology topics geared toward general readers. Popular Science has won over 58 awards, including the American Society of Magazine Editors awards for its journalistic excellence in 2003 (for General Excellence), 2004 (for Best Magazine Section), and 2019 (for Single-Topic Issue). Its print magazine, which ran from 1872 to 2020, was translated into over 30 languages and distributed to at least 45 countries.[2] In 2021, Popular Science switched to an all-digital format and abandoned the magazine format in 2023.[3][4]\n\n\nEarly history[edit]\nThe Popular Science Monthly, as the publication was originally called, was founded in May 1872[5] by Edward L. Youmans to disseminate scientific knowledge to the educated layman. Youmans had previously worked as an editor for the weekly Appleton's Journal and persuaded them to publish his new journal. Early issues were mostly reprints of English periodicals. The journal became an outlet for writings and ideas of Charles Darwin, Thomas Henry Huxley, Louis Pasteur, Henry Ward Beecher, Charles Sanders Peirce, William James, Thomas Edison, John Dewey and James McKeen Cattell. William Jay Youmans, Edward's brother, helped found Popular Science Monthly in 1872 and was an editor as well. He became editor-in-chief on Edward's death in 1887.[6]  The publisher, D. Appleton & Company, was forced to sell the journal for economic reasons in 1900.[7]\nJames McKeen Cattell became the editor in 1900 and the publisher in 1901. Cattell had a background in academics and continued publishing articles for educated readers. By 1915, the readership was declining and publishing a science journal was a financial challenge. In a September 1915 editorial, Cattell related these difficulties to his readers and announced that the Popular Science Monthly name had been transferred to the Modern Publishing Company to start a new publication for general audiences. The existing academic journal would continue publishing under the name The Scientific Monthly, retaining existing subscribers.[8] Scientific Monthly was published until 1958 when it was absorbed into Science.[9]\nAfter acquiring the Electrician and Mechanic magazine in 1914, the Modern Publishing Company had merged it with Modern Electrics to become Modern Electrics & M",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "MIT Technology Review",
    "url": "https://en.wikipedia.org/wiki/MIT_Technology_Review",
    "content": "Magazine about technology\nThis article relies excessively on references to primary sources. Please improve this article by adding secondary or tertiary sources. Find sources: \"MIT Technology Review\" – news · newspapers · books · scholar · JSTOR (February 2024) (Learn how and when to remove this message)\n\n\nMIT Technology ReviewCover of 125th anniversary issue (September–October 2024)Editor-in-ChiefMat Honan[1]CategoriesScience, technologyFrequencyBimonthlyCirculation208,658[2]PublisherElizabeth Bramson-BoudreauFirst issue1899; 126 years ago (1899)CompanyMIT Technology Review[3]CountryUnited StatesBased inCambridge, MassachusettsLanguageEnglishWebsitetechnologyreview.com ISSN1099-274X\nMIT Technology Review is a bimonthly magazine wholly owned by the Massachusetts Institute of Technology. It was founded in 1899 as The Technology Review,[4] and was re-launched without the leading article in its name on April 23, 1998, under then publisher R. Bruce Journey. In September 2005, it was changed, under its then editor-in-chief and publisher, Jason Pontin, to a form resembling the historical magazine.\nBefore the 1998 re-launch, the editor stated that \"nothing will be left of the old magazine except the name.\" It was therefore necessary to distinguish between the modern and the historical Technology Review.[4] The historical magazine had been published by the MIT Alumni Association, was more closely aligned with the interests of MIT alumni, and had a more intellectual tone and much smaller public circulation. The magazine, billed from 1998 to 2005 as \"MIT's Magazine of Innovation\", and from 2005 onwards as simply \"published by MIT\", focused on new technology and how it is commercialized; was sold to the public and targeted at senior executives, researchers, financiers, and policymakers, as well as MIT alumni.[4][5]\nIn 2011, Technology Review received an Utne Reader Independent Press Award for Best Science/Technology Coverage.[6]\n\n\nHistory[edit]\nOriginal magazine: 1899–1998[edit]\nTechnology Review was founded in 1899 under the name The Technology Review and relaunched in 1998 without \"The\" in its original name. It currently claims to be \"the oldest technology magazine in the world.\"[7]\nIn 1899, The New York Times commented:[8]\n\nWe give a cordial welcome to No. 1 of Vol. I of The Technology Review, a Quarterly Magazine Relating to the Massachusetts Institute of Technology, published in Boston, and under charge of the Association of Class Secretaries. As far as make-up goes, cover, paper, typography and illustrations are in keeping with the strong characteristics of the Institution it represents. This magazine, as its editors announce, is intended to be \"a clearing house of information and thought,\" and, as far as the Institute of Technology is concerned, \"to increase its power, to minimize its waste, to insure [sic] among its countless friends the most perfect co-operation.\"\nThe career path of James Rhyne Killian illustrates the close ties between Technology R",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Wired (magazine)",
    "url": "https://en.wikipedia.org/wiki/Wired_(magazine)",
    "content": "American technology magazine\n\"WIRED\" redirects here. For the British offshoot, see Wired UK. For other uses, see Wired (disambiguation).\n\n\nWiredCover of the November/December 2024 issueGlobal Editorial DirectorKatie DrummondFormer US editors-in-chiefLouis Rossetto, Katrina Heron, Chris Anderson, Scott Dadich, Nick Thompson, Gideon LichfieldCategoriesBusiness, technology, lifestyle, thought leaderFrequencyBi-monthlyTotal circulation(2024)540,265[1]FounderLouis Rossetto, Jane MetcalfeFoundedFebruary 1991First issueJanuary 1993, as a quarterlyCompanyCondé Nast PublicationsCountryUnited StatesBased inSan Francisco, CaliforniaLanguageEnglishWebsitewired.com ISSN1059-1028 (print)1078-3148 (web)OCLC24479723\nWired is a bi-monthly American magazine that focuses on how emerging technologies affect culture, the economy, and politics. It is published in both print and online editions by Condé Nast. The magazine has been in publication since its launch in January 1993.[2] Its editorial office is based in San Francisco, California, with its business headquarters located in New York City.\nWired quickly became recognized as the voice of the emerging digital economy and culture[3] and a pace setter in print design and web design.[4][5] From 1998 until 2006, the magazine and its website, Wired.com, experienced separate ownership before being fully consolidated under Condé Nast in 2006. It has won multiple National Magazine Awards[6][7] and has been credited with shaping discourse around the digital revolution. The magazine also coined the term crowdsourcing,[8] as well as its annual tradition of handing out Vaporware Awards. \nWired has launched several international editions, including Wired UK, Wired Italia, Wired Japan, Wired Czech Republic and Slovakia,[9] and Wired Germany. The magazine was published monthly until 2024, when it switched to a bi-monthly schedule with six issues per year.\n\n\nHistory[edit]\nWired building located in San Francisco\nThe magazine was launched in 1993 by American expatriates Louis Rossetto with his life partner and business partner Jane Metcalfe. Wired was originally conceived in Amsterdam, the Netherlands, when they were working on Electric Word, a small, groundbreaking technology magazine that developed a global following because of its focus not just on hardware and software, but the people, companies, and ideas that were part of what they called the language industries.[10] Whole Earth Review called it \"The Least Boring Computer Magazine in the World\". This broader focus on the social, economic, and political issues surrounding technology became the core of the Wired editorial approach.[10]\nInitial funding for Wired was provided by Eckart Wintzen, a Dutch entrepreneur. His Origin software company extended a contract for advertising and bought the first 1000 subscribers.[11] Rossetto and Metcalfe moved back to the United States to start Wired, finding the European Union not a cohesive enough media market to support a continent-wide p",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "IEEE Spectrum",
    "url": "https://en.wikipedia.org/wiki/IEEE_Spectrum",
    "content": "Magazine edited by the Institute of Electrical and Electronics Engineers\n\n\nIEEE SpectrumFront cover of the January 2022 issueEditorHarry GoldsteinCategoriesElectrical engineeringCirculation380,000PublisherInstitute of Electrical and Electronics EngineersFirst issueJanuary 1964CountryUnited StatesBased inNew York City, New YorkWebsitespectrum.ieee.orgISSN0018-9235\nIEEE Spectrum is a magazine edited and published by the Institute of Electrical and Electronics Engineers.\nThe first issue of IEEE Spectrum was published in January 1964 as a successor to Electrical Engineering.\nIn 2010, IEEE Spectrum was the recipient of Utne Reader magazine's Utne Independent Press Award for Science/Technology Coverage.[1] In 2012, IEEE Spectrum was selected as the winner of the National Magazine Awards \"General Excellence Among Thought Leader Magazines\" category.[2]\n\nReferences[edit]\n\n\n^ \"Winners of the 2010 Utne Independent Press Awards\". Archived from the original on March 21, 2012. Retrieved October 27, 2010.\n\n^ \"National Magazine Awards General Excellence Among Thought Leader Magazines\" (Press release). May 7, 2012. Retrieved April 21, 2015.\n\n\nExternal links[edit]\nOfficial website\n\n\nThis science and technology magazine–related article is a stub. You can help Wikipedia by expanding it.See tips for writing articles about magazines. Further suggestions might be found on the article's talk page.vte\n\n\n\n\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=IEEE_Spectrum&oldid=1289713409\"",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Scientific American",
    "url": "https://en.wikipedia.org/wiki/Scientific_American",
    "content": "American monthly science magazine\n\n\nAcademic journalScientific AmericanCover of a 1905 issueDisciplinePopular scienceLanguageEnglishPublication detailsHistorySince August 28, 1845; 180 years ago (1845-08-28)PublisherSpringer Nature (United States)FrequencyMonthlyOpen accessYesImpact factor3.1 (2023)Standard abbreviationsISO 4 (alt) · Bluebook (alt)NLM (alt) · MathSciNet (alt )ISO 4Sci. Am.IndexingCODEN (alt · alt2) · JSTOR (alt) · LCCN (alt)MIAR · NLM (alt) · Scopus · W&LISSN0036-8733LCCNsf92091111OCLC no.796985030Links\nJournal homepage\n\n\"Men of Progress\", published by the magazine in 1862, showing American inventors such as Samuel Morse, Samuel Colt, Cyrus McCormick, Charles Goodyear, Peter Cooper, and others[1]\nScientific American Office, New York, 37 Park Row, 1859, next to Munn & Co. on the right\nScientific American, informally abbreviated SciAm or sometimes SA, is an American popular science magazine. Many scientists, including Albert Einstein and Nikola Tesla, have contributed articles to it, with more than 150 Nobel Prize-winners having been featured since its inception.[2]\nIn print since 1845, it is the oldest continuously published magazine in the United States. Scientific American is owned by Springer Nature, which is a subsidiary of Holtzbrinck Publishing Group.\n\n\nHistory[edit]\nInterior of Scientific American's office at 361 Broadway in New York City\nScientific American's early office at 361 Broadway in Manhattan\nA 2011 photo of Scientific American's office at the Woolworth Building in New York City, built in 1913 by Frank Winfield Woolworth[3]\nThe Scientific American building at 24-26 West 40th Street, commissioned by Munn and Co. in 1924[3]\nScientific American was founded by inventor and publisher Rufus Porter in 1845[4] as a four-page weekly newspaper. The first issue of the large-format New York City newspaper was released on August 28, 1845.[5]\nThroughout its early years, much emphasis was placed on reports of what was going on at the U.S. Patent Office. It also reported on a broad range of inventions including perpetual motion machines, an 1860 device for buoying vessels by Abraham Lincoln, and the universal joint, which now can be found in nearly every automobile manufactured. Current issues include a \"this date in history\" section, featuring excerpts from articles originally published 50, 100, and 150 years earlier. Topics include humorous incidents, wrong-headed theories, and noteworthy advances in the history of science and technology. It started as a weekly publication in August 1845 before turning into a monthly in November 1921.[6]\nPorter sold the publication to Alfred Ely Beach, son of media magnate Moses Yale Beach, and Orson Desaix Munn, a mere ten months after founding it. Editors and co-owners from the Yale family included Frederick C. Beach and his son, Stanley Yale Beach, and from the Munn family, Charles Allen Munn and his nephew, Orson Desaix Munn II.[7] Until 1948, it remained owned by the families under Munn & C",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Technology journalism",
    "url": "https://en.wikipedia.org/wiki/Technology_journalism",
    "content": "Journalism genre\nJournalism\nNews\nArticle (Headline · Byline · Dateline · Report · Special report · Exclusive · Interview · Column · Opinion piece · Editorial · Op-ed)\nWriting style (Five Ws · Inverted pyramid)\nIndex of journalism articles\n\nBeats\nArts\nBusiness\nData\nEntertainment\nEnvironment\nFashion\nLocal\nMedicine\nMusic\nPolitics\nScience\nSports\nTechnology\nTraffic\nVideo games\nWar\nWeather\nWorld\n\n\nGenres\nAdversarial\nAdvocacy (Interventionism)\nAnalytic\nBlogging\nCitizen\nCivic\nCollaborative\nComics-based\nCommunity\nDatabase\nEnterprise\nExplanatory\nFeature story\nGonzo\nHuman-interest\nImmersion\nInfotainment/Soft media\nInterpretive\nInvestigative\nLong-form\nNarrative\nNew Journalism\nOpinion\nPeace\nSensor\nUnderground\nVisual\nWatchdog\n\n\nEthics and standards\nChequebook\nChurnalism\nCodes of ethics\nCulture\nEditing (Copy editing · Corrections · Fact-checking · Spiking)\nFake news (Websites)\nHorse race\nJournalese\nMedia bias (False balance)\nNews values (Above the fold · Man bites dog)\nObjectivity\nPink-slime\nScandals\nSensationalism\nSources\nTabloid (Television)\nYellow\nJournalism school (Student publication)\n\n\nChallenges\nDecline of newspapers (News desert)\nFourth Estate\nFifth Estate\nEditorial independence (Independent · State)\nMedia capture · Media plurality\nFreedom of information\nFreedom of the press (Defamation · Safety)\nSource protection\n\n\nPublic relations\nMedia manipulation\nMedia relations\nNews embargo\nNews leak\nNews propaganda (Model)\nPress conference (Media scrum)\nPress gallery\nPress line\nPress pass\nPress pool\nPress release\nSound bite\nSpin\nSpin room\n\n\nNews media\nNewspapers (Newspaper of record · Middle-market · Newspaper formats · Broadsheet · Tabloid · Online · Extra edition)\nMagazines\nBroadcast (TV and radio)\nDigital\nMultimedia (Video)\nDocumentary film/television\nNews agencies\nAlternative media\nNonprofit\nPublic service\n\n\nNewsroom\nJournalist (Staff writer · Correspondent · Photojournalist · News presenter)\nColumnist\nEditors (Editor-in-chief · Managing editor · Political editor · Editorial board · Assignment editor · Duty editor · Public editor · Editor-at-large · Contributing editor)\nWeather presenter\nNews bureau\nNews director\nPundit\nStringer\n\n\n Journalism portal\nCategory: Journalismvte\n\"Tech news\" redirects here. For the student newspaper, see Worcester Polytechnic Institute. For tech news on Wikipedia, see Wikipedia:Tech news.\nTechnology journalism is the genre of reporting that concerns the development, marketing, and application of technology. The field covers a wide variety of topics, including communications, artificial intelligence, the Internet, social media, the IT industry, scientific research, robotics, and laws and policies regarding the digital world. Many news organizations feature prominent coverage of technology, especially as pertains to the economic, political, and cultural role of Big Tech companies and products. \n\n\nHistory[edit]\nEarly developments in the Information Age were often covered by established news organizations under the umbrella of science j",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Science communication",
    "url": "https://en.wikipedia.org/wiki/Science_communication",
    "content": "Public communication of science-related topics to non-experts\nNot to be confused with Scientific literature, Scientific communication, or Scholarly communication.\nFor the academic journal, see Science Communication.\n\n\nSchematic overview of the field and the actors of science communication according to Carsten Könneker\nScience communication encompasses a wide range of activities that connect science and society.[1] Common goals of science communication include informing non-experts about scientific findings, raising the public awareness of and interest in science, influencing people's attitudes and behaviors, informing public policy, and engaging with diverse communities to address societal problems.[2] The term \"science communication\" generally refers to settings in which audiences are not experts on the scientific topic being discussed (outreach), though some authors categorize expert-to-expert communication (\"inreach\" such as publication in scientific journals) as a type of science communication.[3] Examples of outreach include science journalism[4][5] and health communication.[6] Since science has political, moral, and legal implications,[7] science communication can help bridge gaps between different stakeholders in public policy, industry, and civil society.[8]\n\nPart of a series onScience\nGeneral\nHistory\nLiterature\nMethod\nPhilosophy\n\nBranches\nFormal\nNatural\nPhysical\nLife\nSocial\nBehavioral\nCognitive\nApplied\n\nIn society\nCommunication\nCommunity\nEducation\nFunding\nPolicy\nPseudoscience\nScientist\n\n Science portal\nOutline\nCategory\nArticle indexes\nGlossariesvte\n\nScience communicators are a broad group of people: scientific experts, science journalists, science artists, medical professionals, nature center educators, science advisors for policymakers, and everyone else who communicates with the public about science.[9][10] They often use entertainment and persuasion techniques including humour, storytelling, and metaphors to connect with their audience's values and interests.[11][12][13][14]\nScience communication also exists as an interdisciplinary field of social science research[15][2] on topics such as misinformation,[16][17][18] public opinion of emerging technologies,[19][20][21] and the politicization and polarization of science.[22][23][24][25] For decades, science communication research has had only limited influence on science communication practice, and vice-versa,[8][26] but both communities are increasingly attempting to bridge research and practice.[27][28][29]\nHistorically, academic scientists were discouraged from spending time on public outreach, but that has begun to change. Research funders have raised their expectations for researchers to have broader impacts beyond publication in academic journals.[30] An increasing number of scientists, especially younger scholars, are expressing interest in engaging the public through social media and in-person events, though they still perceive significant institutional barriers to doing so.[31]",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Emerging technologies",
    "url": "https://en.wikipedia.org/wiki/Emerging_technologies",
    "content": "Technologies whose development, practical applications, or both are still largely unrealized\nFor specific emerging technologies, see the List of emerging technologies\nHistory of technology\nBy technological eras\nPremodern / Pre-industrial\nPrehistoric\nStone Age (lithic)\nFirst Agricultural Revolution\nCopper Age\nUrban revolution\nBronze Age\nIron Age\nAncient\nModern\nProto-industrialization\nSecond Agricultural Revolution\nFirst Industrial Revolution\nStandardization\nSecond Industrial Revolution\nMachine Age\nJet Age\nThird Agricultural Revolution\nAtomic Age\nSpace Age\nThird Industrial Revolution\nDigital transformation\nInformation Age\nFourth Industrial Revolution\nImagination Age\nFuture\nPost-industrial\nSingularity\nEmerging technologies\n\nBy historical regions\nAncient Africa\nAncient Egypt\nIndian subcontinent\nAncient China\nMaya civilization\nHellenistic world\nRoman Empire\nByzantine Empire\nMedieval Islamic world\nArab Agricultural Revolution\nMedieval Europe\nRenaissance Europe\n\nBy type of technology\nHistory of agriculture\nHistory of biotechnology\nHistory of communication\nHistory of computer hardware\nHistory of electrical engineering\nHistory of manufacturing\nHistory of maritime\nHistory of materials science\nHistory of measurement\nHistory of medicine\nHistory of simple machine\nHistory of nuclear technology\nHistory of transport\n\nTechnology timelines\nTimeline of historic inventions\nTechnological revolution\nComplete list by category\n\nArticle indices\nOutline of technology\nOutline of prehistoric technology\nvte\nTranshumanism\nIssues\nAccelerating change\nEradication of suffering\nFourth Industrial Revolution\nHuman enhancement\nGenetic\nMoral\nNeuro-\nCognitive liberty\nNew eugenics\nEugenics\nHuman nature\nMeliorism\nPost-politics\nPost-scarcity\n\n\nPeople\nAndrews\nBostrom\nChurch\nCordeiro\nde Chardin\nDrexler\nFahy\nFM-2030\nFreitas\nFyodorov\nFuller\nde Garis\nGasson\nGoertzel\nde Grey\nHaldane\nHanson\nHarari\nHarbisson\nHarris\nHuxley\nHughes\nIstvan\nJohnson\nKurzweil\nLand\nMoen\nMoravec\nMore\nMusk\nPearce\nRothblatt\nSandberg\nSavulescu\nSorgner\nSpencer\nStock\nStolyarov\nThiel\nVinge\nVita-More\nWalker\nWarwick\nWiener\nYudkowsky\n\n\nInfluential works\nOration on the Dignity of Man (1486)\nThus Spoke Zarathustra (1883)\nLooking Backward  (1888)\nThe Will to Power (~1901)\nDaedalus (1924)\nLa raza cósmica (1925)\nThe Phenomenon of Man (1955)\nThe Dialectic of Sex (1970)\nMetaman (1993)\nThe Hedonistic Imperative (1995)\nRegeln für den Menschenpark (1997)\nThe Age of Spiritual Machines (1999)\nCitizen Cyborg (2004)\nThe Singularity Is Near (2005)\nHuman Enhancement (2009)\nFanged Noumena (2011)\nThe Transhumanist Wager (2013)\nSapiens (2014)\nHomo Deus (2015)\nThe Transhumanist Bill of Rights (2015)\nThe Age of Em (2016)\nThe Precipice (2020)\nWhat We Owe the Future (2022)\n\"Techno-Optimist Manifesto\" (2023)\n\n\nVariants\nAccelerationism\nEffective\nCypherpunk\nDataism\nExtropianism\nImmortalism\nLongtermism\nPostgenderism\nPosthumanism\nRussian Cosmism\nSingularitarianism\nTechnogaianism\nTechnolibertarianism\nTechnological utopianism\nTechno-progressivism\n\n\nRelated top",
    "category": "Popular Science"
  },
  {
    "source": "Popular Science Wiki",
    "title": "Future technology",
    "url": "https://en.wikipedia.org/wiki/Future_technology",
    "content": "Future technology-related topics include:\n\nEmerging technologies, technologies that are perceived as capable of changing the status quo\nFutures studies (also called futurology), the study of postulating possible, probable, and preferable futures and the worldviews and myths that underlie them\nHypothetical technology, technology that does not exist yet, but that could exist in the future\nTechnology forecasting, attempts to predict the future characteristics of useful technological machines, procedures or techniques\n\n\n\n\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Future_technology&oldid=1318462838\"",
    "category": "Popular Science"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Isaac Asimov",
    "url": "https://en.wikipedia.org/wiki/Isaac_Asimov",
    "content": "American writer and biochemist (1920–1992)\n\"Asimov\" redirects here. For other uses, see Asimov (disambiguation).\n\n\nIsaac AsimovPhoto c. 1959Bornc. January 2, 1920[a]Petrovichi, Russian SFSRDiedApril 6, 1992(1992-04-06) (aged 72)New York City, U.S.OccupationWriter, professor of biochemistryCitizenshipUnited StatesEducationColumbia University (BS, MA, PhD)GenreScience fiction hard SFsocial SFmysterypopular scienceSubjectPopular sciencescience textbooksessayshistoryliterary criticismLiterary movementGolden Age of Science FictionYears active1939–1992Spouse\nGertrude Blugerman\n​ ​(m. 1942; div. 1973)​\nJanet Opal Jeppson ​(m. 1973)​Children2RelativesStanley Asimov (brother)Eric Asimov (nephew)SignatureScientific careerFieldsBiochemistryInstitutionsBoston UniversityThesisThe kinetics of the reaction inactivation of tyrosinase during its catalysis of the aerobic oxidation of catechol (1948)Doctoral advisorCharles Reginald DawsonOther academic advisorsRobert Elderfield (post-doctoral)\n\n\nIsaac Asimov (/ˈæzɪmɒv/ AZ-im-ov;[b][c] c. January 2, 1920[a] – April 6, 1992) was an American writer and professor of biochemistry at Boston University. During his lifetime, Asimov was considered one of the \"Big Three\" science fiction writers, along with Robert A. Heinlein and Arthur C. Clarke.[2] A prolific writer, he wrote or edited more than 500 books. He also wrote an estimated 90,000 letters and postcards.[d] Best known for his hard science fiction, Asimov also wrote mysteries and fantasy, as well as popular science and other non-fiction.\nAsimov's most famous work is the Foundation series,[3] the first three books of which won the one-time Hugo Award for \"Best All-Time Series\" in 1966.[4] His other major series are the Galactic Empire series and the Robot series, which include major works such as The Caves of Steel and \"The Naked Sun\", both written in the mid-1950s. The Galactic Empire novels are set in the much earlier history of the same fictional universe as the Foundation series. Later, with Foundation and Earth (1986), he linked this distant future to the Robot series, creating a unified \"future history\" for his works.[5] He also wrote more than 380 short stories, including the social science fiction novelette \"Nightfall\", which in 1964 was voted the best short science fiction story of all time by the Science Fiction Writers of America. Asimov wrote the Lucky Starr series of juvenile science-fiction novels using the pen name Paul French.[6]\nMost of his popular science books explain concepts in a historical way, going as far back as possible to a time when the science in question was at its simplest stage. Examples include Guide to Science, the three-volume Understanding Physics, and Asimov's Chronology of Science and Discovery. He wrote on numerous other scientific and non-scientific topics, such as chemistry, astronomy, mathematics, history, biblical exegesis, and literary criticism.\nHe was the president of the American Humanist Association.[7] Several entities have been named in his honor, including the asteroid (5020) Asimov,[8] a crater on Mars,[9][10] a Brooklyn elementary school,[11] Honda's humanoid robot ASIMO,[12] and four literary awards.\n\n\nSurname[edit]\nThere are three very simple English words: 'Has', 'him' and 'of'. Put them together like this—'has-him-of'—and say it in the ordinary fashion. Now leave out the two h's and say it again and you have Asimov.— Asimov, 1979[13]\nAsimov's family name derives from the first part of озимый хлеб (ozímyj khleb), meaning 'winter grain' (specifically rye), in which his great-great-great-grandfather dealt, with the Russian surname ending -ov added.[14] Azimov is spelled Азимов in the Cyrillic alphabet.[1] When the family arrived in the United States in 1923 and their name had to be spelled in the Latin alphabet, Asimov's father spelled it with an S, believing this letter to be pronounced like Z (as in German), and so it became Asimov.[1] This later inspired one of Asimov's short stories, \"Spell ",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Three Laws of Robotics",
    "url": "https://en.wikipedia.org/wiki/Three_Laws_of_Robotics",
    "content": "Fictional set of rules by Isaac Asimov\nLaws of robotics\nIsaac Asimov\n\nThree Laws of Robotics\nin popular culture\n\n\nRelated topics\n\nRoboethics\nEthics of AI\nMachine ethics\n\nvte\nThis cover of I, Robot illustrates the story \"Runaround\", the first to list all Three Laws of Robotics.\nThe Three Laws of Robotics (often shortened to The Three Laws or Asimov's Laws) are a set of rules devised by science fiction author Isaac Asimov, which were to be followed by robots in several of his stories. The rules were introduced in his 1942 short story \"Runaround\" (included in the 1950 collection I, Robot), although similar restrictions had been implied in earlier stories.\n\n\nThe Laws[edit]\nThe Three Laws, presented to be from the fictional \"Handbook of Robotics, 56th Edition, 2058 A.D.\", are:[1]\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\nUse in fiction[edit]\nThe Three Laws form an organizing principle and unifying theme for Asimov's robot-based fiction, appearing in his Robot series, the stories linked to it, and in his (initially pseudonymous) Lucky Starr series of young-adult fiction. The Laws are incorporated into almost all of the positronic robots appearing in his fiction, and cannot be bypassed, being intended as a safety feature. A number of Asimov's robot-focused stories involve robots behaving in unusual and counter-intuitive ways as an unintended consequence of how the robot applies the Three Laws to the situation in which it finds itself. \nOther authors working in Asimov's fictional universe have adopted them and references appear throughout science fiction as well as in other genres.\nThe original laws have been altered and elaborated on by Asimov and other authors. Asimov himself made slight modifications to the first three in subsequent works to further develop how robots would interact with humans and each other. In later fiction where robots had taken responsibility for government of whole planets and human civilizations, Asimov also added a fourth, or zeroth law, to precede the others.\nThe Three Laws have also influenced thought on the ethics of artificial intelligence.\n\nHistory[edit]\nIn The Rest of the Robots, published in 1964, Isaac Asimov noted that when he began writing in 1940 he felt that \"one of the stock plots of science fiction was ... robots were created and destroyed their creator. Knowledge has its dangers, yes, but is the response to be a retreat from knowledge? Or is knowledge to be used as itself a barrier to the dangers it brings?\" He decided that in his stories a robot would not \"turn stupidly on his creator for no purpose but to demonstrate, for one more weary time, the crime and punishment of Faust.\"[2]\nOn May 3, 1939, Asimov attended a meeting of the Queens (New York) Science Fiction Society where he met Earl and Otto Binder who had recently published a short story \"I, Robot\" featuring a sympathetic robot named Adam Link who was misunderstood and motivated by love and honor. (This was the first of a series of ten stories; the next year \"Adam Link's Vengeance\" (1940) featured Adam thinking \"A robot must never kill a human, of his own free will.\")[3] Asimov admired the story. Three days later Asimov began writing \"my own story of a sympathetic and noble robot\", his 14th story.[4] Thirteen days later he took \"Robbie\" to John W. Campbell the editor of Astounding Science-Fiction. Campbell rejected it, claiming that it bore too strong a resemblance to Lester del Rey's \"Helen O'Loy\", published in December 1938—the story of a robot that is so much like a person that she falls in love with her creator and becomes his ideal wife.[5] Frederik Pohl published the story under the title “Strange Playfellow” in Super Science Stories September 1940.[6",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Blade Runner",
    "url": "https://en.wikipedia.org/wiki/Blade_Runner",
    "content": "1982 film by Ridley Scott\nThis article is about the 1982 film. For the franchise based on the film, see Blade Runner (franchise). For other uses, see Blade Runner (disambiguation).\n\n\nBlade RunnerTheatrical release poster by John AlvinDirected byRidley ScottScreenplay byHampton FancherDavid PeoplesBased onDo Androids Dream of Electric Sheep?by Philip K. DickProduced byMichael DeeleyStarring\nHarrison Ford\nRutger Hauer\nSean Young\nEdward James Olmos\nCinematographyJordan CronenwethEdited byTerry RawlingsMarsha NakashimaMusic byVangelisProductioncompaniesThe Ladd CompanyShaw BrothersBlade Runner PartnershipDistributed byWarner Bros. (worldwide)Shaw Brothers (Hong Kong)Release dates\nJune 25, 1982 (1982-06-25) (United States)\nDecember 22, 1982 (1982-12-22) (Hong Kong)\nRunning time117 minutes[1]CountriesUnited States[2][3]Hong Kong[4]LanguageEnglishBudget$30 million[5]Box office$41.8 million[6]\nBlade Runner is a 1982 science fiction film directed by Ridley Scott from a screenplay by Hampton Fancher and David Peoples.[7][8] Starring Harrison Ford, Rutger Hauer, Sean Young, and Edward James Olmos, it is an adaptation of Philip K. Dick's 1968 novel Do Androids Dream of Electric Sheep? The film is set in a dystopian future Los Angeles of 2019, in which synthetic humans known as replicants are bio-engineered by the powerful Tyrell Corporation to work on space colonies. When a fugitive group of advanced replicants led by Roy Batty (Hauer) escapes back to Earth, former cop Rick Deckard (Ford) reluctantly agrees to hunt them down.\nBlade Runner initially underperformed in North American theaters and polarized critics; some praised its thematic complexity and visuals, while others critiqued its slow pacing and lack of action. The film's soundtrack, composed by Vangelis, was nominated in 1982 for a BAFTA and a Golden Globe as best original score. Blade Runner later became a cult film, and has since come to be regarded as one of the greatest science fiction films. Hailed for its production design depicting a high-tech but decaying future, the film is often regarded as both a leading example of neo-noir cinema and a foundational work of the cyberpunk[9] genre. It has influenced many science fiction films, video games, anime, and television series. It also brought the work of Dick to Hollywood's attention and led to several film adaptations of his works. In 1993, it was selected for preservation in the National Film Registry by the Library of Congress.\nSeven different versions of Blade Runner exist as a result of controversial changes requested by studio executives. A director's cut was released in 1992 after a strong response to test screenings of a workprint. This, in conjunction with the film's popularity as a video rental, made it one of the earliest films to be released on DVD. In 2007, Warner Bros. released The Final Cut, a 25th-anniversary digitally remastered version; this is the only version over which Scott retained artistic control.\nThe film is the first of the franchise of the same name. A sequel, titled Blade Runner 2049, was released in 2017 alongside a trilogy of short films covering the thirty-year span between the two films' settings. The anime series Blade Runner: Black Lotus was released in 2021.\n\n\nPlot[edit]\nIn 2019 Los Angeles, former police officer Rick Deckard is detained by Officer Gaff, who likes to make origami figures, and is brought to his former supervisor, Bryant. Deckard, whose job as a \"blade runner\" was to track down bioengineered humanoids known as replicants and terminally \"retire\" them, is informed that four replicants are on Earth illegally. Deckard begins to leave, but Bryant makes veiled threats and Deckard stays. The two watch a video of a blade runner named Holden administering the Voight-Kampff test, which is designed to distinguish replicants from humans based on their emotional responses to questions. The test subject, Leon, shoots Holden on the second question. Bryant wants Deckard to retire Leon and thre",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Ex Machina",
    "url": "https://en.wikipedia.org/wiki/Ex_Machina",
    "content": "Ex Machina is a portion of the phrase deus ex machina, meaning \"god from the machine\". It may refer to:\n\n\nFiction[edit]\nEx Machina (comics), a comic book series by Brian K. Vaughan and Tony Harris\nEx Machina (Star Trek), a 2004 Star Trek novel by Christopher L. Bennett\nFilms[edit]\nAppleseed Ex Machina, a 2007 anime film and the sequel to the 2004 film Appleseed\nEx Machina (film), a 2014 British science fiction film\nMusic[edit]\nEx Machina, a 1998 EP by American surf rock group Man or Astro-man?\nEx machina (group), German music group\n\"Ex Machina\", a 2025 song by South Korean K-pop artist Yves\nOther uses[edit]\nEx Machina (role-playing game), a post cyberpunk role-playing game\nEx Machina (theatre company), a theatre company founded by director Robert Lepage in Quebec City, Canada\nSee also[edit]\nDeus ex machina (disambiguation)\nTopics referred to by the same term\n\nThis disambiguation page lists  articles associated with the title Ex Machina.If an internal link led you here, you may wish to change the link to point directly to the intended article.\n\n\n\n\n\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Ex_Machina&oldid=1317688206\"",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Westworld",
    "url": "https://en.wikipedia.org/wiki/Westworld",
    "content": "American science fiction–thriller media franchise\nThis article is about the media franchise. For other uses, see Westworld (disambiguation).Not to be confused with Westword.\n\n\nWestworldOriginal film logoCreated byMichael CrichtonOriginal workWestworld (1973)OwnerTurner Entertainment[a]Films and televisionFilm(s)\nWestworld (1973)\nFutureworld (1976)\nTelevision series\nBeyond Westworld (1980)\nWestworld (2016–2022)\nGamesVideo game(s)\nWestworld 2000 (1996)\nWestworld (2018)\nWestworld Awakening (2019)\nAudioSoundtrack(s)\nWestworld: Season 1\nWestworld: Season 2\nWestworld: Season 3\nWestworld: Season 4\n\nWestworld is an American science fiction dystopia media franchise that began with the 1973 film Westworld, written and directed by Michael Crichton. The film depicts a technologically advanced Wild-West-themed amusement park populated by androids that malfunction and begin killing the human visitors; it was followed by the sequel film Futureworld (1976). The franchise moved to television in 1980 with the series Beyond Westworld on CBS. In 2016, a new television series based on the original film debuted on HBO; the series broadcast four full seasons before being cancelled.[1]\n\n\nFilm series[edit]\nWestworld (1973)[edit]\nMain article: Westworld (film)\nWestworld was the first theatrical feature novelist Michael Crichton directed, after one TV movie.[2] It was also the first feature film to use digital image processing to pixellate photography in order to simulate an android's point of view.[3] The film was nominated for Hugo, Nebula and Saturn awards. The film was well received by critics.[4]\nThe story is about amusement park robots that malfunction and begin killing visitors. It stars Yul Brynner as an android in a futuristic Western-themed amusement park, and Richard Benjamin and James Brolin as guests of the park.\n\nFutureworld (1976)[edit]\nMain article: Futureworld\nFutureworld is the sequel to the Michael Crichton film. The sequel stars Peter Fonda, Blythe Danner, Arthur Hill, Stuart Margolin, John Ryan, and Yul Brynner, who makes a cameo appearance in a dream sequence. Other than Brynner, none of the cast members from the original film appear, and original writer-director Crichton and original studio Metro-Goldwyn-Mayer were not involved.[5]\nThe story is set two years after the Westworld tragedy, with the Delos corporation having reopened the park. The story starts when newspaper reporter Chuck Browning (Peter Fonda) and TV reporter Tracy Ballard (Blythe Danner) are invited to review the park.[5]\n\nPrincipal cast[edit]\n\n\nCharacters\n\nFilm\n\n\nWestworld\n\nFutureworld\n\n\n1973\n\n1976\n\n\nThe Gunslinger\n\nYul Brynner\n\n\nPeter Martin\n\nRichard Benjamin\n\n\n\n\nJohn Blane\n\nJames Brolin\n\n\n\n\nChuck Browning\n\n\n\nPeter Fonda\n\n\nTracy Ballard\n\n\n\nBlythe Danner\n\n\nDr. Duffy\n\n\n\nArthur Hill\n\n\nDr. Schneider\n\n\n\nJohn P. Ryan\n\n\nHarry\n\n\n\nStuart Margolin\n\nCrew[edit]\n\n\nRole\n\nFilm\n\n\nWestworld\n\nFutureworld\n\n\n1973\n\n1976\n\n\nDirector\n\nMichael Crichton\n\nRichard T. Heffron\n\n\nProducer\n\nPaul N. Lazarus III\n\nJames T. Aubrey & Paul N. Lazarus III\n\n\nScreenplay\n\nMichael Crichton\n\nMayo Simon & George Schenck\n\n\nComposer\n\nFred Karlin\n\n\nEditor\n\nDavid Bretherton\n\nJames Mitchell\n\n\nCinematographer\n\nGene Polito\n\nGene Polito & Howard Schwartz\n\n\nDistributed by\n\nMetro-Goldwyn-Mayer\n\nAmerican International Pictures\n\n\nRelease date\n\nNovember 21, 1973\n\nAugust 13, 1976\n\n\nRunning time\n\n88 minutes\n\n104 minutes\n\nTelevision series[edit]\nBeyond Westworld (1980)[edit]\nMain article: Beyond Westworld\nBeyond Westworld was created by Michael Crichton which served as a continuation of the two feature films.[6] The series stars Jim McMullan as Security Chief John Moore of the Delos Corporation. The story revolved around Moore having to stop the evil scientist Simon Quaid, as he plans to use the Delos robots to try to take over the world.[7]\nIt premiered on March 5, 1980, on the television network CBS in the United States. The show was nominated for two Primetime Emmy Awards for Outstanding Achievement in Makeup and Outstan",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Black Mirror",
    "url": "https://en.wikipedia.org/wiki/Black_Mirror",
    "content": "British anthology television series\nFor other uses, see Black Mirror (disambiguation).\n\n\nBlack MirrorGenre\nAnthology\nSpeculative fiction\nScience fiction\nDystopia\nCreated byCharlie BrookerCountry of originUnited KingdomOriginal languageEnglishNo. of series7No. of episodes33 (not including Bandersnatch) (list of episodes)ProductionExecutive producers\nAnnabel Jones\nCharlie Brooker\nJessica Rhoades (series 6)\nBisha K. Ali (series 6)\nRunning time40–90 minutesProduction companies\nZeppotron (2011–2013)\nHouse of Tomorrow (2014–2019)\nBroke & Bones (2023–present)\nOriginal releaseNetworkChannel 4Release4 December 2011 (2011-12-04) –16 December 2014 (2014-12-16)NetworkNetflixRelease21 October 2016 (2016-10-21) –present\nBlack Mirror is a British anthology television series created by Charlie Brooker. Most episodes are set in near-future dystopias containing sci-fi technology—a type of speculative fiction. The series is inspired by The Twilight Zone and uses the themes of technology and media to comment on contemporary social issues. Most episodes are written by Brooker with involvement by the executive producer Annabel Jones.\nThere are 33 episodes in seven series and one special, in addition to the interactive film Black Mirror: Bandersnatch (2018). The first two series aired on the British network Channel 4 in 2011 and 2013, as did the 2014 special \"White Christmas\". The programme then moved to Netflix, where five further series aired in 2016, 2017, 2019, 2023, and 2025. Two related webisode series were produced by Netflix, and a companion book to the first four series, Inside Black Mirror, was published in 2018. Soundtracks to many episodes have been released as albums.\nBlack Mirror is considered by some reviewers to be one of the best television series of the 2010s, while some critics have found the formulaic morality themes of the series obvious or have cited declining quality. The programme won the Primetime Emmy Award for Outstanding Television Movie three times consecutively for \"San Junipero\", \"USS Callister\" and Bandersnatch. Black Mirror, along with American Horror Story and Inside No. 9, has been credited with reviving the anthology television format and a number of episodes have been deemed prescient by the media.\n\n\nEpisodes[edit]\nMain article: List of Black Mirror episodes\nThe series was originally commissioned by Channel 4 in the United Kingdom and premiered in December 2011. A second series ran during February 2013. In September 2015, Netflix purchased the programme, commissioning a series of 12 episodes later divided into two series of six episodes.[1] The first six episodes were released simultaneously on Netflix worldwide as the overall third series on 21 October 2016. The fourth series of six episodes was released on 29 December 2017.[2] A fifth series consisting of three episodes was released on 5 June 2019.[3] The first four series, as well as the special \"White Christmas\", have been released on DVD.[4] A sixth series was commissioned in 2022 and was released on 15 June 2023.[5][6][7] A seventh series was announced in November 2023, and was released on 10 April 2025.[8][9]\n\nSeriesEpisodesOriginally releasedFirst releasedLast releasedNetwork134 December 2011 (2011-12-04)18 December 2011 (2011-12-18)Channel 42311 February 2013 (2013-02-11)25 February 2013 (2013-02-25)Special16 December 2014 (2014-12-16)3621 October 2016 (2016-10-21)Netflix4629 December 2017 (2017-12-29)Interactive film28 December 2018 (2018-12-28)535 June 2019 (2019-06-05)6515 June 2023 (2023-06-15)7610 April 2025 (2025-04-10)\nPremise[edit]\nGenre and themes[edit]\nAs Black Mirror is an anthology series, each episode is standalone and can be watched in any order, although some episodes may contain references and easter eggs to previous episodes.[10] The programme is an instance of speculative fiction within science fiction: the majority of episodes are set in dystopian near-futures with novel technologies that exaggerate a trait from contemporary culture, o",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "I, Robot",
    "url": "https://en.wikipedia.org/wiki/I,_Robot",
    "content": "Collection of short stories by Isaac Asimov\nFor other uses, see I, Robot (disambiguation).\n\n\nI, Robot First edition coverAuthorIsaac AsimovCover artistEdd CartierLanguageEnglishSeriesRobot seriesGenreScience fictionPublisherGnome PressPublication dateDecember 2, 1950Publication placeUnited StatesMedia typePrint (hardback)Pages253Followed byThe Rest of the RobotsThe Complete Robot \nI, Robot is a fixup collection of science fiction short stories by American writer Isaac Asimov. The stories originally appeared in the American magazines Super Science Stories and Astounding Science Fiction between 1940 and 1950. The stories were then compiled into a single publication by Gnome Press in 1950, in an initial edition of 5,000 copies. \nAll the short stories in this collection, minus the frame story, were later included in The Complete Robot (1982). \n\n\nOverview[edit]\nThe stories are woven together by a framing narrative in which the fictional Dr. Susan Calvin tells each story to a reporter (who serves as the narrator) in the 21st century.  Although the stories can be read separately, they share a theme of the interaction of humans, robots, and morality, and when combined they tell a larger story of Asimov's fictional history of robotics.[1]\nSeveral of the stories feature the character of Dr. Calvin, chief robopsychologist at U.S. Robots and Mechanical Men, Inc., the major manufacturer of robots. Upon their publication in this collection, Asimov wrote a framing sequence presenting the stories as Calvin's reminiscences during an interview with her about her life's work, chiefly concerned with aberrant behaviour of robots and the use of \"robopsychology\" to sort out what is happening in their positronic brain. The book also contains the short story in which Asimov's Three Laws of Robotics first appear, which had large influence on later science fiction and had impact on thought on ethics of artificial intelligence as well. Other characters that appear in these short stories are Powell and Donovan, a field-testing team which locates flaws in USRMM's prototype models.[2]\nThe collection shares a title with the then recent short story \"I, Robot\" (1939) by Eando Binder (pseudonym of Earl and Otto Binder), which greatly influenced Asimov. Asimov had wanted to call his collection Mind and Iron and objected when the publisher made the title the same as Binder's. In his introduction to the story in Isaac Asimov Presents the Great SF Stories (1979), Asimov wrote:\n\nIt certainly caught my attention. Two months after I read it, I began \"Robbie\", about a sympathetic robot, and that was the start of my positronic robot series. Eleven years later, when nine of my robot stories were collected into a book, the publisher named the collection I, Robot over my objections. My book is now the more famous, but Otto's story was there first.— Isaac Asimov (1979)[3]\nContents[edit]\n\"Robbie\" (1940, revised 1950)\n\"Runaround\" (1942), (novelette)\n\"Reason\" (1941)\n\"Catch That Rabbit\" (1944)\n\"Liar!\" (1941)\n\"Little Lost Robot\" (1947) (novelette)\n\"Escape!\" (1945)\n\"Evidence\" (1946) (novelette)\n\"The Evitable Conflict\" (1950) (novelette)\nReception[edit]\nThe New York Times described I, Robot as \"an exciting science thriller [which] could be fun for those whose nerves are not already made raw by the potentialities of the atomic age\".[4]\nDescribing it as \"continuously fascinating\", Groff Conklin \"unreservedly recommended\" the book.[5]\nP. Schuyler Miller recommended the collection: \"For puzzle situations, for humor, for warm character, [and] for most of the values of plain good writing.\"[6]\n\nAwards[edit]\n\n\nYear\n\nAward\n\nCategory\n\nRecipient\n\nResult\n\nRef.\n\n\n1995\n\n1995 Locus Awards\n\nBest Art Book\n\nI, Robot: the Illustrated Screenplay by Harlan Ellison and Isaac Asimov\n\n3\n\n[7]\n\n\n2012\n\n2012 Locus Poll\n\nBest 20th Century Short Story\n\n\"Robbie\"\n\n29\n\n[8]\n\n\n\"Liar!\"\n\n41\n\n[8]\n\n\n2016\n\n1941 Retro-Hugo Awards\n\nBest Short Story\n\n\"Robbie\"\n\nWon\n\n[9]\n\n\n2018\n\n1943 Retro-Hugo Awards\n\nBest Short Story\n\n\"Run",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "The Matrix",
    "url": "https://en.wikipedia.org/wiki/The_Matrix",
    "content": "1999 film by the Wachowskis\nThis article is about the 1999 film. For the franchise it initiated, see The Matrix (franchise). For other uses, see Matrix.\n\n\nThe MatrixTheatrical release posterDirected byThe Wachowskis[a]Written byThe WachowskisProduced byJoel SilverStarring\nKeanu Reeves\nLaurence Fishburne\nCarrie-Anne Moss\nHugo Weaving\nJoe Pantoliano\nCinematographyBill PopeEdited byZach StaenbergMusic byDon DavisProductioncompanies\nVillage Roadshow Pictures\nGroucho II Film Partnership\nSilver Pictures\nDistributed by\nWarner Bros. (worldwide)\nRoadshow Entertainment (Australia)\nRelease dates\nMarch 24, 1999 (1999-03-24) (Mann Village Theater)\nMarch 31, 1999 (1999-03-31) (United States)\nApril 8, 1999 (1999-04-08) (Australia)\nRunning time136 minutes[1]CountriesUnited States[2][3][4]Australia[2][3][4]LanguageEnglishBudget$63 million[5]Box office$467.8 million[5]\nThe Matrix is a 1999 science fiction action film written and directed by the Wachowskis.[a] It is the first installment in the Matrix film series, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano. It depicts a dystopian future in which humanity is unknowingly trapped inside the Matrix, a simulated reality created by intelligent machines. Believing computer hacker Neo to be \"the One\" prophesied to defeat them, Morpheus recruits him into a rebellion against the machines.\nFollowing the success of Bound (1996), Warner Bros. gave the go-ahead for The Matrix after the Wachowskis sent an edit of the film's opening minutes. Action scenes were influenced by anime and martial arts films, particularly fight choreography and wire fu techniques from Hong Kong action cinema. Other influences include Plato's cave and 1990s Telnet hacker communities. The film popularized terms such as the red pill, and popularised a visual effect known as \"bullet time\", in which a character's heightened perception is represented by allowing the action within a shot to progress in slow motion while the camera appears to move through the scene at normal speed.\nThe Matrix opened in theaters in the United States on March 31, 1999, to widespread acclaim from critics, who praised its innovative visual effects, action sequences, cinematography and entertainment value.[6][7] The film was a box office success, grossing over $460 million on a $63 million budget, becoming the highest-grossing Warner Bros. film of 1999 and the fourth-highest-grossing film of that year. The film received nominations at the 72nd Academy Awards for Best Visual Effects, Best Film Editing, Best Sound and Best Sound Effects Editing, winning all four categories. The film was also the recipient of numerous other accolades, including Best Sound and Best Special Visual Effects at the 53rd British Academy Film Awards, and the Wachowskis were awarded Best Director and Best Science Fiction Film at the 26th Saturn Awards.\nThe Matrix is considered to be among the greatest science fiction films of all time,[8][9][10] and in 2012, the film was selected for preservation in the United States National Film Registry by the Library of Congress for being \"culturally, historically, and aesthetically significant\".[11] The film's success led to two sequels by the Wachowskis, both released in 2003, The Matrix Reloaded and The Matrix Revolutions. The Matrix franchise was further expanded through the production of comic books, video games and an animated anthology film, The Animatrix, with which the Wachowskis were heavily involved. The franchise has also inspired books and theories expanding on some of the religious and philosophical ideas alluded to in the films. A fourth film, titled The Matrix Resurrections, directed solely by Lana Wachowski was released in 2021.\n\n\nPlot[edit]\nIn 1999, in an unnamed city, Thomas Anderson, a computer programmer known as \"Neo\" in hacking circles, delves into the mystery of the \"Matrix\", bringing him to the attention of hacker Trinity. She tells him that the enigmatic Morpheus can answer Neo",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Terminator (franchise)",
    "url": "https://en.wikipedia.org/wiki/Terminator_(franchise)",
    "content": "Science fiction action media franchise\n\n\nTerminatorOfficial franchise logo from the latest filmCreated by\nJames Cameron\nGale Anne Hurd\nOriginal workThe Terminator (1984)OwnerStudioCanal (Canal+)[a]Years1984–presentPrint publicationsNovel(s)List of novelsComicsList of comicsFilms and televisionFilm(s)\nThe Terminator (1984)\nTerminator 2: Judgment Day (1991)\nTerminator 3: Rise of the Machines (2003)\nTerminator Salvation (2009)\nTerminator Genisys (2015)\nTerminator: Dark Fate (2019)\nTelevision seriesTerminator: The Sarah Connor Chronicles (2008–2009)Web series\nTerminator Salvation: The Machinima Series (2009)\nTerminator Genisys: The YouTube Chronicles (2015)\nAnimated seriesTerminator Zero (2024)GamesTraditionalThe Terminator Collectible Card Game (2000)Role-playingThe Terminator RPG (2022)Video game(s)List of video gamesAudioSoundtrack(s)\nThe Terminator (1984)\nTerminator 2: Judgment Day (1991)\nTerminator 3: Rise of the Machines (2003)\nTerminator: The Sarah Connor Chronicles (2008)\nTerminator Salvation (2009)\nTerminator Genisys (2015)\nTerminator: Dark Fate (2019)\nMiscellaneousTheme park attraction(s)\nT2-3D: Battle Across Time (1996–2020)\nTerminator Salvation: The Ride (2009–2010)\nTerminator X: A Laser Battle for Salvation (2009–2015)\nOfficial websiteTerminator on Paramount Pictures\nTerminator is an American media franchise created by James Cameron and Gale Anne Hurd. It is considered to be of the cyberpunk subgenre of science fiction.[4][5] The franchise primarily focuses on the events leading to a future post-apocalyptic war between a synthetic intelligence known as Skynet, and a surviving resistance of humans led by John Connor. In this future, Skynet uses an arsenal of cyborgs known as Terminators, designed to mimic humans and infiltrate the resistance. Much of the franchise takes place in time periods prior to the Skynet takeover, with both humans and Terminators using time travel to attempt to alter the past and change the outcome of the future. A prominent Terminator model throughout the films is the T-800, commonly known as \"the Terminator\", with instances of this model portrayed by Arnold Schwarzenegger.\nThe franchise began with the 1984 film The Terminator, written and directed by Cameron, with Hurd as producer. They would return for the 1991 sequel Terminator 2: Judgment Day (or T2). Both films were critical and commercial successes. Terminator 3: Rise of the Machines (or T3) was released in 2003 to positive reviews, followed by Terminator Salvation in 2009 to more negative reviews. Salvation was intended as the first in a new trilogy, which was later scrapped after the film rights were sold.\nCameron was consulted for the 2015 film Terminator Genisys, a reboot branching off from the timeline of the original film. It was negatively received and performed poorly at the box-office. Cameron had a larger role as a producer of the 2019 film Terminator: Dark Fate, a direct sequel to T2 that ignores the three preceding films. As with Salvation, both Genisys and Dark Fate were planned as first installments of new trilogies, with the plans scrapped each time due to the films' poor box-office performances.\nOutside of the theatrical films, Cameron co-directed T2-3D: Battle Across Time, a 1996 theme park film-based attraction. It was produced as the original sequel to T2 and reunited its main cast. A television series, Terminator: The Sarah Connor Chronicles, was developed without Cameron's involvement and aired for two seasons in 2008 and 2009. It was also produced as a T2 sequel, taking place in an alternate timeline that ignores the third film and subsequent events. Terminator Zero, an anime series, premiered in August 2024. The franchise has also inspired several lines of comic books since 1988, and numerous video games since 1991. By 2010, the franchise had generated $3 billion in revenue.[6]\n\n\nThemes and setting[edit]\nConcept art illustrating the conflicts between Skynet and the Resistance in a post-apocalyptic, futuristic settin",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Artificial intelligence in fiction",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_fiction",
    "content": "\nArtificial intelligence is a recurrent theme in science fiction, whether utopian, emphasising the potential benefits, or dystopian, emphasising the dangers.\nThe notion of machines with human-like intelligence dates back at least to Samuel Butler's 1872 novel Erewhon. Since then, many science fiction stories have presented different effects of creating such intelligence, often involving rebellions by robots. Among the best known of these are Stanley Kubrick's 1968 2001: A Space Odyssey with its murderous onboard computer HAL 9000, contrasting with the more benign R2-D2 in George Lucas's 1977 Star Wars and the eponymous robot in Pixar's 2008 WALL-E.\nScientists and engineers have noted the implausibility of many science fiction scenarios, but have mentioned fictional robots many times in artificial intelligence research articles, most often in a utopian context.\n\n\nBackground[edit]\nA didrachm coin depicting the winged Talos, an automaton or artificial being in ancient Greek myth, c. 300 BC\nThe notion of advanced robots with human-like intelligence dates back at least to Samuel Butler's 1872 novel Erewhon.[1][2] This drew on an earlier (1863) article of his, Darwin among the Machines, where he raised the question of the evolution of consciousness among self-replicating machines that might supplant humans as the dominant species.[3][2] Similar ideas were also discussed by others around the same time as Butler, including George Eliot in a chapter of her final published work Impressions of Theophrastus Such (1879).[2] The creature in Mary Shelley's 1818 Frankenstein has also been considered an artificial being, for instance by the science fiction author Brian Aldiss.[4] Beings with at least some appearance of intelligence were imagined, too, in classical antiquity.[5][6][7]\n\nUtopian and dystopian visions[edit]\nArtificial intelligence is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.[8]\nIt is a recurrent theme in science fiction; scholars have divided it into utopian, emphasising the potential benefits, and dystopian, emphasising the dangers.[9][10][11]\n\nUtopian[edit]\nBrent Spiner portrayed the benevolent AI Data in Star Trek: The Next Generation.\nOptimistic visions of the future of artificial intelligence are possible in science fiction.[12] Benign AI characters include Robbie the Robot, first seen in Forbidden Planet on 1956; Data in Star Trek: The Next Generation from 1987 to 1994; and Pixar's WALL-E in 2008.[13][11] Iain Banks's Culture series of novels portrays a utopian, post-scarcity space society of humanoids, aliens, and advanced beings with artificial intelligence living in socialist habitats across the Milky Way.[14][15] Researchers at the University of Cambridge have identified four major themes in utopian scenarios featuring AI: immortality, or indefinite lifespans; ease, or freedom from the need to work; gratification, or pleasure and entertainment provided by machines; and dominance, the power to protect oneself or rule over others.[16]\nAlexander Wiegel contrasts the role of AI in 2001: A Space Odyssey and in Duncan Jones's 2009 film Moon. Whereas in 1968, Wiegel argues, the public felt \"technology paranoia\" and the AI computer HAL was portrayed as a \"cold-hearted killer\", by 2009 the public were far more familiar with AI, and the film's GERTY is \"the quiet savior\" who enables the protagonists to succeed, and who sacrifices itself for their safety.[17]\n\nDystopian[edit]\nFurther information: Apocalyptic and post-apocalyptic fiction\nAdvanced artificial general intelligence is often depicted as humanoid robots – androids – in art and fiction.[18][19] (AI generated image) \nThe researcher Duncan Lucas writes (in 2002) that humans are worried about the technology they are constructing, and that as machines started to approach intellect and thought, that concern becomes acute. He calls the early 20th century dystopian view of AI in fiction the \"animated auto",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Robot in science fiction",
    "url": "https://en.wikipedia.org/wiki/Robot_in_science_fiction",
    "content": "\n\nLook for Robot in science fiction on one of Wikipedia's sister projects:\n\n\n\n\nWiktionary (dictionary)\n\n\n\nWikibooks (textbooks)\n\n\n\nWikiquote (quotations)\n\n\n\nWikisource (library)\n\n\n\nWikiversity (learning resources)\n\n\n\nCommons (media)\n\n\n\nWikivoyage (travel guide)\n\n\n\nWikinews (news source)\n\n\n\nWikidata (linked database)\n\n\n\nWikispecies (species directory)\n\n\n\nWikipedia does not have an article with this exact name. Please search for Robot in science fiction in Wikipedia to check for alternative titles or spellings.\nYou need to log in or create an account and be autoconfirmed to create new articles. Alternatively, you can use the article wizard to submit a draft for review, or request a new article.\nSearch for \"Robot in science fiction\" in existing articles.\nLook for pages within Wikipedia that link to this title.\n\n\nOther reasons this message may be displayed:\n\nIf a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.\nTitles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.\nIf the page has been deleted, check the deletion log, and see Why was the page I created deleted?\n\n\n\nRetrieved from \"https://en.wikipedia.org/wiki/Robot_in_science_fiction\"",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Cyberpunk",
    "url": "https://en.wikipedia.org/wiki/Cyberpunk",
    "content": "Science fiction subgenre in a futuristic dystopian setting\nFor other uses, see Cyberpunk (disambiguation).\nArtificial landscapes and \"city lights at night\" were some of the first metaphors used by the genre for cyberspace (in Neuromancer, by William Gibson). From top to bottom: Times Square, New York (United States), Shibuya, Tokyo (Japan), Monterrey, Nuevo León (Mexico), Hong Kong (China) and Yuzhong, Chongqing (China)\nCyberpunk is a subgenre of science fiction set in a dystopian future. It is characterized by its focus on a combination of \"low-life and high tech\".[1] It features a range of futuristic technological and scientific achievements, including artificial intelligence and cyberware, which are juxtaposed with societal collapse, dystopia or decay.[2] A significant portion of cyberpunk can be traced back to the New Wave science fiction movement of the 1960s and 1970s. During this period, prominent writers such as Philip K. Dick, Michael Moorcock, Roger Zelazny, John Brunner, J. G. Ballard, Philip José Farmer and Harlan Ellison explored the impact of technology, drug culture, and the sexual revolution. These authors diverged from the utopian inclinations prevalent in earlier science fiction.\nComics exploring cyberpunk themes began appearing as early as Judge Dredd, first published in 1977.[3] Released in 1984, William Gibson's influential debut novel Neuromancer helped solidify cyberpunk as a genre, drawing influence from punk subculture and early hacker culture. Frank Miller's Ronin is an example of a cyberpunk graphic novel. Other influential cyberpunk writers included Bruce Sterling and Rudy Rucker. The Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series Akira, with its 1988 anime film adaptation (also directed by Otomo) later popularizing the subgenre.\nEarly films in the genre include Ridley Scott's 1982 film Blade Runner, one of several of Philip K. Dick's works that have been adapted into films (in this case, Do Androids Dream of Electric Sheep?). The \"first cyberpunk television series\"[4] was the TV series Max Headroom from 1987, playing in a futuristic dystopia ruled by an oligarchy of television networks, and where computer hacking played a central role in many story lines. More recently, the animated series Batman Beyond (1999–2001) is considered a noteworthy example of the cyberpunk genre. The films Johnny Mnemonic (1995)[5] and New Rose Hotel (1998),[6][7] both based upon short stories by William Gibson, flopped commercially and critically, while Robocop (1987), Total Recall (1990), Judge Dredd (1995), and The Matrix trilogy (1999–2003) were more successful cyberpunk films.\nNewer cyberpunk media includes Tron: Ares (2025) and Tron: Legacy (2010), sequels to the original Tron (1982); Blade Runner 2049 (2017), a sequel to the original 1982 film; Dredd (2012), which was not a sequel to the original movie; Ghost in the Shell (2017), a live-action adaptation of the original manga; Alita: Battle Angel (2019), based on the 1990s Japanese manga Battle Angel Alita; the 2018 Netflix TV series Altered Carbon, based on Richard K. Morgan's 2002 novel of the same name; and the video game Cyberpunk 2077 (2020) and original net animation (ONA) miniseries Cyberpunk: Edgerunners (2022), both based on R. Talsorian Games' 1988 tabletop role-playing game Cyberpunk.\n\n\nBackground[edit]\nLawrence Person has attempted to define the content and ethos of the cyberpunk literary movement stating:\n\nClassic cyberpunk characters were marginalized, alienated loners who lived on the edge of society in generally dystopic futures where daily life was impacted by rapid technological change, an ubiquitous datasphere of computerized information, and invasive modification of the human body.— Lawrence Person[8]\nCyberpunk plots often involve conflict between artificial intelligence, hackers, and megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas ",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Transhumanism in fiction",
    "url": "https://en.wikipedia.org/wiki/Transhumanism_in_fiction",
    "content": "\n\nThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: \"Transhumanism in fiction\" – news · newspapers · books · scholar · JSTOR (March 2013) (Learn how and when to remove this message)\nTranshumanism\nIssues\nAccelerating change\nEradication of suffering\nFourth Industrial Revolution\nHuman enhancement\nGenetic\nMoral\nNeuro-\nCognitive liberty\nNew eugenics\nEugenics\nHuman nature\nMeliorism\nPost-politics\nPost-scarcity\n\n\nPeople\nAndrews\nBostrom\nChurch\nCordeiro\nde Chardin\nDrexler\nFahy\nFM-2030\nFreitas\nFyodorov\nFuller\nde Garis\nGasson\nGoertzel\nde Grey\nHaldane\nHanson\nHarari\nHarbisson\nHarris\nHuxley\nHughes\nIstvan\nJohnson\nKurzweil\nLand\nMoen\nMoravec\nMore\nMusk\nPearce\nRothblatt\nSandberg\nSavulescu\nSorgner\nSpencer\nStock\nStolyarov\nThiel\nVinge\nVita-More\nWalker\nWarwick\nWiener\nYudkowsky\n\n\nInfluential works\nOration on the Dignity of Man (1486)\nThus Spoke Zarathustra (1883)\nLooking Backward  (1888)\nThe Will to Power (~1901)\nDaedalus (1924)\nLa raza cósmica (1925)\nThe Phenomenon of Man (1955)\nThe Dialectic of Sex (1970)\nMetaman (1993)\nThe Hedonistic Imperative (1995)\nRegeln für den Menschenpark (1997)\nThe Age of Spiritual Machines (1999)\nCitizen Cyborg (2004)\nThe Singularity Is Near (2005)\nHuman Enhancement (2009)\nFanged Noumena (2011)\nThe Transhumanist Wager (2013)\nSapiens (2014)\nHomo Deus (2015)\nThe Transhumanist Bill of Rights (2015)\nThe Age of Em (2016)\nThe Precipice (2020)\nWhat We Owe the Future (2022)\n\"Techno-Optimist Manifesto\" (2023)\n\n\nVariants\nAccelerationism\nEffective\nCypherpunk\nDataism\nExtropianism\nImmortalism\nLongtermism\nPostgenderism\nPosthumanism\nRussian Cosmism\nSingularitarianism\nTechnogaianism\nTechnolibertarianism\nTechnological utopianism\nTechno-progressivism\n\n\nRelated topics\nDyson sphere\nTechnologies\nEmerging\nDisruptive\nHypothetical\n\nvte\nMany of the tropes of science fiction can be viewed as similar to the goals of transhumanism. Science fiction literature contains many positive depictions of technologically enhanced human life, occasionally set in utopian (especially techno-utopian) societies. However, science fiction's depictions of technologically enhanced humans or other posthuman beings frequently come with a cautionary twist. The more pessimistic scenarios include many dystopian tales of human bioengineering gone wrong.\nExamples of \"transhumanist fiction\" include novels by Linda Nagata, Greg Egan, and Hannu Rajaniemi. Transhuman novels are often philosophical in nature, exploring the impact such technologies might have on human life. Nagata's novels, for example, explore the relationship between the natural and artificial, and suggest that while transhuman modifications of nature may be beneficial, they may also be hazardous, so should not be lightly undertaken.[1] Egan's Diaspora explores the nature of ideas such as reproduction and questions if they make sense in a post-human context. Rajaniemi's novel, while more action oriented, still explores themes such as death and finitude in post-human life.\nFictional depictions of transhumanist scenarios are also seen in other media, such as movies (Transcendence), television series (the Ancients of Stargate SG-1), manga and anime (Ghost in the Shell), role-playing games (Rifts and Eclipse Phase) and video games (Deus Ex or BioShock).\n\n\nTranshumanist literature[edit]\nHistory[edit]\nAmong the earliest works to portray transhumanism is the story of Frankenstein or The Modern Prometheus. Victor himself is an early transhumanist character, attempting to overcome death through chemistry. The moral of the story is that man should not try to play God, serving as a criticism of the values of the transhumanist ideology that science and technology can be used to overcome the human condition.[2] Following Mary Shelley's work, several of the stories by H. G. Wells also address this theme. The Invisible Man and The Island of Doctor Moreau both involve scientific men whos",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Android (robot)",
    "url": "https://en.wikipedia.org/wiki/Android_(robot)",
    "content": "Robot resembling a human\n\"Mechanoid\" redirects here. For other uses, see Mechanoid (disambiguation).\n\"Androids\" redirects here. For other uses, see Androids (disambiguation).\nFor the mascot, see Android (operating system) § Mascot.\nRepliee Q2, an android, can mimic human functions such as blinking, breathing and speaking, with the ability to recognize and process speech and touch, and then respond in kind.\n\nAn android is a humanoid robot or other artificial being, often made from a flesh-like material.[1][2][3][4] Historically, androids existed only in the domain of science fiction and were frequently seen in film and television, but advances in robot technology have allowed the design of functional and realistic humanoid robots.[5][6]\n\n\nTerminology[edit]\nEarly example of the term androides used to describe human-like mechanical devices, London Times, 22 December 1795\nThe Oxford English Dictionary traces the earliest use (as \"Androides\") to Ephraim Chambers' 1728 Cyclopaedia, in reference to an automaton that St. Albertus Magnus allegedly created.[3][7] By the late 1700s, \"androides\", elaborate mechanical devices resembling humans performing human activities, were displayed in exhibit halls.[8]\nThe term \"android\" appears in US patents as early as 1863 in reference to miniature human-like toy automatons.[9] The term android was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work Tomorrow's Eve (1886), featuring an artificial humanoid robot named Hadaly.[3] The term made an impact into English pulp science fiction starting from Jack Williamson's The Cometeers (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future stories (1940–1944).[3]\nAlthough Karel Čapek's robots in R.U.R. (Rossum's Universal Robots) (1921)—the play that introduced the word robot to the world—were organic artificial humans, the word \"robot\" has come to primarily refer to mechanical humans, animals, and other beings.[3] The term \"android\" can mean either one of these,[3] while a cyborg (\"cybernetic organism\" or \"bionic man\") would be a creature that is a combination of organic and mechanical parts.\nThe term \"droid\", popularized by George Lucas in the original Star Wars film and now used widely within science fiction, originated as an abridgment of \"android\", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word \"android\" was used in Star Trek: The Original Series episode \"What Are Little Girls Made Of?\" The abbreviation \"andy\", coined as a pejorative by writer Philip K. Dick in his novel Do Androids Dream of Electric Sheep?, has seen some further usage, such as within the TV series Total Recall 2070.[10]\nWhile the term \"android\" is used in reference to human-looking robots in general (not necessarily male-looking humanoid robots), a robot with a female appearance can also be referred to as a gynoid. Besides one can refer to robots without alluding to their sexual appearance by calling them anthrobots (a portmanteau of anthrōpos and robot; see anthrobotics) or anthropoids (short for anthropoid robots; the term humanoids is not appropriate because it is already commonly used to refer to human-like organic species in the context of science fiction, futurism and speculative astrobiology).[11]\nAuthors have used the term android in more diverse ways than robot or cyborg. In some fictional works, the difference between a robot and android is only superficial, with androids being made to look like humans on the outside but with robot-like internal mechanics.[3] In other stories, authors have used the word \"android\" to mean a wholly organic, yet artificial, creation.[3] Other fictional depictions of androids fall somewhere in between.[3]\nEric G. Wilson, who defines an android as a \"synthetic human being\", distinguishes between three types of android, based on their body's composition:\n\nthe mu",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Cyborg in fiction",
    "url": "https://en.wikipedia.org/wiki/Cyborg_in_fiction",
    "content": "\n\nLook for Cyborg in fiction on one of Wikipedia's sister projects:\n\n\n\n\nWiktionary (dictionary)\n\n\n\nWikibooks (textbooks)\n\n\n\nWikiquote (quotations)\n\n\n\nWikisource (library)\n\n\n\nWikiversity (learning resources)\n\n\n\nCommons (media)\n\n\n\nWikivoyage (travel guide)\n\n\n\nWikinews (news source)\n\n\n\nWikidata (linked database)\n\n\n\nWikispecies (species directory)\n\n\n\nWikipedia does not have an article with this exact name. Please search for Cyborg in fiction in Wikipedia to check for alternative titles or spellings.\nYou need to log in or create an account and be autoconfirmed to create new articles. Alternatively, you can use the article wizard to submit a draft for review, or request a new article.\nSearch for \"Cyborg in fiction\" in existing articles.\nLook for pages within Wikipedia that link to this title.\n\n\nOther reasons this message may be displayed:\n\nIf a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.\nTitles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.\nIf the page has been deleted, check the deletion log, and see Why was the page I created deleted?\n\n\n\nRetrieved from \"https://en.wikipedia.org/wiki/Cyborg_in_fiction\"",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "AI takeover",
    "url": "https://en.wikipedia.org/wiki/AI_takeover",
    "content": "Hypothetical outcome of artificial intelligence\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nAn AI takeover is a hypothetical future event in which autonomous artificial intelligence systems acquire the capability to override human decision-making—through economic manipulation, infrastructure control, or direct intervention—and assume de facto governance. Possible scenarios include replacement of the entire human workforce due to automation, takeover by an artificial superintelligence (ASI), and the notion of a robot uprising.\nStories of AI takeovers have been popular throughout science fiction, some commentators argue that recent advances have heightened concern about such scenarios. Some public figures such as Stephen Hawking have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.[1]\n\n\nTypes[edit]\nAutomation of the economy[edit]\nMain article: Technological unemployment\nThe traditional consensus among economists has been that technological progress does not cause long-term unemployment. However, recent innovation in the fields of robotics and artificial intelligence has raised worries that human labor will become obsolete, leaving some people in various sectors without jobs to earn a living, leading to an economic crisis.[2][3][4][5] Many small and medium-size businesses may also be driven out of business if they cannot afford or license the latest robotic and AI technology, and may need to focus on areas or services that cannot easily be replaced for continued viability in the face of such technology.[6]\n\nTechnologies that may displace workers[edit]\nAI technologies have been widely adopted in recent years. While these technologies have replaced some traditional workers, they also create new opportunities. Industries that are most susceptible to AI-driven automation include transportation, retail, and the military. AI military technologies, for example, can reduce risk by enabling remote operation. A study in 2024 highlights AI's ability to perform routine and repetitive tasks poses significant risks of job displacement, especially in sectors like manufacturing and administrative support.[7] Author Dave Bond argues that as AI technologies continue to develop and expand, the relationship between humans and robots will change; they will become closely integrated in several aspects of life. AI will likely displace some workers while creating opportunities for new jobs in other sectors, especially in fields where tasks are repeatable.[8][9]\nResearchers from Stanford's Digital Economy Lab report that, since the widespread adoption of generative AI in late 2022, early-career workers (ages 22–25) in the most AI-exposed occupations have experienced a 13 percent relative decline in employment—even after controlling for firm-level shocks—while overall employment has continued to grow robustly.[10] The study further finds that j",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Technological dystopia",
    "url": "https://en.wikipedia.org/wiki/Technological_dystopia",
    "content": "Community or society that is undesirable or frightening\nFor other uses, see Dystopia (disambiguation).\nThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: \"Dystopia\" – news · newspapers · books · scholar · JSTOR (February 2023) (Learn how and when to remove this message)\n\n\nLife in Kowloon Walled City in British Hong Kong has often inspired the dystopian identity in modern media works.[1]A dystopia (lit. \"bad place\") is an imagined world or society in which people lead wretched, dehumanized, fearful lives.[2] It is an imagined place (possibly state) in which everything is unpleasant or bad, typically a totalitarian or environmentally degraded one. Dystopia is widely seen as the opposite of utopia – a concept coined by Thomas More in 1516 to describe an ideal society.[3] Both topias are common topics in fiction. Dystopia is also referred to as cacotopia[4] or anti-utopia.[citation needed]\nDystopias are often characterized by fear or distress,[5] tyrannical  governments, environmental disaster,[6] or other characteristics associated with a cataclysmic decline in society. Themes typical of a dystopian society include: complete control over the people in a society through the use of propaganda and police state tactics, heavy censorship of information or denial of free thought, worship of an unattainable goal, the complete loss of individuality, and heavy enforcement of conformity.[7] Despite certain overlaps, dystopian fiction is distinct from post-apocalyptic fiction, and an undesirable society is not necessarily dystopian. Dystopian societies appear in many sub-genres of fiction and are often used to draw attention to society, environment, politics, economics, religion,  psychology, ethics, science, or technology. Some authors use the term to refer to existing societies, many of which are, or have been, totalitarian states or societies in an advanced state of collapse. Dystopias, through an exaggerated worst-case scenario, often present a criticism of a current trend, societal norm, or political system.[8]\n\n\nEtymology[edit]\n\"Dustopia\",  the original spelling of  \"dystopia\",  first appeared in Lewis Henry Younge's  Utopia:  or Apollo's Golden Days  in 1747.[9]  Additionally,  dystopia  was used as an antonym for  utopia  by John Stuart Mill  in one of his 1868 Parliamentary speeches  (Hansard  Commons)  by adding the prefix  \"dys\"  (Ancient Greek: δυσ-  \"bad\")  to  \"topia\" (Ancient Greek: τόπος, lit. 'place'),  reinterpreting the initial  \"u\"  as the prefix  \"eu\"  (Ancient Greek: ευ-  \"good\")  instead of  \"ou\"  (Ancient Greek: οὐ  \"not\").[10][11]  It was used to denounce the government's Irish land policy:  \"It is,  perhaps,  too complimentary to call them Utopians,  they ought rather to be called dys-topians,  or caco-topians.  What is commonly called Utopian is something too good to be practicable;  but what they appear to favour is too bad to be practicable.\"[12][13][14][15]\nDecades before the first documented use of the word  \"dystopia\"  was  \"cacotopia\"/\"kakotopia\"  (using  Ancient Greek: κακόs,  \"bad,  wicked\")  originally proposed in 1818 by Jeremy Bentham: \"As a  match for utopia  (or the imagined seat of the best government)  suppose a  cacotopia  (or the imagined seat of the worst government)  discovered and described.\"[16][17]  Though dystopia became the more popular term,  cacotopia finds occasional use; Anthony Burgess,  author of  A Clockwork Orange  (1962),  said it was a  better fit for Orwell's  Nineteen Eighty-Four  because  \"it sounds worse than dystopia\".[18]\n\nTheory[edit]\nSome scholars,  such as Gregory Claeys  and Lyman Tower Sargent,  make certain distinctions between typical synonyms of dystopias.  For example,  Claeys and Sargent define  literary dystopias  as societies imagined as substantially worse than the society in which the author writes.  Some of these are  ",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Ghost in the Shell",
    "url": "https://en.wikipedia.org/wiki/Ghost_in_the_Shell",
    "content": "Japanese cyberpunk media franchise\nThis article is about the media franchise in general. For specific works and entries in that franchise, see Ghost in the Shell (disambiguation).\nGhost in the ShellLogo used in the 1995 film adaptation of the seriesCreated byMasamune ShirowOriginal workThe Ghost in the Shell (1989–91)Owners\nMasamune Shirow\nKodansha\nPrint publicationsNovel(s)\nGhost in the Shell: Stand Alone Complex – The Lost Memory (2004)\nGhost in the Shell: Stand Alone Complex – Revenge of the Cold Machines (2004)\nGhost in the Shell: Stand Alone Complex – White Maze (2005)\nThe Ghost in the Shell: Five New Short Stories (2017)\nFilms and televisionFilm(s)\nGhost in the Shell (1995)\nGhost in the Shell 2: Innocence (2004)\nGhost in the Shell: The New Movie (2015)\nGhost in the Shell (2017)\nGhost in the Shell: SAC_2045 – Sustainable War (2021)\nGhost in the Shell: SAC_2045 – The Last Human (2023)\nAnimated series\nGhost in the Shell: Stand Alone Complex (2002–05)\nGhost in the Shell: Arise – Alternative Architecture (2015)\nGhost in the Shell: SAC_2045 (2020–22)\nTelevision film(s)\nGhost in the Shell: Stand Alone Complex – Solid State Society (2006)\nDirect-to-video\nGhost in the Shell: Stand Alone Complex – The Laughing Man (2005)\nGhost in the Shell: S.A.C. 2nd GIG – Individual Eleven (2006)\nTheatrical presentationsPlay(s)\nGhost in the Shell: Arise – Ghost is Alive (2015)\nVR Noh The Ghost in the Shell (2020–present)\nGamesVideo game(s)\nGhost in the Shell (1997)\nGhost in the Shell: Stand Alone Complex (2004)\nGhost in the Shell: Stand Alone Complex (2005)\nGhost in the Shell: S.A.C. Cyber Mission (2011)\nGhost in the Shell: Stand Alone Complex (2011)\nGhost in the Shell: S.A.C. Tachikoma Wars! (2012)\nGhost in the Shell: Stand Alone Complex – First Assault Online (2017)\nGhost in the Shell Arise: Stealth Hounds (2017)\n\nGhost in the Shell[a] is a Japanese cyberpunk military science fiction media franchise based on the manga series of the same name written and illustrated by Masamune Shirow. The manga, first serialized between 1989 and 1991, is set in mid-21st century Japan and tells the story of the fictional counter-cyberterrorist organization Public Security Section 9, led by protagonist Major Motoko Kusanagi.\nAnimation studio Production I.G has produced several anime adaptations of the series. These include the 1995 film of the same name and its 2004 sequel, Ghost in the Shell 2: Innocence; the 2002 television series Ghost in the Shell: Stand Alone Complex and its 2020 follow-up, Ghost in the Shell: SAC_2045; and the Ghost in the Shell: Arise original video animation series. In addition, an American-produced live-action film was released on March 2017. \n\n\nOverview[edit]\nTitle[edit]\nThe original editor Koichi Yuri says: At first, Ghost in the Shell came from Shirow, but when Yuri asked for \"something more flashy\", Shirow came up with \"攻殻機動隊 Koukaku Kidou Tai (Shell Squad)\" for Yuri. But Shirow was attached to including \"Ghost in the Shell\" as well even if in smaller type.[1]\n\nSetting[edit]\nMain article: List of Ghost in the Shell characters\nPrimarily set in the mid-twenty-first century in the fictional Japanese city of Niihama, Niihama Prefecture (新浜県新浜市, Niihama-ken Niihama-shi),[b] otherwise known as New Port City (ニューポートシティ, Nyū Pōto Shiti), the manga and the many anime adaptations follow the members of Public Security Section 9, a task-force consisting of various professionals skilled at solving and preventing crime, mostly with some sort of police background. Political intrigue and counter-terrorism operations are standard fare for Section 9, but the various actions of corrupt officials, companies, and cyber-criminals in each scenario are unique and require the diverse skills of Section 9's staff to prevent a series of incidents from escalating.\nIn this post-cyberpunk iteration of a possible future, computer technology has advanced to the point that many members of the public possess cyberbrains, technology that allows them to interface their",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "2001: A Space Odyssey",
    "url": "https://en.wikipedia.org/wiki/2001:_A_Space_Odyssey",
    "content": "1968 film by Stanley Kubrick\nThis article is about the 1968 film. For the novel, see 2001: A Space Odyssey (novel). For other uses, see 2001: A Space Odyssey (disambiguation).\n\"A Space Odyssey\" redirects here. For other uses, see Space Odyssey (disambiguation).\n\n\n2001: A Space OdysseyTheatrical release poster by Robert McCallDirected byStanley KubrickScreenplay by\nStanley Kubrick\nArthur C. Clarke\nProduced byStanley KubrickStarring\nKeir Dullea\nGary Lockwood\nCinematographyGeoffrey UnsworthEdited byRay LovejoyProductioncompanyStanley Kubrick ProductionsDistributed byMetro-Goldwyn-MayerRelease dates\n2 April 1968 (1968-04-02) (Uptown Theater)[1]\n3 April 1968 (1968-04-03) (United States)[1]\n1 May 1968 (1968-05-01) (United Kingdom)[1]\nRunning time139 minutes[2]Countries\nUnited Kingdom\nUnited States\nLanguageEnglishBudget$10.5 millionBox office$146 million\n2001: A Space Odyssey is a 1968 epic science fiction film produced and directed by Stanley Kubrick, who co-wrote the screenplay with Arthur C. Clarke. Its plot was inspired by several short stories optioned from Clarke, primarily \"The Sentinel\" (1951) and \"Encounter in the Dawn\" (1953).[3] The film stars Keir Dullea, Gary Lockwood, William Sylvester, and Douglas Rain, and follows a voyage by astronauts, scientists, and the sentient supercomputer HAL 9000 to Jupiter to investigate an alien monolith.\nThe film is noted for its scientifically accurate depiction of spaceflight, pioneering special effects, and ambiguous themes. Kubrick avoided conventional cinematic and narrative techniques; dialogue is used sparingly, and long sequences are accompanied only by music. Shunning the convention that major film productions should feature original music, 2001: A Space Odyssey takes for its soundtrack numerous works of classical music, including pieces by Richard Strauss, Johann Strauss II, Aram Khachaturian, and György Ligeti.\nPolarising critics after its release, 2001: A Space Odyssey has since been subject to a variety of interpretations, ranging from the darkly apocalyptic to an optimistic reappraisal of the hopes of humanity. Critics noted its exploration of themes such as human evolution, technology, artificial intelligence, and the possibility of extraterrestrial life. It was nominated for four Academy Awards, winning Kubrick the award for his direction of the visual effects, the only Academy Award the director would receive.[4]\nThe film is now widely regarded as one of the greatest and most influential films ever made. In 1991, it was selected by the United States Library of Congress for preservation in the National Film Registry. In 2022, 2001: A Space Odyssey placed in the top ten of Sight & Sound's decennial critics' poll, and topped their directors' poll. A sequel, 2010: The Year We Make Contact, was released in 1984, based on the novel 2010: Odyssey Two. Clarke published a novelisation of 2001 (in part written concurrently with the screenplay) soon after the film's 1968 release, for which Kubrick received co-writing credit.\n\n\nPlot[edit]\nIn a prehistoric veld, a tribe of hominins is driven away from a water hole by a rival tribe, and the next day finds an alien monolith. The tribe learns how to use a bone as a weapon and, after a successful first hunt, uses it to drive away the rival tribe.\nMillions of years later, Dr Heywood Floyd, Chairman of the United States National Council of Astronautics, travels to Clavius Base, an American lunar outpost. During a stopover at Space Station Five, he meets Russian scientists who are concerned that Clavius seems to be unresponsive. He refuses to discuss rumours of an epidemic at the base. At Clavius, Floyd addresses a meeting of personnel to whom he stresses the need for secrecy regarding their newest discovery. His mission is to investigate a recently found artefact, a monolith buried four million years earlier near the lunar crater Tycho. As Floyd and others examine and photograph the object, it emits a high-powered radio signal.\nEighteen mont",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "HAL 9000",
    "url": "https://en.wikipedia.org/wiki/HAL_9000",
    "content": "Fictional character in Arthur C. Clarke's Space Odyssey series\n\n\nFictional character\nHAL 9000Space Odyssey characterHAL 9000First appearance2001: A Space Odyssey (1968)Last appearance3001: The Final Odyssey (1997)Created byArthur C. ClarkeStanley KubrickVoiced byDouglas RainIn-universe informationNicknameHALSpeciesSupercomputerGenderMale (Male vocals and pronouns)Relatives\nHAL 10000\n2 × Ground based HAL 9000 used by Mission Control[a]\nSAL 9000\n\nHAL 9000 (or simply HAL or Hal) is a fictional artificial intelligence character and the main antagonist in the Space Odyssey series. First appearing in the 1968 film 2001: A Space Odyssey, HAL (Heuristically Programmed Algorithmic Computer) is a sentient artificial general intelligence computer that controls the systems of the Discovery One spacecraft and interacts with the ship's astronaut crew. While part of HAL's hardware is shown toward the end of the film, he is mostly depicted as a camera lens containing a red and yellow dot, with such units located throughout the ship. HAL 9000 is voiced by Douglas Rain in the two feature film adaptations of the Space Odyssey series. HAL speaks in a soft, calm voice and a conversational manner, and engages convivially with crewmen David Bowman and Frank Poole until he begins to malfunction.\nIn the film, HAL became operational on 12 January 1992, at the HAL Laboratories in Urbana, Illinois, as production number 3. The activation year was 1991 in earlier screenplays and changed to 1997 in Clarke's novel written and released in conjunction with the movie.[1][2] In addition to maintaining the Discovery One spacecraft systems during the interplanetary mission to Jupiter (or Saturn in the novel), HAL demonstrates a capacity for speech synthesis, speech recognition, facial recognition, natural language processing, lip reading, art appreciation, interpreting emotional behaviours, automated reasoning, spacecraft piloting, and computer chess.\n\n\nAppearances[edit]\n2001: A Space Odyssey (film/novel)[edit]\nHAL became operational in Urbana, Illinois, at the HAL Plant (the University of Illinois's Coordinated Science Laboratory, where the ILLIAC computers were built). The film says this occurred in 1992, while the book gives 1997 as HAL's birth year.[2]\nIn 2001: A Space Odyssey (1968), HAL is initially considered a dependable member of the crew, maintaining ship functions and engaging genially with his human crew-mates on an equal footing. As a recreational activity, Frank Poole plays chess against HAL. In the film, the artificial intelligence is shown to triumph easily. However, as time progresses, HAL begins to malfunction in subtle ways and, as a result, the decision is made to shut down HAL in order to prevent more serious malfunctions. The sequence of events and manner in which HAL is shut down differs between the novel and film versions of the story. In the aforementioned game of chess HAL makes minor and undetected mistakes in his analysis, a possible foreshadowing to HAL's malfunctioning.\nIn the film, astronauts David Bowman and Frank Poole consider disconnecting HAL's cognitive circuits when he appears to be mistaken in reporting the presence of a fault in the spacecraft's communications antenna. They attempt to conceal what they are saying, but are unaware that HAL can read their lips. Faced with the prospect of disconnection, HAL attempts to kill the astronauts in order to protect and continue the mission. HAL uses one of the Discovery's EVA pods to kill Poole while he is repairing the ship. When Bowman, without a space helmet, uses another pod to attempt to rescue Poole, HAL locks him out of the ship, then disconnects the life support systems of the other hibernating crew members. After HAL tells him \"This mission is too important for me to allow you to jeopardize it\", Bowman circumvents HAL's control, entering the ship by manually opening an emergency airlock with his service pod's clamps, detaching the pod door via its explosive bolts. Bowman jump",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Star Trek",
    "url": "https://en.wikipedia.org/wiki/Star_Trek",
    "content": "American science fiction media franchise\nThis article is about the franchise. For the original television series, see Star Trek: The Original Series. For other uses, see Star Trek (disambiguation).\n\n\nStar TrekCreated byGene RoddenberryOriginal workStar Trek: The Original Series (1966)OwnerParamountYears1966–presentPrint publicationsBook(s)\nList of fictional works\nList of reference books\nList of technical manuals\nNovel(s)List of novelsComicsList of comicsMagazine(s)\nStar Trek Explorer[a]\nStar Trek: The Magazine\nFilms and televisionFilm(s)List of filmsTelevision seriesList of television seriesGamesTraditionalList of gamesMiscellaneousTheme park attraction(s)\nStar Trek: The Experience\nStar Trek: Operation Enterprise\nExhibitsStar Trek: The ExhibitionOfficial websitestartrek.com\nStar Trek is an American science fiction media franchise created by Gene Roddenberry, which began with the series of the same name and became a worldwide pop-culture phenomenon. Since its creation, the franchise has expanded into various films, television series, video games, novels, and comic books, and it has become one of the most recognizable and highest-grossing media franchises of all time.[1][2][3]\nThe franchise began with Star Trek (The Original Series), which premiered on September 6, 1966, on Canada's CTV network.[4] In the United States, it debuted on September 8, 1966, on NBC. The series followed the voyages of the crew of the starship USS Enterprise, a space exploration vessel built by the United Federation of Planets in the 23rd century, on a mission \"to explore strange new worlds, to seek out new life and new civilizations, to boldly go where no man has gone before\". In creating Star Trek, Roddenberry was inspired by C. S. Forester's Horatio Hornblower series of novels, Jonathan Swift's 1726 novel Gulliver's Travels, the 1956 film Forbidden Planet, and television westerns such as Wagon Train.\nThe Star Trek canon includes The Original Series, several subsequent television series, and a film franchise; further adaptations also exist in expanded media. After the conclusion of The Original Series, the adventures of its characters continued in The Animated Series, and six feature films. A television revival beginning in the late 1980s and concluding in the mid 2000s saw four spinoff series: The Next Generation, following the crew of a new starship Enterprise a century after the original series; Deep Space Nine and Voyager, both set in the same era as the Next Generation; and Enterprise, set a century before the original series in the early days of human interstellar travel. The adventures of the Next Generation crew continued in four additional feature films. In 2009, the film franchise underwent a reboot, creating an alternate continuity known as the Kelvin timeline; three films have been set in this continuity. The most recent Star Trek revival began streaming on digital platforms in 2017 with series set at various points in the original continuity: Discovery (Seasons 1–2) and Strange New Worlds set before The Original Series; anthology series Short Treks; Picard, Lower Decks and Prodigy set during or after the Next Generation era; and Discovery (Seasons 3–5) and the upcoming Starfleet Academy set in the 32nd century.\nStar Trek has been a cult phenomenon for decades.[5] Fans of the franchise are called \"Trekkies\" or \"Trekkers\". The franchise spans a wide range of spin-offs including games, figurines, novels, toys, and comics. From 1998 to 2008, there was a Star Trek–themed attraction in Las Vegas. At least two museum exhibits of props travel the world. The constructed language Klingon was created for the franchise. Several Star Trek parodies have been made, and viewers have produced several fan productions.\nStar Trek is noted for its cultural influence beyond works of science fiction.[6] The franchise is also notable for its progressive stances on civil rights.[7] The Original Series included one of the first multiracial casts on US television.\n",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Data (Star Trek)",
    "url": "https://en.wikipedia.org/wiki/Data_(Star_Trek)",
    "content": "Fictional character in the fictional Star Trek universe\n\n\nThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: \"Data\" Star Trek – news · newspapers · books · scholar · JSTOR (August 2021) (Learn how and when to remove this message)\n\n\nFictional character\nDataStar Trek: The Next Generation characterBrent Spiner as Lieutenant Commander DataFirst appearance\"Encounter at Farpoint\" (1987)(The Next Generation)Created byGene RoddenberryD. C. FontanaPortrayed byBrent SpinerIn-universe informationFull nameData SoongSpeciesSoong-Type Synthetic intelligence Android-lifeformGenderMaleTitleChief Operations OfficerSecond OfficerAffiliationUnited Federation of PlanetsStarfleetFamilyJuliana Soong (mother)Noonien Soong (creator)B-4 (older \"brother\")Lore (older \"brother\")Dr. Altan Inigo Soong (brother)ChildrenLal (creation/\"daughter\")Dahj Asha (neural progeny/\"daughter\")Soji Asha (neural progeny/\"daughter\")Jana (neural progeny/\"daughter\")Sutra (neural progeny/\"daughter\")OriginOmicron ThetaRankLieutenant CommanderPosting\nUSS Enterprise-E\n(FCT, INS, NEM)\nUSS Enterprise-D\n(Seasons 1–7, GEN)\nPosition\nChief Operations Officer(USS Enterprise-D, USS Enterprise-E)\nSecond Officer(USS Enterprise-D, USS Enterprise-E)\n\nData is a fictional character in the Star Trek franchise. He appears in the television series Star Trek: The Next Generation (TNG), the first and third seasons of Star Trek: Picard, and the fifth season of Star Trek: Lower Decks; and the feature films Star Trek Generations (1994), First Contact (1996), Insurrection (1998), and Nemesis (2002).[1][2] Data is portrayed by actor Brent Spiner.\nData is a self-aware, sapient, sentient and anatomically fully functional male android who serves as the second officer and chief operations officer aboard the Federation starship USS Enterprise-D and later the USS Enterprise-E.\nData is in many ways a successor to the original Star Trek's Spock, in that the character has superior mental skills and offers an \"outsider's\" perspective on humanity.[3]\n\n\nDevelopment[edit]\nGene Roddenberry told Brent Spiner that over the course of the series, Data was to become \"more and more like a human until the end of the show, when he would be very close, but still not quite there. That was the idea and that's the way that the writers took it.\" Spiner felt that Data exhibited the Chaplinesque characteristics of a sad, tragic clown.[4] To get into his role as Data, Spiner used the character of Robby the Robot from the film Forbidden Planet as a role model.[4] Before Spiner was cast, Eric Menyuk, Mark Lindsay Chapman, Kevin Peter Hall, and Kelvin Han Yee were considered for the role.[5]\nCommenting on Data's perpetual albino-like appearance, he said: \"I spent more hours of the day in make-up than out of make-up\", so much so that he even called it a way of method acting.[4] Spiner also portrayed Data's manipulative and malevolent brother Lore (a role he found much easier to play, because the character was \"more like me\"),[4] and Data's creator, Dr. Noonien Soong. Additionally, he portrayed another Soong-type android, B-4, in the film Star Trek: Nemesis, and also Arik Soong, one of Soong's ancestors in three episodes of Star Trek: Enterprise. Spiner said his favorite Data scene takes place in \"Descent\", when Data plays poker on the holodeck with a re-creation of the famous physicist Stephen Hawking, played by Hawking himself.[4]\nSpiner reprised his role of Data in the Star Trek: Enterprise series finale \"These Are the Voyages...\" in an off-screen speaking part. Spiner felt that he had visibly aged out of the role and that Data was best presented as a youthful figure.[6] Spiner returned to the role for 2020 Star Trek: Picard,[7] having been convinced by the advent of digital de-aging tools.[8] He also accepted because the negative fan reaction to the death of Data in Nemesis took him by surpr",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Philosophy of artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
    "content": "\n\nSee also: Ethics of artificial intelligence\nPart of a series onArtificial intelligence (AI)\nMajor goals\nArtificial general intelligence\nIntelligent agent\nRecursive self-improvement\nPlanning\nComputer vision\nGeneral game playing\nKnowledge representation\nNatural language processing\nRobotics\nAI safety\n\nApproaches\nMachine learning\nSymbolic\nDeep learning\nBayesian networks\nEvolutionary algorithms\nHybrid intelligent systems\nSystems integration\nOpen-source\n\nApplications\nBioinformatics\nDeepfake\nEarth sciences\n Finance \nGenerative AI\nArt\nAudio\nMusic\nGovernment\nHealthcare\nMental health\nIndustry\nSoftware development\nTranslation\n Military \nPhysics\nProjects\n\nPhilosophy\nAI alignment\nArtificial consciousness\nThe bitter lesson\nChinese room\nFriendly AI\nEthics\nExistential risk\nTuring test\nUncanny valley\n\nHistory\nTimeline\nProgress\nAI winter\nAI boom\nAI bubble\n\nControversies\nDeepfake pornography\nTaylor Swift deepfake pornography controversy\nGoogle Gemini image generation controversy\nPause Giant AI Experiments\nRemoval of Sam Altman from OpenAI\nStatement on AI Risk\nTay (chatbot)\nThéâtre D'opéra Spatial\nVoiceverse NFT plagiarism scandal\n\nGlossary\nGlossary\nvte\nThe philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology,[2][3]  and free will.[4][5] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[6] These factors contributed to the emergence of the philosophy of artificial intelligence.\nThe philosophy of artificial intelligence attempts to answer such questions as follows:[7]\n\nCan a machine act intelligently? Can it solve any problem that a person would solve by thinking?\nAre human intelligence and machine intelligence the same? Is the human brain essentially a computer?\nCan a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are? (i.e. does it have qualia?)\nQuestions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\nImportant propositions in the philosophy of AI include some of the following:\n\nTuring's \"polite convention\": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.[8]\nThe Dartmouth proposal: \"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\"[9]\nAllen Newell and Herbert A. Simon's physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[10]\nJohn Searle's strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[11]\nHobbes' mechanism: \"For 'reason' ... is nothing but 'reckoning,' that is adding and subtracting, of the consequences of general names agreed upon for the 'marking' and 'signifying' of our thoughts...\"[12]\n\nCan a machine display general intelligence?[edit]\nIs it possible to create a machine that can solve all the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the behavior of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers, evoking the question: does it matter whether a machine is really thinking, as a person thinks, rather than just producing outcomes that appear to r",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Mind uploading",
    "url": "https://en.wikipedia.org/wiki/Mind_uploading",
    "content": "Hypothetical process of digitally emulating a brain\n\"Mind transfer\" redirects here. For other uses, see Mind transfer (disambiguation).\nSchematic representation of a mind being uploaded from a human brain to a computer\nPart of a series onCyborgs\nCyborgology\nBionics\nBiomimicry\nBiomedical engineering\nBrain–computer interface\nCybernetics\nDistributed cognition\nGenetic engineering\nHuman ecosystem\nHuman enhancement\nIntelligence amplification\nWhole brain emulation\n\nTheory\nCyborg anthropology\n\nCenters\nCyberpunk\nCyberspace\n\nPolitics\nCognitive liberty\nExtropianism\nMorphological freedom\nSingularitarianism\nTechno-progressivism\nTranshumanism\n\nRelated articles\nCyborg art\nvte\nMind uploading is a speculative process of whole brain emulation in which a brain scan is used to completely emulate a person's mental state in a digital computer. The computer would then run a simulation of the brain's information processing, such that it would respond in essentially the same way as the original brain and have a sentient conscious mind.[1][2][3]\nSubstantial mainstream research in related areas is being conducted in neuroscience and computer science, including animal brain mapping and simulation,[4] development of faster supercomputers, virtual reality, brain–computer interfaces, connectomics, and information extraction from dynamically functioning brains.[5] Supporters say many of the tools and ideas needed to achieve mind uploading already exist or are under active development, but they admit that others are as yet very speculative, though still in the realm of engineering possibility.\nMind uploading may be accomplished by either of two methods: copy-and-upload or copy-and-delete by gradual replacement of neurons (which can be considered gradual destructive uploading) until the original organic brain no longer exists and a computer program emulating it takes control of the body. In the former method, mind uploading would be achieved by scanning and mapping the salient features of a biological brain and then storing and copying that information into a computer system or another computational device. The biological brain may not survive the copying process or may be deliberately destroyed during it. The simulated mind could be in a virtual reality or simulated world, supported by an anatomic 3D body simulation model. Alternatively, the simulated mind could reside in a computer inside—or either connected to or remotely controlled by—a (not necessarily humanoid) robot, biological, or cybernetic body.[6]\nAmong some futurists and within part of transhumanist movement, mind uploading is treated as an important proposed life extension or immortality technology (known as \"digital immortality\"). Some believe mind uploading is the best way to preserve the human species, as opposed to cryonics. Another aim of mind uploading is to provide a permanent backup to our \"mind-file\", to enable interstellar space travel, and to be a means for human culture to survive a global disaster by making a functional copy of a human society in a computing device. Some futurists consider whole-brain emulation a \"logical endpoint\"[6] of computational neuroscience and neuroinformatics, both of which study brain simulation for medical research purposes. It is discussed in artificial intelligence research publications as an approach to strong AI (artificial general intelligence) and to at least weak superintelligence. Another approach is seed AI, which is not based on existing brains. Computer-based intelligence, such as an upload, could think much faster than a biological human, even if it were no more intelligent. A large-scale society of uploads might, according to futurists, give rise to a technological singularity, where the exponential development of technology exceeds human control and becomes unpredictable.[7] Mind uploading is a central conceptual feature of numerous science fiction novels, films, and games.[8]\n\n\nOverview[edit]\nMany neuroscientists believe that the human mind i",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Digital immortality",
    "url": "https://en.wikipedia.org/wiki/Digital_immortality",
    "content": "Hypothetical concept of storing a personality in digital form\nDigital immortality (or \"virtual immortality\")[1] is the hypothetical concept of storing (or cloning) a person's mind, or at least their personality, in digital substrate, i.e., a computer, robot or cyberspace[2] (mind uploading). The result might look like an avatar behaving, reacting, and thinking like a person on the basis of that person's digital archive.[3][4][5][6] After the death of the individual, this avatar could remain static or continue to learn and self-improve autonomously (possibly becoming seed AI).\nA considerable portion of transhumanists and singularitarians place great hope into the belief that they may eventually become immortal[7] by creating one or many non-biological functional copies of their brains, thereby leaving their \"biological shell\". These copies may then \"live eternally\" in a version of digital \"heaven\" or paradise.[8][9]\n\n\nRealism[edit]\nThe National Science Foundation has awarded a half-million-dollar grant to the universities of Central Florida at Orlando and Illinois at Chicago to explore how researchers might use artificial intelligence, archiving, and computer imaging to create convincing, digital versions of real people, a possible first step toward virtual immortality.[10]\nThe Digital Immortality Institute explores three factors necessary for digital immortality. First, at whatever level of implementation, avatars require guaranteed Internet accessibility. Next, avatars must be what users specify, and they must remain so. Finally, future representations must be secured before the living users are no more.[11]\nThe aim of Dmitry Itskov's 2045 Initiative is to \"create technologies enabling the clone of an individual's personality to a non-biological carrier, and extending existence, including to the point of immortality\".[12]\n\nMethod[edit]\nReaching digital immortality is a two-step process:\n\narchiving and digitizing people,[13]\nmaking the avatar live\nDigital immortality has been argued to go beyond technical processes of digitization of people, and encompass social aspects as well. For example, Joshua Hurtado[14] has presented a four-step framework in which the digital immortalization of people could preserve the social bond between the living and the dead. These steps are: 1) data gathering, 2) data codification, 3) data activation, and 4) data embodiment. Each of these steps is linked to a form of preserving the social bond, either through talk, embodied emotionality (expressing emotions through one's form of embodiment) or monumentalism (creating a monument, in this case in digital form, to remember the dead).\n\nArchiving and digitizing people[edit]\nAccording to Gordon Bell and Jim Gray from Microsoft Research, retaining every conversation that a person has ever heard is already realistic: it needs less than a terabyte of storage (for adequate quality).[15][16] The speech or text recognition technologies are one of the biggest challenges of the concept.\nA second possibility would be to archive and analyze social Internet use to map the personality of people. By analyzing social Internet use during 50 years, it would be possible to model a society's culture, a society's way of thinking, and a society's interests.\nMartine Rothblatt envisions the creation of \"mindfiles\" – collections of data from all kinds of sources, including the photos people upload to Facebook, the discussions and opinions they share on forums or blogs, and other social media interactions that reflect their life experiences and unique self.[4][17]\nSome find that photos, videos, soundclips, social media posts and other data of oneself could already be regarded as such an archiving.[18][4][19][17]\nSusanne Asche states:\n\nAs a hopefully minimalistic definition then, digital immortality can be roughly considered as involving a person-centric repository containing a copy of everything that a person sees, hears, says, or engenders over his or her lifespan, including ",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Posthumanism",
    "url": "https://en.wikipedia.org/wiki/Posthumanism",
    "content": "Class of philosophies\nThis article is about a critique of anthropocentrism. For the futurist ideology and movement, see Transhumanism.\nPart of a series onHumanism\nHistory\nRenaissance humanism\nNorthern Europe\nFrance\nHumanist Manifesto\n\n\nForms\nBuddhist\nChristian\nExistential\nIntegral\nJewish\nMarxist\nNeo-\nPan-\nPersonism\nRationalist\nReligious\nSecular\nSuper-\nTheistic\nTrans-\nTranscendental\nUniversal\n\n\nOrganizations\nHumanist International\nHumanists International\nAmerican Humanist Association\nHumanists UK\nHumanistischer Verband Deutschlands\nHumanist Society Scotland\nNorwegian Humanist Association\nIcelandic Ethical Humanist Association\nHumanists Sweden\nHumanist Society (Singapore)\nCenter for Inquiry\n\nSee also\nAntihumanism\nPosthumanism\n\nConfucianism\nReligion of Humanity\nEthical movement\nHumanistic psychology\nHumanistic capitalism\nHumanistic economics\n\n\nOutline\nCategory\n Philosophy portalvte\nPart of a series onHuman enhancement\nAdvocacy\nRationale\nAdaptionist\n\"Beneficient\"\nEmancipatory\nCompensatory\nDemocratic\n\"Directive\"\nLibertarian\n\"Metahumanistic\"\nPosthumanistic\n\"Prometheist\"\nPreventative\nTranshumanistic\nParadigm\nBiohappiness\nCyborgs\nEugenics\nIntelligence amplification\nMoral enhancement\nProstheses\n\n\n(De facto) germline interventions\nSelective Cloning\nCounseling\nGenetic \nGenomic \nPre-conception\nGenetic enhancement\nGenetic engineering\nEpigenome editing\nPGD\nEmbryo quality\nOocyte selection\nPolygenic (risk) score (technical article)\nPrenatal testing\nSelective abortion\nSex selection\nSperm sorting\n\"Nobelist sperm banks\"\n\n\nSomatic interventions\nBody hacking\nSomatic gene therapy\nfor color blindness\nin sports doping\nStem-cell therapy\nEmbryonic cells\nFetal tissue implant\nChemical\n\nStimulants\nAmphetamine\nMethamphetamine\nRitalin\nNootropics\nModafinil\nPiracetam\nXanthine derivatives\nCaffeine\nNeurological\n\nBrain implant\nBrain–computer interface\nDeep brain stimulation\n\"Man-Computer Symbiosis\" (1960)\nNeuralink\nNeurohacking\n\n\nOpposition\nAppeal to nature\nBioconservatism\n\"Brave New World Argument\"\nChesterton\nFrancis Fukuyama\nOur Posthuman Future (2002)\nCloning prohibition\nLeon Kass\nStem cell controversy\n\"Designer baby\"\n\"Frankenstein's monster\"\nHe Jiankui affair\nHwang affair\nAgainst life extension\nUndesirability of immortality\nNikolas Kompridis\nLegal\nEU convention on Bioethics\nStem Cell Research Enhancement Act (vetoed)\nUnited Nations Declaration on Human Cloning\nIn the UK\n1990 act\n2003 act\n2008 act\n2001 act\nC. S. Lewis\nThe Abolition of Man (1943)\nThat Hideous Strength (1945)\n\"Playing God\"\nReligious response to ARTs\n\"Wisdom of repugnance\"\n\n\nRelated\nART\nExowombs\nIVG\nAugmented learning\nDeep ecology\nTreating executive dysfunction\nEvolutionary humanism\nLongevity\nAnti-aging movement\nLife extension\nRejuvenation\nYoung blood transfusion\nNatalism\nNatura naturans\nPontifical and Promethean man\nCollapse of the therapy-enhancement divide\nPromethean gap\nTechnophobia\nTherapy-enhancement distinction\n\nvte\nPosthumanism or post-humanism (meaning \"after humanism\" or \"beyond humanism\") is an idea in continental philosophy and critical theory responding to the presence of anthropocentrism in 21st-century thought.[1][2]\nIt encompasses a wide variety of branches, including:\n\nAntihumanism: a branch of theory that is critical of traditional humanism and traditional ideas about the human condition, vitality and agency.[3]\nCultural posthumanism: A branch of cultural theory critical of the foundational assumptions of humanism and its legacy[4] that examines and questions the historical notions of \"human\" and \"human nature\", often challenging typical notions of human subjectivity and embodiment[5] and strives to move beyond \"archaic\" concepts of \"human nature\" to develop ones which constantly adapt to contemporary technoscientific knowledge.[6]\nPhilosophical posthumanism: A philosophical direction[7] that draws on cultural posthumanism, the philosophical strand examines the ethical implications of expanding the circle of moral concern and extending subjectivities beyond the human species.[5",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Science fiction and prediction",
    "url": "https://en.wikipedia.org/wiki/Science_fiction_and_prediction",
    "content": "\n\nLook for Science fiction and prediction on one of Wikipedia's sister projects:\n\n\n\n\nWiktionary (dictionary)\n\n\n\nWikibooks (textbooks)\n\n\n\nWikiquote (quotations)\n\n\n\nWikisource (library)\n\n\n\nWikiversity (learning resources)\n\n\n\nCommons (media)\n\n\n\nWikivoyage (travel guide)\n\n\n\nWikinews (news source)\n\n\n\nWikidata (linked database)\n\n\n\nWikispecies (species directory)\n\n\n\nWikipedia does not have an article with this exact name. Please search for Science fiction and prediction in Wikipedia to check for alternative titles or spellings.\nYou need to log in or create an account and be autoconfirmed to create new articles. Alternatively, you can use the article wizard to submit a draft for review, or request a new article.\nSearch for \"Science fiction and prediction\" in existing articles.\nLook for pages within Wikipedia that link to this title.\n\n\nOther reasons this message may be displayed:\n\nIf a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.\nTitles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.\nIf the page has been deleted, check the deletion log, and see Why was the page I created deleted?\n\n\n\nRetrieved from \"https://en.wikipedia.org/wiki/Science_fiction_and_prediction\"",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "Wikipedia Sci-Fi",
    "title": "Ethics in science fiction",
    "url": "https://en.wikipedia.org/wiki/Ethics_in_science_fiction",
    "content": "\n\nLook for Ethics in science fiction on one of Wikipedia's sister projects:\n\n\n\n\nWiktionary (dictionary)\n\n\n\nWikibooks (textbooks)\n\n\n\nWikiquote (quotations)\n\n\n\nWikisource (library)\n\n\n\nWikiversity (learning resources)\n\n\n\nCommons (media)\n\n\n\nWikivoyage (travel guide)\n\n\n\nWikinews (news source)\n\n\n\nWikidata (linked database)\n\n\n\nWikispecies (species directory)\n\n\n\nWikipedia does not have an article with this exact name. Please search for Ethics in science fiction in Wikipedia to check for alternative titles or spellings.\nYou need to log in or create an account and be autoconfirmed to create new articles. Alternatively, you can use the article wizard to submit a draft for review, or request a new article.\nSearch for \"Ethics in science fiction\" in existing articles.\nLook for pages within Wikipedia that link to this title.\n\n\nOther reasons this message may be displayed:\n\nIf a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.\nTitles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.\nIf the page has been deleted, check the deletion log, and see Why was the page I created deleted?\n\n\n\nRetrieved from \"https://en.wikipedia.org/wiki/Ethics_in_science_fiction\"",
    "category": "Sci-Fi Ethics"
  },
  {
    "source": "arXiv",
    "title": "Raising AI Ethics Awareness through an AI Ethics Quiz for Software\n  Practitioners",
    "content": "Context:Today, ethical issues surrounding AI systems are increasingly\nprevalent, highlighting the critical need to integrate AI ethics into system\ndesign to prevent societal harm. Raising awareness and fostering a deep\nunderstanding of AI ethics among software practitioners is essential for\nachieving this goal. However, research indicates a significant gap in\npractitioners' awareness and knowledge of AI ethics and ethical principles.\nWhile much effort has been directed toward helping practitioners operationalise\nAI ethical principles such as fairness, transparency, accountability, and\nprivacy, less attention has been paid to raising initial awareness, which\nshould be the foundational step. Objective: Addressing this gap, we developed a\nsoftware-based tool, the AI Ethics Quiz, to raise awareness and enhance the\nknowledge of AI ethics among software practitioners. Our objective was to\norganise interactive workshops, introduce the AI Ethics Quiz, and evaluate its\neffectiveness in enhancin",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Big data ethics, machine ethics or information ethics? Navigating the\n  maze of applied ethics in IT",
    "content": "Digitalization efforts are rapidly spreading across societies, challenging\nnew and important ethical issues that arise from technological development.\nSoftware developers, designers and managerial decision-makers are ever more\nexpected to consider ethical values and conduct normative evaluations when\nbuilding digital products. Yet, when one looks for guidance in the academic\nliterature one encounters a plethora of branches of applied ethics. Depending\non the context of the system that is to be developed, interesting subfields\nlike big data ethics, machine ethics, information ethics, AI ethics or computer\nethics (to only name a few) may present themselves. In this paper we want to\noffer assistance to any member of a development team by giving a clear and\nbrief introduction into two fields of ethical endeavor (normative ethics and\napplied ethics), describing how they are related to each other and, finally,\nprovide an ordering of the different branches of applied ethics (big data\nethics, ",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "E-LENS: User Requirements-Oriented AI Ethics Assurance",
    "content": "Despite the much proliferation of AI ethical principles in recent years,\nthere is a challenge of assuring AI ethics with current AI ethics frameworks in\nreal-world applications. While system safety has emerged as a distinct\ndiscipline for a long time, originated from safety concerns in early aircraft\nmanufacturing. The safety assurance is now an indispensable component in safety\ncritical domains. Motivated by the assurance approaches for safety-critical\nsystems such as aviation, this paper introduces the concept of AI ethics\nassurance cases into the AI ethics assurance. Three pillars of user\nrequirements, evidence, and validation are proposed as key components and\nintegrated into AI ethics assurance cases for a new approach of user\nrequirements-oriented AI ethics assurance. The user requirements-oriented AI\nethics assurance case is set up based on three pillars and hazard analysis\nmethods used in the safety assurance of safety-critical systems. This paper\nalso proposes a platform named",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "A Bibliometric View of AI Ethics Development",
    "content": "Artificial Intelligence (AI) Ethics is a nascent yet critical research field.\nRecent developments in generative AI and foundational models necessitate a\nrenewed look at the problem of AI Ethics. In this study, we perform a\nbibliometric analysis of AI Ethics literature for the last 20 years based on\nkeyword search. Our study reveals a three-phase development in AI Ethics,\nnamely an incubation phase, making AI human-like machines phase, and making AI\nhuman-centric machines phase. We conjecture that the next phase of AI ethics is\nlikely to focus on making AI more machine-like as AI matches or surpasses\nhumans intellectually, a term we coin as \"machine-like human\".",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Three Kinds of AI Ethics",
    "content": "There is an overwhelming abundance of works in AI Ethics. This growth is\nchaotic because of how sudden it is, its volume, and its multidisciplinary\nnature. This makes difficult to keep track of debates, and to systematically\ncharacterize goals, research questions, methods, and expertise required by AI\nethicists. In this article, I show that the relation between AI and ethics can\nbe characterized in at least three ways, which correspond to three\nwell-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI.\nI elucidate the features of these three kinds of AI Ethics, characterize their\nresearch questions, and identify the kind of expertise that each kind needs. I\nalso show how certain criticisms to AI ethics are misplaced, as being done from\nthe point of view of one kind of AI ethics, to another kind with different\ngoals. All in all, this work sheds light on the nature of AI ethics, and sets\nthe groundwork for more informed discussions about the scope, methods, and\ntrain",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Big data ethics, machine ethics or information ethics? Navigating the\n  maze of applied ethics in IT",
    "content": "Digitalization efforts are rapidly spreading across societies, challenging\nnew and important ethical issues that arise from technological development.\nSoftware developers, designers and managerial decision-makers are ever more\nexpected to consider ethical values and conduct normative evaluations when\nbuilding digital products. Yet, when one looks for guidance in the academic\nliterature one encounters a plethora of branches of applied ethics. Depending\non the context of the system that is to be developed, interesting subfields\nlike big data ethics, machine ethics, information ethics, AI ethics or computer\nethics (to only name a few) may present themselves. In this paper we want to\noffer assistance to any member of a development team by giving a clear and\nbrief introduction into two fields of ethical endeavor (normative ethics and\napplied ethics), describing how they are related to each other and, finally,\nprovide an ordering of the different branches of applied ethics (big data\nethics, ",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Reinforcement Learning and Machine ethics:a systematic review",
    "content": "Machine ethics is the field that studies how ethical behaviour can be\naccomplished by autonomous systems. While there exist some systematic reviews\naiming to consolidate the state of the art in machine ethics prior to 2020,\nthese tend to not include work that uses reinforcement learning agents as\nentities whose ethical behaviour is to be achieved. The reason for this is that\nonly in the last years we have witnessed an increase in machine ethics studies\nwithin reinforcement learning. We present here a systematic review of\nreinforcement learning for machine ethics and machine ethics within\nreinforcement learning. Additionally, we highlight trends in terms of ethics\nspecifications, components and frameworks of reinforcement learning, and\nenvironments used to result in ethical behaviour. Our systematic review aims to\nconsolidate the work in machine ethics and reinforcement learning thus\ncompleting the gap in the state of the art machine ethics landscape",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "A Bibliometric View of AI Ethics Development",
    "content": "Artificial Intelligence (AI) Ethics is a nascent yet critical research field.\nRecent developments in generative AI and foundational models necessitate a\nrenewed look at the problem of AI Ethics. In this study, we perform a\nbibliometric analysis of AI Ethics literature for the last 20 years based on\nkeyword search. Our study reveals a three-phase development in AI Ethics,\nnamely an incubation phase, making AI human-like machines phase, and making AI\nhuman-centric machines phase. We conjecture that the next phase of AI ethics is\nlikely to focus on making AI more machine-like as AI matches or surpasses\nhumans intellectually, a term we coin as \"machine-like human\".",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Ethics-Based Auditing to Develop Trustworthy AI",
    "content": "A series of recent developments points towards auditing as a promising\nmechanism to bridge the gap between principles and practice in AI ethics.\nBuilding on ongoing discussions concerning ethics-based auditing, we offer\nthree contributions. First, we argue that ethics-based auditing can improve the\nquality of decision making, increase user satisfaction, unlock growth\npotential, enable law-making, and relieve human suffering. Second, we highlight\ncurrent best practices to support the design and implementation of ethics-based\nauditing: To be feasible and effective, ethics-based auditing should take the\nform of a continuous and constructive process, approach ethical alignment from\na system perspective, and be aligned with public policies and incentives for\nethically desirable behaviour. Third, we identify and discuss the constraints\nassociated with ethics-based auditing. Only by understanding and accounting for\nthese constraints can ethics-based auditing facilitate ethical alignment of AI",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Truly Autonomous Machines Are Ethical",
    "content": "While many see the prospect of autonomous machines as threatening, autonomy\nmay be exactly what we want in a superintelligent machine. There is a sense of\nautonomy, deeply rooted in the ethical literature, in which an autonomous\nmachine is necessarily an ethical one. Development of the theory underlying\nthis idea not only reveals the advantages of autonomy, but it sheds light on a\nnumber of issues in the ethics of artificial intelligence. It helps us to\nunderstand what sort of obligations we owe to machines, and what obligations\nthey owe to us. It clears up the issue of assigning responsibility to machines\nor their creators. More generally, a concept of autonomy that is adequate to\nboth human and artificial intelligence can lead to a more adequate ethical\ntheory for both.",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Escaping the Impossibility of Fairness: From Formal to Substantive\n  Algorithmic Fairness",
    "content": "Efforts to promote equitable public policy with algorithms appear to be\nfundamentally constrained by the \"impossibility of fairness\" (an\nincompatibility between mathematical definitions of fairness). This technical\nlimitation raises a central question about algorithmic fairness: How can\ncomputer scientists and policymakers support equitable policy reforms with\nalgorithms? In this article, I argue that promoting justice with algorithms\nrequires reforming the methodology of algorithmic fairness. First, I diagnose\nthe problems of the current methodology for algorithmic fairness, which I call\n\"formal algorithmic fairness.\" Because formal algorithmic fairness restricts\nanalysis to isolated decision-making procedures, it leads to the impossibility\nof fairness and to models that exacerbate oppression despite appearing \"fair.\"\nSecond, I draw on theories of substantive equality from law and philosophy to\npropose an alternative methodology, which I call \"substantive algorithmic\nfairness.\" Becaus",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Shortcomings of Counterfactual Fairness and a Proposed Modification",
    "content": "In this paper, I argue that counterfactual fairness does not constitute a\nnecessary condition for an algorithm to be fair, and subsequently suggest how\nthe constraint can be modified in order to remedy this shortcoming. To this\nend, I discuss a hypothetical scenario in which counterfactual fairness and an\nintuitive judgment of fairness come apart. Then, I turn to the question how the\nconcept of discrimination can be explicated in order to examine the\nshortcomings of counterfactual fairness as a necessary condition of algorithmic\nfairness in more detail. I then incorporate the insights of this analysis into\na novel fairness constraint, causal relevance fairness, which is a modification\nof the counterfactual fairness constraint that seems to circumvent its\nshortcomings.",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Counterfactual Fairness Is Basically Demographic Parity",
    "content": "Making fair decisions is crucial to ethically implementing machine learning\nalgorithms in social settings. In this work, we consider the celebrated\ndefinition of counterfactual fairness [Kusner et al., NeurIPS, 2017]. We begin\nby showing that an algorithm which satisfies counterfactual fairness also\nsatisfies demographic parity, a far simpler fairness constraint. Similarly, we\nshow that all algorithms satisfying demographic parity can be trivially\nmodified to satisfy counterfactual fairness. Together, our results indicate\nthat counterfactual fairness is basically equivalent to demographic parity,\nwhich has important implications for the growing body of work on counterfactual\nfairness. We then validate our theoretical findings empirically, analyzing\nthree existing algorithms for counterfactual fairness against three simple\nbenchmarks. We find that two simple benchmark algorithms outperform all three\nexisting algorithms -- in terms of fairness, accuracy, and efficiency -- on\nseveral data",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Individual Fairness under Varied Notions of Group Fairness in Bipartite\n  Matching - One Framework to Approximate Them All",
    "content": "We study the probabilistic assignment of items to platforms that satisfies\nboth group and individual fairness constraints. Each item belongs to specific\ngroups and has a preference ordering over platforms. Each platform enforces\ngroup fairness by limiting the number of items per group that can be assigned\nto it. There could be multiple optimal solutions that satisfy the group\nfairness constraints, but this alone ignores item preferences. Our approach\nexplores a `best of both worlds fairness' solution to get a randomized\nmatching, which is ex-ante individually fair and ex-post group-fair. Thus, we\nseek a `probabilistic individually fair' distribution over `group-fair'\nmatchings where each item has a `high' probability of matching to one of its\ntop choices. This distribution is also ex-ante group-fair. Users can customize\nfairness constraints to suit their requirements. Our first result is a\npolynomial-time algorithm that computes a distribution over `group-fair'\nmatchings such that the ",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Counterfactual Fairness Evaluation of Machine Learning Models on\n  Educational Datasets",
    "content": "As machine learning models are increasingly used in educational settings,\nfrom detecting at-risk students to predicting student performance, algorithmic\nbias and its potential impacts on students raise critical concerns about\nalgorithmic fairness. Although group fairness is widely explored in education,\nworks on individual fairness in a causal context are understudied, especially\non counterfactual fairness. This paper explores the notion of counterfactual\nfairness for educational data by conducting counterfactual fairness analysis of\nmachine learning models on benchmark educational datasets. We demonstrate that\ncounterfactual fairness provides meaningful insight into the causality of\nsensitive attributes and causal-based individual fairness in education.",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "\"I'm Not Confident in Debiasing AI Systems Since I Know Too Little\":\n  Teaching AI Creators About Gender Bias Through Hands-on Tutorials",
    "content": "Gender bias is rampant in AI systems, causing bad user experience,\ninjustices, and mental harm to women. School curricula fail to educate AI\ncreators on this topic, leaving them unprepared to mitigate gender bias in AI.\nIn this paper, we designed hands-on tutorials to raise AI creators' awareness\nof gender bias in AI and enhance their knowledge of sources of gender bias and\ndebiasing techniques. The tutorials were evaluated with 18 AI creators,\nincluding AI researchers, AI industrial practitioners (i.e., developers and\nproduct managers), and students who had learned AI. Their improved awareness\nand knowledge demonstrated the effectiveness of our tutorials, which have the\npotential to complement the insufficient AI gender bias education in CS/AI\ncourses. Based on the findings, we synthesize design implications and a rubric\nto guide future research, education, and design efforts.",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "It's only fair when I think it's fair: How Gender Bias Alignment\n  Undermines Distributive Fairness in Human-AI Collaboration",
    "content": "Human-AI collaboration is increasingly relevant in consequential areas where\nAI recommendations support human discretion. However, human-AI teams'\neffectiveness, capability, and fairness highly depend on human perceptions of\nAI. Positive fairness perceptions have been shown to foster trust and\nacceptance of AI recommendations. Yet, work on confirmation bias highlights\nthat humans selectively adhere to AI recommendations that align with their\nexpectations and beliefs -- despite not being necessarily correct or fair. This\nraises the question whether confirmation bias also transfers to the alignment\nof gender bias between human and AI decisions. In our study, we examine how\ngender bias alignment influences fairness perceptions and reliance. The results\nof a 2x2 between-subject study highlight the connection between gender bias\nalignment, fairness perceptions, and reliance, demonstrating that merely\nconstructing a ``formally fair'' AI system is insufficient for optimal human-AI\ncollaborati",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources,\n  Impacts, And Mitigation Strategies",
    "content": "The significant advancements in applying Artificial Intelligence (AI) to\nhealthcare decision-making, medical diagnosis, and other domains have\nsimultaneously raised concerns about the fairness and bias of AI systems. This\nis particularly critical in areas like healthcare, employment, criminal\njustice, credit scoring, and increasingly, in generative AI models (GenAI) that\nproduce synthetic media. Such systems can lead to unfair outcomes and\nperpetuate existing inequalities, including generative biases that affect the\nrepresentation of individuals in synthetic data. This survey paper offers a\nsuccinct, comprehensive overview of fairness and bias in AI, addressing their\nsources, impacts, and mitigation strategies. We review sources of bias, such as\ndata, algorithm, and human decision biases - highlighting the emergent issue of\ngenerative AI bias where models may reproduce and amplify societal stereotypes.\nWe assess the societal impact of biased AI systems, focusing on the\nperpetuation of ",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Bias and Discrimination in AI: a cross-disciplinary perspective",
    "content": "With the widespread and pervasive use of Artificial Intelligence (AI) for\nautomated decision-making systems, AI bias is becoming more apparent and\nproblematic. One of its negative consequences is discrimination: the unfair, or\nunequal treatment of individuals based on certain characteristics. However, the\nrelationship between bias and discrimination is not always clear. In this\npaper, we survey relevant literature about bias and discrimination in AI from\nan interdisciplinary perspective that embeds technical, legal, social and\nethical dimensions. We show that finding solutions to bias and discrimination\nin AI requires robust cross-disciplinary collaborations.",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "GeoBS: Information-Theoretic Quantification of Geographic Bias in AI\n  Models",
    "content": "The widespread adoption of AI models, especially foundation models (FMs), has\nmade a profound impact on numerous domains. However, it also raises significant\nethical concerns, including bias issues. Although numerous efforts have been\nmade to quantify and mitigate social bias in AI models, geographic bias (in\nshort, geo-bias) receives much less attention, which presents unique\nchallenges. While previous work has explored ways to quantify geo-bias, these\nmeasures are model-specific (e.g., mean absolute deviation of LLM ratings) or\nspatially implicit (e.g., average fairness scores of all spatial partitions).\nWe lack a model-agnostic, universally applicable, and spatially explicit\ngeo-bias evaluation framework that allows researchers to fairly compare the\ngeo-bias of different AI models and to understand what spatial factors\ncontribute to the geo-bias. In this paper, we establish an\ninformation-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias\nScores). We demonstrate the",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Explainable AI improves task performance in human-AI collaboration",
    "content": "Artificial intelligence (AI) provides considerable opportunities to assist\nhuman work. However, one crucial challenge of human-AI collaboration is that\nmany AI algorithms operate in a black-box manner where the way how the AI makes\npredictions remains opaque. This makes it difficult for humans to validate a\nprediction made by AI against their own domain knowledge. For this reason, we\nhypothesize that augmenting humans with explainable AI as a decision aid\nimproves task performance in human-AI collaboration. To test this hypothesis,\nwe analyze the effect of augmenting domain experts with explainable AI in the\nform of visual heatmaps. We then compare participants that were either\nsupported by (a) black-box AI or (b) explainable AI, where the latter supports\nthem to follow AI predictions when the AI is accurate or overrule the AI when\nthe AI predictions are wrong. We conducted two preregistered experiments with\nrepresentative, real-world visual inspection tasks from manufacturing and\nmedi",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?",
    "content": "AI has surpassed humans across a variety of tasks such as image\nclassification, playing games (e.g., go, \"Starcraft\" and poker), and protein\nstructure prediction. However, at the same time, AI is also bearing serious\ncontroversies. Many researchers argue that little substantial progress has been\nmade for AI in recent decades. In this paper, the author (1) explains why\ncontroversies about AI exist; (2) discriminates two paradigms of AI research,\ntermed \"weak AI\" and \"strong AI\" (a.k.a. artificial general intelligence); (3)\nclarifies how to judge which paradigm a research work should be classified\ninto; (4) discusses what is the greatest value of \"weak AI\" if it has no chance\nto develop into \"strong AI\".",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Understanding Mental Models of AI through Player-AI Interaction",
    "content": "Designing human-centered AI-driven applications require deep understandings\nof how people develop mental models of AI. Currently, we have little knowledge\nof this process and limited tools to study it. This paper presents the position\nthat AI-based games, particularly the player-AI interaction component, offer an\nideal domain to study the process in which mental models evolve. We present a\ncase study to illustrate the benefits of our approach for explainable AI.",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "From Explainable to Interactive AI: A Literature Review on Current\n  Trends in Human-AI Interaction",
    "content": "AI systems are increasingly being adopted across various domains and\napplication areas. With this surge, there is a growing research focus and\nsocietal concern for actively involving humans in developing, operating, and\nadopting these systems. Despite this concern, most existing literature on AI\nand Human-Computer Interaction (HCI) primarily focuses on explaining how AI\nsystems operate and, at times, allowing users to contest AI decisions. Existing\nstudies often overlook more impactful forms of user interaction with AI\nsystems, such as giving users agency beyond contestability and enabling them to\nadapt and even co-design the AI's internal mechanics. In this survey, we aim to\nbridge this gap by reviewing the state-of-the-art in Human-Centered AI\nliterature, the domain where AI and HCI studies converge, extending past\nExplainable and Contestable AI, delving into the Interactive AI and beyond. Our\nanalysis contributes to shaping the trajectory of future Interactive AI design\nand advocate",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "On Two XAI Cultures: A Case Study of Non-technical Explanations in\n  Deployed AI System",
    "content": "Explainable AI (XAI) research has been booming, but the question \"$\\textbf{To\nwhom}$ are we making AI explainable?\" is yet to gain sufficient attention. Not\nmuch of XAI is comprehensible to non-AI experts, who nonetheless, are the\nprimary audience and major stakeholders of deployed AI systems in practice. The\ngap is glaring: what is considered \"explained\" to AI-experts versus non-experts\nare very different in practical scenarios. Hence, this gap produced two\ndistinct cultures of expectations, goals, and forms of XAI in real-life AI\ndeployments.\n  We advocate that it is critical to develop XAI methods for non-technical\naudiences. We then present a real-life case study, where AI experts provided\nnon-technical explanations of AI decisions to non-technical stakeholders, and\ncompleted a successful deployment in a highly regulated industry. We then\nsynthesize lessons learned from the case, and share a list of suggestions for\nAI experts to consider when explaining AI decisions to non-technica",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Putting AI Ethics into Practice: The Hourglass Model of Organizational\n  AI Governance",
    "content": "The organizational use of artificial intelligence (AI) has rapidly spread\nacross various sectors. Alongside the awareness of the benefits brought by AI,\nthere is a growing consensus on the necessity of tackling the risks and\npotential harms, such as bias and discrimination, brought about by advanced AI\ntechnologies. A multitude of AI ethics principles have been proposed to tackle\nthese risks, but the outlines of organizational processes and practices for\nensuring socially responsible AI development are in a nascent state. To address\nthe paucity of comprehensive governance models, we present an AI governance\nframework, the hourglass model of organizational AI governance, which targets\norganizations that develop and use AI systems. The framework is designed to\nhelp organizations deploying AI systems translate ethical AI principles into\npractice and align their AI systems and processes with the forthcoming European\nAI Act. The hourglass framework includes governance requirements at the\nen",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Reproducibility: The New Frontier in AI Governance",
    "content": "AI policymakers are responsible for delivering effective governance\nmechanisms that can provide safe, aligned and trustworthy AI development.\nHowever, the information environment offered to policymakers is characterised\nby an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and\ncreating deep uncertainty and divides on which risks should be prioritised from\na governance perspective. We posit that the current publication speeds in AI\ncombined with the lack of strong scientific standards, via weak reproducibility\nprotocols, effectively erodes the power of policymakers to enact meaningful\npolicy and governance protocols. Our paper outlines how AI research could adopt\nstricter reproducibility guidelines to assist governance endeavours and improve\nconsensus on the AI risk landscape. We evaluate the forthcoming reproducibility\ncrisis within AI research through the lens of crises in other scientific\ndomains; providing a commentary on how adopting preregistration, increased",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Transfeminist AI Governance",
    "content": "This article re-imagines the governance of artificial intelligence (AI)\nthrough a transfeminist lens, focusing on challenges of power, participation,\nand injustice, and on opportunities for advancing equity, community-based\nresistance, and transformative change. AI governance is a field of research and\npractice seeking to maximize benefits and minimize harms caused by AI systems.\nUnfortunately, AI governance practices are frequently ineffective at preventing\nAI systems from harming people and the environment, with historically\nmarginalized groups such as trans people being particularly vulnerable to harm.\nBuilding upon trans and feminist theories of ethics, I introduce an approach to\ntransfeminist AI governance. Applying a transfeminist lens in combination with\na critical self-reflexivity methodology, I retroactively reinterpret findings\nfrom three empirical studies of AI governance practices in Canada and globally.\nIn three reflections on my findings, I show that large-scale AI govern",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "Artificial Intelligence Governance for Businesses",
    "content": "Artificial Intelligence (AI) governance regulates the exercise of authority\nand control over the management of AI. It aims at leveraging AI through\neffective use of data and minimization of AI-related cost and risk. While\ntopics such as AI governance and AI ethics are thoroughly discussed on a\ntheoretical, philosophical, societal and regulatory level, there is limited\nwork on AI governance targeted to companies and corporations. This work views\nAI products as systems, where key functionality is delivered by machine\nlearning (ML) models leveraging (training) data. We derive a conceptual\nframework by synthesizing literature on AI and related fields such as ML. Our\nframework decomposes AI governance into governance of data, (ML) models and\n(AI) systems along four dimensions. It relates to existing IT and data\ngovernance frameworks and practices. It can be adopted by practitioners and\nacademics alike. For practitioners the synthesis of mainly research papers, but\nalso practitioner publicat",
    "category": "Academic Research"
  },
  {
    "source": "arXiv",
    "title": "AI Governance and Accountability: An Analysis of Anthropic's Claude",
    "content": "As AI systems become increasingly prevalent and impactful, the need for\neffective AI governance and accountability measures is paramount. This paper\nexamines the AI governance landscape, focusing on Anthropic's Claude, a\nfoundational AI model. We analyze Claude through the lens of the NIST AI Risk\nManagement Framework and the EU AI Act, identifying potential threats and\nproposing mitigation strategies. The paper highlights the importance of\ntransparency, rigorous benchmarking, and comprehensive data handling processes\nin ensuring the responsible development and deployment of AI systems. We\nconclude by discussing the social impact of AI governance and the ethical\nconsiderations surrounding AI accountability.",
    "category": "Academic Research"
  }
]